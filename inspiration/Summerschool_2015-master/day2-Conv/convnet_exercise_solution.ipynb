{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convnet exercise\n",
    "\n",
    "In this exercise you will be implementing a (pretty slow) convnet! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward network from yesterday's assignment\n",
    "\n",
    "We extend on the neural network from yesterday which is provided (in full) below. Just execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onehot(t, num_classes):\n",
    "    out = np.zeros((t.shape[0], num_classes))\n",
    "    for row, col in enumerate(t):\n",
    "        out[row, col] = 1\n",
    "    return out\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.exp(x)-np.exp(-x)) / (np.exp(x)+np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softplus(x):\n",
    "    return np.log(np.exp(x) + 1)\n",
    "\n",
    "class LinearLayer():\n",
    "    def __init__(self, num_inputs, num_units, scale=0.01):\n",
    "        self.num_units = num_units\n",
    "        self.num_inputs = num_inputs\n",
    "        self.W = np.random.normal(size=(num_inputs, num_units), scale=scale)\n",
    "        self.b = np.zeros(num_units)\n",
    "\n",
    "    def __str__(self): \n",
    "        return \"LinearLayer(%i, %i)\" % (self.num_inputs, self.num_units)\n",
    "\n",
    "    def fprop(self, x, *args):\n",
    "        self.x = x\n",
    "        self.a = np.dot(x, self.W) + self.b\n",
    "        return self.a\n",
    "        \n",
    "    def bprop(self, delta_in):\n",
    "        x_t = np.transpose(self.x)\n",
    "        self.grad_W = np.dot(x_t, delta_in)\n",
    "        self.grad_b = delta_in.sum(axis=0)\n",
    "        W_T = np.transpose(self.W)\n",
    "        self.delta_out = np.dot(delta_in,W_T)\n",
    "        return self.delta_out\n",
    "        \n",
    "    def update_params(self, lr):\n",
    "        self.W = self.W - self.grad_W*lr\n",
    "        self.b = self.b - self.grad_b*lr\n",
    "        \n",
    "class SigmoidActivationLayer():\n",
    "    def __str__(self): \n",
    "        return \"Sigmoid()\"\n",
    "    \n",
    "    def fprop(self, x, train=True):\n",
    "        self.a = 1.0 / (1+np.exp(-x))\n",
    "        return self.a\n",
    "        \n",
    "    def bprop(self, delta_in):\n",
    "        delta_out = self.a * (1 - self.a)*delta_in\n",
    "        return delta_out\n",
    "        \n",
    "    def update_params(self, lr):\n",
    "        pass\n",
    "    \n",
    "class TanhActivationLayer():\n",
    "    def __str__(self): \n",
    "        return \"Tanh()\"\n",
    "    \n",
    "    def fprop(self, x, train=True):\n",
    "        self.a = (np.exp(x)-np.exp(-x)) / (np.exp(x)+np.exp(-x))\n",
    "        return self.a\n",
    "\n",
    "    def bprop(self, delta_in):\n",
    "        delta_out = (1 - ((np.exp(self.a)-np.exp(-self.a)) / (np.exp(self.a)+np.exp(-self.a))))*delta_in\n",
    "        return delta_out\n",
    "        \n",
    "    def update_params(self, lr):\n",
    "        pass\n",
    "\n",
    "class ReluActivationLayer():\n",
    "    def __str__(self): \n",
    "        return \"ReLU()\"\n",
    "\n",
    "    def fprop(self, x, train=True):\n",
    "        self.a = np.maximum(0, x)\n",
    "        return self.a\n",
    "        \n",
    "    def bprop(self, delta_in):\n",
    "        return delta_in * (self.a > 0).astype(self.a.dtype)\n",
    "        \n",
    "    def update_params(self, lr):\n",
    "        pass\n",
    "    \n",
    "class SoftplusActivationLayer():\n",
    "    def __str__(self):\n",
    "        return \"Softplus()\"\n",
    "    \n",
    "    def fprop(self, x, train=True):\n",
    "        self.a = np.log(np.exp(x) + 1)\n",
    "        return self.a\n",
    "    \n",
    "    def bprop(self, delta_in):\n",
    "        return delta_in * (1./(1.+np.exp(-x)))\n",
    "        \n",
    "    def update_params(self, lr):\n",
    "        pass\n",
    "\n",
    "    \n",
    "class SoftmaxActivationLayer():\n",
    "    def __str__(self): \n",
    "        return \"Softmax()\"\n",
    "    \n",
    "    def fprop(self, x, train=True):\n",
    "        x_exp = np.exp(x)\n",
    "        normalizer = x_exp.sum(axis=-1, keepdims=True)\n",
    "        self.a = x_exp / normalizer\n",
    "        return self.a\n",
    "        \n",
    "    def bprop(self, delta_in):\n",
    "        return delta_in\n",
    "        \n",
    "    def update_params(self, lr):\n",
    "        pass\n",
    "\n",
    "class MeanSquaredLoss():\n",
    "    def __str__(self): \n",
    "        return \"MeanSquaredLoss()\"\n",
    "    \n",
    "    def fprop(self, x, t):\n",
    "        num_batches = x.shape[0]\n",
    "        cost = 0.5 * (x-t)**2 / num_batches\n",
    "        return np.mean(np.sum(cost, axis=-1))\n",
    "        \n",
    "    def bprop(self, y, t):\n",
    "        num_batches = y.shape[0]\n",
    "        delta_out = (1./num_batches) * (y-t)\n",
    "        return delta_out\n",
    "        \n",
    "    def update_params(self):\n",
    "        pass\n",
    "\n",
    "class CrossEntropyLoss():\n",
    "    def __str__(self): \n",
    "        return \"CrossEntropyLoss()\"\n",
    "    \n",
    "    def fprop(self, x, t):\n",
    "        tol = 1e-8\n",
    "        return np.mean(np.sum(-t * np.log(x + tol), axis=-1))\n",
    "        \n",
    "    def bprop(self, y, t):\n",
    "        num_batches = y.shape[0]\n",
    "        delta_out = (1./num_batches) * (y-t)\n",
    "        return delta_out\n",
    "        \n",
    "    def update_params(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.load('mnist.npz')\n",
    "num_classes = 10\n",
    "x_train = data['X_train']\n",
    "targets_train = data['y_train']\n",
    "x_train = np.reshape(x_train, (-1, 1, 28, 28))\n",
    "targets_train = onehot(targets_train, num_classes)\n",
    "\n",
    "mean = np.mean(x_train)\n",
    "std = np.std(x_train)\n",
    "x_train -= mean\n",
    "x_train /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient checking\n",
    "\n",
    "In order to verify the correctness of your layers, you will need to [check their gradients numerically](http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization). Below, we have implemented gradient checking functionality for you. Just execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradclose(a, b, rtol=None, atol=None):\n",
    "    rtol = 1e-05 if rtol is None else rtol\n",
    "    atol = 1e-08 if atol is None else atol\n",
    "    diff = abs(a - b) - atol - rtol * (abs(a) + abs(b))\n",
    "    is_close = np.all(diff < 0)\n",
    "    if not is_close:\n",
    "        denom = abs(a) + abs(b)\n",
    "        mask = denom == 0\n",
    "        rel_error = abs(a - b) / (denom + mask)\n",
    "        rel_error[mask] = 0\n",
    "        rel_error = np.max(rel_error)\n",
    "        abs_error = np.max(abs(a - b))\n",
    "        print('rel_error=%.4e, abs_error=%.4e, rtol=%.2e, atol=%.2e'\n",
    "              % (rel_error, abs_error, rtol, atol))\n",
    "    return is_close\n",
    "\n",
    "\n",
    "\n",
    "def approx_fprime(x, f, eps=None, *args):\n",
    "    '''\n",
    "    Central difference approximation of the gradient of a scalar function.\n",
    "    '''\n",
    "    if eps is None:\n",
    "        eps = np.sqrt(np.finfo(np.float_).eps)\n",
    "    grad = np.zeros_like(x)\n",
    "    step = np.zeros_like(x)\n",
    "    for idx in np.ndindex(x.shape):\n",
    "        step[idx] = eps * max(abs(x[idx]), 1.0)\n",
    "        grad[idx] = (f(*((x+step,) + args)) -\n",
    "                     f(*((x-step,) + args))) / (2*step[idx])\n",
    "        step[idx] = 0.0\n",
    "    return grad\n",
    "\n",
    "\n",
    "def check_grad(layer, x0, seed=1, eps=None, rtol=None, atol=None):\n",
    "    '''\n",
    "    Numerical gradient checking of layer bprop.\n",
    "    '''\n",
    "    # Check input gradient\n",
    "    def fun(x):\n",
    "        y = layer.fprop(x)\n",
    "        return np.sum(y)\n",
    "\n",
    "    def fun_grad(x):\n",
    "        y = layer.fprop(x)\n",
    "        y_grad = np.ones_like(y)\n",
    "        x_grad = layer.bprop(y_grad)\n",
    "        return x_grad\n",
    "\n",
    "    g_approx = approx_fprime(x0, fun, eps)\n",
    "    g_true = fun_grad(x0)\n",
    "    if not gradclose(g_true, g_approx, rtol, atol):\n",
    "        raise RuntimeError(\n",
    "            'Incorrect input gradient: \\nbprop:\\n%s\\napprox:\\n%s'\n",
    "            % (g_true, g_approx)\n",
    "        )\n",
    "\n",
    "    # Check parameter gradients\n",
    "    def fun(x, p_idx):\n",
    "        param_array = layer.params()[p_idx]\n",
    "        param_array *= 0\n",
    "        param_array += x\n",
    "        y = layer.fprop(x0)\n",
    "        return np.sum(y)\n",
    "\n",
    "    def fun_grad(x, p_idx):\n",
    "        param_array = layer.params()[p_idx]\n",
    "        param_array *= 0\n",
    "        param_array += x\n",
    "        out = layer.fprop(x0)\n",
    "        y_grad = np.ones_like(out)\n",
    "        layer.bprop(y_grad)\n",
    "        param_grad = layer.grads()[p_idx]\n",
    "        return param_grad\n",
    "\n",
    "    for p_idx, p in enumerate(layer.params()):\n",
    "        x = np.copy(layer.params()[p_idx])\n",
    "        g_true = fun_grad(x, p_idx)\n",
    "        g_approx = approx_fprime(x, fun, eps, p_idx)\n",
    "        if not gradclose(g_true, g_approx, rtol, atol):\n",
    "            raise RuntimeError(\n",
    "                'Incorrect parameter gradient: \\nbprop:\\n%s\\napprox:\\n%s'\n",
    "                % (g_true, g_approx)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task #1: Convolution layer\n",
    "\n",
    "\n",
    "You should implement a 2D convolution layer by filling out the missing pieces and execute the cell. If the gradient check fails, you will get an error.\n",
    "\n",
    "### Bonus task:\n",
    "- Implement support for border modes `'full'` and `'valid'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv_bc01(imgs, filters, padding):\n",
    "    batch_size, n_channels_img, img_h, img_w = imgs.shape\n",
    "    n_filters, n_channels, win_h, win_w = filters.shape\n",
    "    pad_y, pad_x = padding\n",
    "    if n_channels != n_channels_img:\n",
    "        raise ValueError('Mismatch in # of channels')\n",
    "\n",
    "    # Create output array\n",
    "    out_h = (img_h - win_h + 2*pad_y) + 1\n",
    "    out_w = (img_w - win_w + 2*pad_x) + 1\n",
    "    out_shape = (batch_size, n_filters, out_h, out_w)\n",
    "    out = np.zeros(out_shape)\n",
    "\n",
    "    # Pad input images\n",
    "    imgs = np.pad(imgs, ((0, 0), (0, 0), padding, padding), mode='constant')\n",
    "\n",
    "    # Perform convolution\n",
    "    for b in range(batch_size):\n",
    "        for f in range(n_filters):\n",
    "            for c in range(n_channels):\n",
    "                out[b, f] += scipy.signal.convolve(imgs[b, c], filters[f, c], mode='valid')\n",
    "    return out\n",
    "    \n",
    "\n",
    "class ConvLayer():\n",
    "    def __init__(self, n_channels, n_filters, filter_size=5, scale=0.01,\n",
    "                 border_mode='same'):\n",
    "        self.n_channels = n_channels\n",
    "        self.n_filters = n_filters\n",
    "        self.filter_size = filter_size\n",
    "        w_shape = (n_filters, n_channels, filter_size, filter_size)\n",
    "        self.W = np.random.normal(size=w_shape, scale=scale)\n",
    "        self.b = np.zeros((1, n_filters, 1, 1))\n",
    "        if border_mode == 'valid':\n",
    "            self.padding = 0\n",
    "        elif border_mode == 'same':\n",
    "            self.padding = filter_size // 2\n",
    "        elif border_mode == 'full':\n",
    "            self.padding = filter_size - 1\n",
    "        else:\n",
    "            raise ValueError('Invalid border_mode: %s' % border_mode)\n",
    "        self.padding = (self.padding, self.padding)\n",
    "\n",
    "        \n",
    "    def __str__(self): \n",
    "        return (\"ConvLayer(%i, %i, %i)\"\n",
    "                % (self.n_channels, self.n_filters, self.filter_size))\n",
    "\n",
    "    def fprop(self, x, *args):\n",
    "        '''\n",
    "        Input:\n",
    "            x: Array of shape (batch_size, n_channels, img_height, img_width)\n",
    "        Output:\n",
    "            Array of shape (batch_size, n_filters, out_height, out_width)\n",
    "        '''\n",
    "        # Store x for brop()\n",
    "        self.x = x\n",
    "\n",
    "        # Perform convolution\n",
    "        y = conv_bc01(x, self.W, self.padding)\n",
    "        \n",
    "        # Add bias\n",
    "        y = y + self.b\n",
    "        return y\n",
    "        \n",
    "    def bprop(self, dy):\n",
    "        # Flip weights\n",
    "        w = self.W[:, :, ::-1, ::-1]\n",
    "        # Transpose channel/filter dimensions of weights\n",
    "        w = np.transpose(w, (1, 0, 2, 3))\n",
    "\n",
    "        # Propagate gradients to x\n",
    "        dx = conv_bc01(dy, w, self.padding)\n",
    "        \n",
    "        # Propagate gradients to weights\n",
    "        x = np.pad(self.x, ((0, 0), (0, 0), self.padding, self.padding), mode='constant')\n",
    "\n",
    "        self.grad_W = np.zeros_like(self.W)\n",
    "        for b in range(dy.shape[0]):\n",
    "            for f in range(self.W.shape[0]):\n",
    "                for c in range(self.W.shape[1]):\n",
    "                    self.grad_W[f, c] += scipy.signal.convolve(x[b, c], dy[b, f], mode='valid')\n",
    "        self.grad_W = self.grad_W[:, :, ::-1, ::-1]\n",
    "\n",
    "        # Propagate gradients to bias\n",
    "        self.grad_b = np.sum(dy, keepdims=True, axis=(0, 2, 3))\n",
    "        return dx\n",
    "        \n",
    "    def update_params(self, lr):\n",
    "        self.W = self.W - self.grad_W*lr\n",
    "        self.b = self.b - self.grad_b*lr\n",
    "\n",
    "    def params(self):\n",
    "        return self.W, self.b\n",
    "\n",
    "    def grads(self):\n",
    "        return self.grad_W, self.grad_b\n",
    "\n",
    "\n",
    "# Remember to try different parameters. The given parameters are chosen \n",
    "# as simple as possible and you may easily discover mistakes in your\n",
    "# code by changing the parameters.\n",
    "\n",
    "batch_size = 2\n",
    "n_channels = 1\n",
    "img_shape = (5, 5)\n",
    "n_filters = 2\n",
    "filter_size = 3\n",
    "\n",
    "# Border_modes 'full' and 'valid' are left as a bonus task.\n",
    "border_mode = 'same'\n",
    "\n",
    "x = np.random.normal(size=(batch_size, n_channels) + img_shape)\n",
    "layer = ConvLayer(n_channels=n_channels, n_filters=n_filters,\n",
    "                  filter_size=filter_size, border_mode=border_mode)\n",
    "\n",
    "check_grad(layer, x)\n",
    "print('Gradient check passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task #2: Pooling layer\n",
    "\n",
    "\n",
    "You should implement average pooling by fillling out the missing pieces and execute the cell. If the gradient check fails, you will get an error.\n",
    "\n",
    "### Bonus task:\n",
    "- Implement max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PoolLayer():\n",
    "    def __init__(self, win_size=3, stride=2):\n",
    "        self.win_size = win_size\n",
    "        self.stride = stride\n",
    "        self.padding = self.win_size // 2\n",
    "\n",
    "    def __str__(self): \n",
    "        return \"PoolLayer(%i, %i)\" % (self.win_size, self.stride)\n",
    "\n",
    "    def fprop(self, imgs, *args):\n",
    "        '''\n",
    "        Input:\n",
    "            x: Array of shape (batch_size, n_channels, img_height, img_width)\n",
    "        Output:\n",
    "            Array of shape (batch_size, n_channels, out_height, out_width)\n",
    "        '''\n",
    "        batch_size, n_channels, img_h, img_w = imgs.shape\n",
    "\n",
    "        # Store x for brop()\n",
    "        self.imgs = imgs\n",
    "\n",
    "        # Create output array\n",
    "        out_h = (img_h - self.win_size + 2*self.padding) // self.stride + 1\n",
    "        out_w = (img_w - self.win_size + 2*self.padding) // self.stride + 1\n",
    "        out = np.zeros((batch_size, n_channels, out_h, out_w))\n",
    "        \n",
    "        # Perform average pooling\n",
    "        imgs = imgs / self.win_size**2\n",
    "        for b in range(batch_size):\n",
    "            for c in range(n_channels):\n",
    "                for y in range(out_h):\n",
    "                    y_ = y * self.stride\n",
    "                    for x in range(out_w):\n",
    "                        x_ = x * self.stride\n",
    "                        win = imgs[b, c, max(y_, 0):y_+self.win_size,\n",
    "                                   max(x_, 0):x_+self.win_size]\n",
    "                        out[b, c, y, x] = np.sum(win)\n",
    "        return out\n",
    "        \n",
    "    def bprop(self, dy):\n",
    "        dx = np.zeros_like(self.imgs)\n",
    "        dy = dy / self.win_size**2\n",
    "        for i in range(dx.shape[0]):\n",
    "            for c in range(dx.shape[1]):\n",
    "                for y in range(dy.shape[2]):\n",
    "                    y_ = y * self.stride\n",
    "                    for x in range(dy.shape[3]):\n",
    "                        x_ = x * self.stride\n",
    "                        dx[i, c, y_:y_+self.win_size, x_:x_+self.win_size] += dy[i, c, y, x]\n",
    "        return dx\n",
    "\n",
    "    def update_params(self, lr):\n",
    "        pass\n",
    "\n",
    "    def params(self):\n",
    "        return []\n",
    "\n",
    "    def grads(self):\n",
    "        return []\n",
    "\n",
    "# Remember to try different parameters. The given parameters are chosen \n",
    "# as simple as possible and you may easily discover mistakes in your\n",
    "# code by changing the parameters.\n",
    "\n",
    "batch_size = 1\n",
    "n_channels = 1\n",
    "img_shape = (5, 5)\n",
    "win_size = 3\n",
    "\n",
    "x = np.random.normal(size=(batch_size, n_channels) + img_shape)\n",
    "\n",
    "layer = PoolLayer(win_size=3, stride=2)\n",
    "check_grad(layer, x)\n",
    "print('Gradient check passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task #3: Flatten layer\n",
    "\n",
    "\n",
    "You should implement flattening such that your convnet layers can be used with a multi-layer perceptron network. Fill out the missing pieces. Gradient checking shouldn't be necessary for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FlattenLayer():\n",
    "    def __str__(self): \n",
    "        return \"Flatten()\"\n",
    "\n",
    "    def fprop(self, x, *args):\n",
    "        '''\n",
    "        Input:\n",
    "            x: Array of shape (batch_size, n_channels, img_height, img_width)\n",
    "        Output:\n",
    "            Array of shape (batch_size, n_channels * img_height * img_width)\n",
    "        '''\n",
    "\n",
    "        # Store shape for brop()\n",
    "        self.shape = x.shape\n",
    "        y = np.reshape(x, (x.shape[0], -1))\n",
    "        return y\n",
    "\n",
    "    def bprop(self, delta_in):\n",
    "        return np.reshape(delta_in, self.shape)\n",
    "\n",
    "    def update_params(self, lr):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task #4: A pretty lousy convnet!\n",
    "\n",
    "Unfortunately, your implementation is too slow to be useful. However, as a final check of your convnet layers, you should try to train a small convnet on MNIST images.\n",
    "\n",
    "Run the code and verify that you get an accuracy above 0.2 after 150 gradient updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_samples, n_channels, img_h, img_w = x_train.shape\n",
    "num_hidden_units = 64\n",
    "num_classes = 10\n",
    "\n",
    "layers = [\n",
    "    ConvLayer(n_channels=1, n_filters=4, filter_size=5, scale=0.1),\n",
    "    PoolLayer(win_size=3, stride=2),\n",
    "    ReluActivationLayer(),\n",
    "    ConvLayer(n_channels=4, n_filters=16, filter_size=5, scale=0.1),\n",
    "    PoolLayer(win_size=3, stride=2),\n",
    "    ReluActivationLayer(),\n",
    "    FlattenLayer(),\n",
    "    LinearLayer(784, num_hidden_units, scale=0.1),\n",
    "    ReluActivationLayer(),\n",
    "    LinearLayer(num_hidden_units, num_classes, scale=0.1),\n",
    "    SoftmaxActivationLayer(),\n",
    "]\n",
    "\n",
    "LossLayer = CrossEntropyLoss()\n",
    "\n",
    "def forward(x):\n",
    "    for layer in layers:\n",
    "        x = layer.fprop(x)\n",
    "    return x\n",
    "\n",
    "def backward(y_probs, targets):\n",
    "    d = LossLayer.bprop(y_probs, targets)\n",
    "    for layer in reversed(layers):\n",
    "        d = layer.bprop(d)\n",
    "    \n",
    "def update(learning_rate):\n",
    "    for layer in layers:\n",
    "        layer.update_params(learning_rate)\n",
    "\n",
    "\n",
    "from confusionmatrix import ConfusionMatrix\n",
    "batch_size = 4\n",
    "num_epochs = 50\n",
    "learning_rate = 0.05\n",
    "num_samples = x_train.shape[0]\n",
    "num_batches = num_samples // batch_size\n",
    "\n",
    "\n",
    "n_updates = 0\n",
    "for epoch in range(num_epochs):\n",
    "    confusion = ConfusionMatrix(num_classes)\n",
    "    for i in range(num_batches):\n",
    "        n_updates += 1\n",
    "        idx = range(i*batch_size, (i+1)*batch_size)\n",
    "        x_batch = x_train[idx]\n",
    "        target_batch = targets_train[idx]\n",
    "        y_probs = forward(x_batch)\n",
    "        loss = LossLayer.fprop(y_probs, target_batch)\n",
    "        backward(y_probs, target_batch)\n",
    "        update(learning_rate)\n",
    "        confusion.batch_add(target_batch.argmax(-1), y_probs.argmax(-1))\n",
    "        \n",
    "        if n_updates % 25 == 0:\n",
    "            curr_acc = confusion.accuracy()\n",
    "            print \"Update %i : Loss %f Train acc %f\" % (n_updates, loss, curr_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
