{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, clear_output\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto-encoders 101\n",
    "\n",
    "In this exercise we'll implement a variational auto-encoder (VAE). An auto-encoder encodes some input into a new and usually more compact representation which can be used to reconstruct the input data again. A VAE makes the assumption that the compact representation follows a probabilistic distribution (usually Gaussian) which makes it possible to sample new data from a trained variational auto-encoder. The \"variational\" part comes from the fact that these models are training through variational inference.\n",
    "\n",
    "The mathematical details of the training can be a bit challenging. However, we believe that probabilistic deep learning will be an important part of future machine learning, which is why we find it important to introduce the concepts.\n",
    "\n",
    "As background material we recommend reading [Tutorial on Variational Autoencoder](http://arxiv.org/abs/1606.05908). For the implementation of the model you can read the article \"Auto-Encoding Variational Bayes\", Kingma & Welling, ICLR 2014: http://arxiv.org/pdf/1312.6114v10.pdf and \"Stochastic Backpropagation and Approximate Inference in Deep Generative Models\", Rezende et al, ICML 2014: http://arxiv.org/pdf/1401.4082v3.pdf.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE crash course\n",
    "\n",
    "VAEs consist of two parts:\n",
    "\n",
    " * Encoder (also known as recognition, inference or Q-model): Maps the input data into a probabilistic latent space by calculating the mean and variance parameters of a Gaussian distribution as a function of the input data x:  $q(z|x) = \\mathcal{N}(z|\\mu_\\theta(x), \\sigma_\\phi(x)I)$\n",
    " * Decoder (also known as generative or P-model): Reconstructs the input data using a sample from the latent space defined by the encoder model: $p(x|z)$\n",
    "<img src=\"VAE.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n",
    "In more mathematical details we have\n",
    "\n",
    "$p(x) = \\int_z p(x|z)p(z)dz$\n",
    "\n",
    "$p(x) = \\int_z p(x|z)p(z)\\frac{q(z|x)}{q(z|x)}dz$\n",
    "\n",
    "\n",
    "$p(x) = \\int_z q(z|x) \\frac{p(x|z)p(z)}{q(z|x)}dz$\n",
    "\n",
    "\n",
    "$\\log p(x) = \\log \\int_z q(z|x) \\frac{p(x|z)p(z)}{q(z|x)}dz$\n",
    "\n",
    "The above can be rewritten by applying [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality):\n",
    "\n",
    "$\\log p(x) \\geq  \\int_z q(z|x)\\log \\frac{p(x|z)p(z)}{q(z|x)}dz$\n",
    "\n",
    "This is denoted the variational lower bound. We continue with a bit of rewriting\n",
    "\n",
    "$\\log p(x) \\geq E_{q(z|x)} \\left[\\log \\frac{p(x|z)p(z)}{q(z|x)}\\right]$\n",
    "\n",
    "$\\log p(x) \\geq E_{q(z|x)} \\left[\\log p(x|z)\\right] - KL(q(z|x) || p(z))$\n",
    "\n",
    "Here the first term on the R.H.S. is the data reconstruction and the second term the Kullback-Leibler divergence between the approximate and true posterior distributions which acts as a probabilistic regularizer.\n",
    "\n",
    "### Training a VAE \n",
    "The VAE is similar to a deterministic autoencoder (lab51) except that we assume that the latent units follows a distribution. Usually we just assume that the units are independent standard Gaussian distributed.\n",
    "\n",
    "Above we defined a lower bound on the log-likelihood of the data. We can train the model by pushing up the lowerbound.  By using the _reparameterization trick_ we can directly backprop through the model and optimize the parameters w.r.t. the lower bound.\n",
    "\n",
    "### Setting up the network\n",
    "\n",
    "We define the network like an auto-encoder except that the bottle-neck is the __SimpleSampleLayer__ which samples the hidden units. The lower bound is calculated in the ```LogLikelihood``` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "First let's load the MNIST dataset and plot a few examples. We only load a limited amount of number classes, so that we can speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "#To speed up training we'll only work on a subset of the data\n",
    "#We discretize the data to 0 and 1 in order to use it with a bernoulli observation model p(x|z) = Ber(mu(z))\n",
    "\n",
    "def bernoullisample(x):\n",
    "    return np.random.binomial(1,x,size=x.shape).astype(theano.config.floatX)\n",
    "\n",
    "data = np.load('../week1/mnist.npz')\n",
    "num_classes = 3\n",
    "idxs_train = []\n",
    "idxs_valid = []\n",
    "idxs_test = []\n",
    "for i in range(num_classes):\n",
    "    idxs_train += np.where(data['y_train'] == i)[0].tolist()\n",
    "    idxs_valid += np.where(data['y_valid'] == i)[0].tolist()\n",
    "    idxs_test += np.where(data['y_test'] == i)[0].tolist()\n",
    "\n",
    "x_train = bernoullisample(data['X_train'][idxs_train]).astype('float32')\n",
    "targets_train = data['y_train'][idxs_train].astype('int32') # Since this is unsupervised, the targets are only used for validation.\n",
    "x_train, targets_train = shuffle(x_train, targets_train, random_state=1234)\n",
    "\n",
    "x_valid = bernoullisample(data['X_valid'][idxs_valid]).astype('float32')\n",
    "targets_valid = data['y_valid'][idxs_valid].astype('int32')\n",
    "\n",
    "x_test = bernoullisample(data['X_test'][idxs_test]).astype('float32')\n",
    "targets_test = data['y_test'][idxs_test].astype('int32')\n",
    "\n",
    "print(\"training set dim(%i, %i).\" % x_train.shape)\n",
    "print(\"validation set dim(%i, %i).\" % x_valid.shape)\n",
    "print(\"test set dim(%i, %i).\" % x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot a few MNIST examples\n",
    "def plot_samples(x,title=''):\n",
    "    idx = 0\n",
    "    canvas = np.zeros((28*10, 10*28))\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_train[idx].reshape((28, 28))\n",
    "            idx += 1\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(canvas, cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_samples(x_train[:100],title='MNIST handwritten digits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate the lower bound, we define following density functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#defined a couple of helper functions\n",
    "c = - 0.5 * math.log(2*math.pi)\n",
    "def log_bernoulli(x, p, eps=0.0):\n",
    "    p = T.clip(p, eps, 1.0 - eps)\n",
    "    return -T.nnet.binary_crossentropy(p, x)\n",
    "\n",
    "def kl_normal2_stdnormal(mean, log_var):\n",
    "    return -0.5*(1 + log_var - mean**2 - T.exp(log_var))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "When defining the model the latent layer must act as a bottleneck of information, so that we ensure that we find a strong internal representation. We initialize the VAE with 1 hidden layer in the encoder and decoder using relu units as non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer,DenseLayer,get_output, get_all_params\n",
    "from lasagne.nonlinearities import rectify, sigmoid\n",
    "from samplelayer import SimpleSampleLayer\n",
    "\n",
    "num_features = x_train.shape[-1]\n",
    "num_latent_z = 2\n",
    "num_units = 128\n",
    "#MODEL SPECIFICATION\n",
    "\n",
    "#ENCODER\n",
    "l_in_x = InputLayer(shape=(None, num_features))\n",
    "l_enc  = DenseLayer(l_in_x, num_units=num_units, nonlinearity=rectify)\n",
    "l_muq  = DenseLayer(l_enc, num_units=num_latent_z, nonlinearity=None)     #mu(x)\n",
    "l_logvarq = DenseLayer(l_enc, num_units=num_latent_z, nonlinearity=lambda x: T.clip(x,-10,10)) #logvar(x), \n",
    "l_z = SimpleSampleLayer(mean=l_muq, log_var=l_logvarq) #sample a latent representation z \\sim q(z|x) = N(mu(x),logvar(x))\n",
    "#we split the model into two parts to allow sampling from the decoder model separately\n",
    "#DECODER\n",
    "l_in_z = InputLayer(shape=(None, num_latent_z))\n",
    "l_dec = DenseLayer(l_in_z, num_units=num_units, nonlinearity=rectify) \n",
    "l_mux = DenseLayer(l_dec, num_units=num_features, nonlinearity=sigmoid)  #reconstruction of input using a sigmoid output since mux \\in [0,1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following we define the Theano functions for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "sym_x = T.matrix('x')\n",
    "sym_z = T.matrix('z')\n",
    "\n",
    "z_train, muq_train, logvarq_train = get_output([l_z,l_muq,l_logvarq],{l_in_x:sym_x},deterministic=False)\n",
    "mux_train = get_output(l_mux,{l_in_z:z_train},deterministic=False)\n",
    "\n",
    "z_eval, muq_eval, logvarq_eval = get_output([l_z,l_muq,l_logvarq],{l_in_x:sym_x},deterministic=True)\n",
    "mux_eval = get_output(l_mux,{l_in_z:z_eval},deterministic=True)\n",
    "\n",
    "mux_sample = get_output(l_mux,{l_in_z:sym_z},deterministic=True)\n",
    "\n",
    "#define the cost function\n",
    "def LogLikelihood(mux,x,muq,logvarq):\n",
    "    log_px_given_z = log_bernoulli(x, mux, eps=1e-6).sum(axis=1).mean() #note that we sum the latent dimension and mean over the samples\n",
    "    KL_qp = kl_normal2_stdnormal(muq, logvarq).sum(axis=1).mean()\n",
    "    LL = log_px_given_z - KL_qp\n",
    "    return LL, log_px_given_z, KL_qp\n",
    "\n",
    "LL_train, logpx_train, KL_train = LogLikelihood(mux_train, sym_x, muq_train, logvarq_train)\n",
    "LL_eval, logpx_eval, KL_eval = LogLikelihood(mux_eval, sym_x, muq_eval, logvarq_eval)\n",
    "\n",
    "all_params = get_all_params([l_z,l_mux],trainable=True)\n",
    "\n",
    "# Let Theano do its magic and get all the gradients we need for training\n",
    "all_grads = T.grad(-LL_train, all_params)\n",
    "\n",
    "# Set the update function for parameters. The Adam optimizer works really well with VAEs.\n",
    "updates = lasagne.updates.adam(all_grads, all_params, learning_rate=1e-2)\n",
    "\n",
    "f_train = theano.function(inputs=[sym_x],\n",
    "                          outputs=[LL_train, logpx_train, KL_train],\n",
    "                          updates=updates)\n",
    "\n",
    "f_eval = theano.function(inputs=[sym_x],\n",
    "                         outputs=[LL_eval, logpx_eval, KL_eval])\n",
    "\n",
    "f_z = theano.function(inputs=[sym_x],\n",
    "                         outputs=[z_eval])\n",
    "\n",
    "f_sample = theano.function(inputs=[sym_z],\n",
    "                         outputs=[mux_sample])\n",
    "\n",
    "f_recon = theano.function(inputs=[sym_x],\n",
    "                         outputs=[mux_eval])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training loop we sample each batch and evaluate the error, latent space and reconstructions every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "samples_to_process = 1e4\n",
    "val_interval = 5e2\n",
    " \n",
    "LL_train, KL_train, logpx_train = [],[],[]\n",
    "LL_valid, KL_valid, logpx_valid = [],[],[]\n",
    "samples_processed = 0\n",
    "plt.figure(figsize=(12, 24))\n",
    "valid_samples_processed = []\n",
    "\n",
    "try:\n",
    "    while samples_processed < samples_to_process:\n",
    "        _LL_train, _KL_train, _logpx_train = [],[],[]\n",
    "        idxs = np.random.choice(range(x_train.shape[0]), size=(batch_size), replace=False)  \n",
    "        x_batch = x_train[idxs]\n",
    "        out = f_train(x_batch)\n",
    "        samples_processed += batch_size\n",
    "           \n",
    "        if samples_processed % val_interval == 0:\n",
    "            valid_samples_processed += [samples_processed]\n",
    "            out = f_eval(x_train)\n",
    "            LL_train += [out[0]] \n",
    "            logpx_train += [out[1]]\n",
    "            KL_train += [out[2]]\n",
    "            \n",
    "            out = f_eval(x_valid)\n",
    "            LL_valid += [out[0]]\n",
    "            logpx_valid += [out[1]]\n",
    "            KL_valid += [out[2]]\n",
    "            \n",
    "            z_eval = f_z(x_valid)[0]\n",
    "            x_sample = f_sample(np.random.normal(size=(100, num_latent_z)).astype('float32'))[0]\n",
    "            x_recon = f_recon(x_valid)[0]\n",
    "            \n",
    "            plt.subplot(num_classes+2,2,1)\n",
    "            plt.legend(['LL', 'log(p(x))'], loc=2)\n",
    "            plt.xlabel('Updates')\n",
    "            plt.plot(valid_samples_processed, LL_train, color=\"black\")\n",
    "            plt.plot(valid_samples_processed, logpx_train, color=\"red\")\n",
    "            plt.plot(valid_samples_processed, LL_valid, color=\"black\", linestyle=\"--\")\n",
    "            plt.plot(valid_samples_processed, logpx_valid, color=\"red\", linestyle=\"--\")\n",
    "            plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "            plt.grid('on')\n",
    "            \n",
    "            plt.subplot(num_classes+2,2,2)\n",
    "            plt.cla()\n",
    "            plt.xlabel('z0'), plt.ylabel('z1')\n",
    "            color = iter(plt.get_cmap('brg')(np.linspace(0, 1.0, num_classes)))\n",
    "            for i in range(num_classes):\n",
    "                clr = next(color)\n",
    "                plt.scatter(z_eval[targets_valid==i, 0], z_eval[targets_valid==i, 1], c=clr, s=5., lw=0, marker='o', )\n",
    "            plt.grid('on')\n",
    "            \n",
    "            plt.savefig(\"out52.png\")\n",
    "            display(Image(filename=\"out52.png\"))\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            plt.subplot(num_classes+2,2,3)\n",
    "            plt.legend(['KL(q||p)'])\n",
    "            plt.xlabel('Updates')\n",
    "            plt.plot(valid_samples_processed, KL_train, color=\"blue\")\n",
    "            plt.plot(valid_samples_processed, KL_valid, color=\"blue\", linestyle=\"--\")\n",
    "            plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "            plt.grid('on')\n",
    "            \n",
    "            plt.subplot(num_classes+2,2,4)\n",
    "            plt.cla()\n",
    "            plt.title('Samples')\n",
    "            plt.axis('off')\n",
    "            idx = 0\n",
    "            canvas = np.zeros((28*10, 10*28))\n",
    "            for i in range(10):\n",
    "                for j in range(10):\n",
    "                    canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_sample[idx].reshape((28, 28))\n",
    "                    idx += 1\n",
    "            plt.imshow(canvas, cmap='gray')\n",
    "            \n",
    "            c=0\n",
    "            for k in range(5, 5 + num_classes*2, 2):\n",
    "                plt.subplot(num_classes+2,2,k)\n",
    "                plt.cla()\n",
    "                plt.title('Inputs for %i' % c)\n",
    "                plt.axis('off')\n",
    "                idx = 0\n",
    "                canvas = np.zeros((28*10, 10*28))\n",
    "                for i in range(10):\n",
    "                    for j in range(10):\n",
    "                        canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_valid[targets_valid==c][idx].reshape((28, 28))\n",
    "                        idx += 1\n",
    "                plt.imshow(canvas, cmap='gray')\n",
    "\n",
    "                plt.subplot(num_classes+2,2,k+1)\n",
    "                plt.cla()\n",
    "                plt.title('Reconstructions for %i' % c)\n",
    "                plt.axis('off')\n",
    "                idx = 0\n",
    "                canvas = np.zeros((28*10, 10*28))\n",
    "                for i in range(10):\n",
    "                    for j in range(10):\n",
    "                        canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_recon[targets_valid==c][idx].reshape((28, 28))\n",
    "                        idx += 1\n",
    "                plt.imshow(canvas, cmap='gray')\n",
    "                c += 1\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Analyzing the VAE\n",
    "1. The above implementation of a VAE is very simple.\n",
    "    - *Experiment with the number of layers and non-linearities in order to improve the reconstructions.*\n",
    "    - *Try to increase the number of digit classes in the training set and analyze the results.*\n",
    "       \n",
    "2. Complexity of the bottleneck.\n",
    "    - *Increase the number of units in the latent layer and train.*\n",
    "    - *Visualize by using [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) and [t-SNE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html).*\n",
    "3. Analyzing the KL-term.\n",
    "    - *Remove the KL-term ($KL \\cdot 0$) and analyze what happens to the training.*\n",
    "\n",
    "4. Use the original paper http://arxiv.org/pdf/1312.6114v10.pdf or [this blog](http://blog.shakirm.com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks/) to explain what the reparameterization trick does.\n",
    "\n",
    "5. The VAE is a probablistic model. We could model $p(x,z,y)$ where $y$ is the label information. Explain how this model could handle semi-supervised learning? You can look through the papers https://arxiv.org/pdf/1406.5298.pdf or  https://arxiv.org/pdf/1602.05473v4.pdf. You'll have to implement this in lab53.\n",
    "\n",
    "6. Sampling in the VAE.\n",
    "    - *Explain how one could implement multiple samples in the VAE and how that would improve learning*.\n",
    "    - *Look through https://arxiv.org/abs/1509.00519 and explain importance weighted auto-encoders.*\n",
    "    - *Implement sampling.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example code for PCA.\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# pca.fit(X)\n",
    "# pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Math intuition (Optional, Credits to Charles Blundell)\n",
    "1. Use Jensen's inequality to show that, if $q(z|x)\\neq 0$,\n",
    "$$\n",
    "log p(x) \\geq \\mathbb{E}_{q(z|x)} \\Big [ \\log \\frac{p(x,z)}{q(z|x)} \\Big ]\n",
    "$$\n",
    "\n",
    "2. Show that if $\\mu_\\theta(x)$ and $\\sigma_\\phi(x)$ are the encoding neural networks with parameters $\\theta$ and $\\phi$ respectively, taking an input $x$ and\n",
    "\n",
    "$$\n",
    "p(z) = \\mathcal{N}(z|0,I)\\\\\n",
    "q(z|x) = \\mathcal{N}(z|\\mu_\\theta(x), \\sigma_\\phi(x)I)\\\\\n",
    "\\mathcal{F}(\\theta,\\phi) = \\mathbb{E}_{q(z|x)} \\Big [ \\log \\frac{p(x|z)p(z)}{q(z|x)} \\Big ]\n",
    "$$\n",
    "\n",
    "then\n",
    "\n",
    "$$\n",
    "\\mathcal{F}(\\theta,\\phi) = \\mathbb{E}_{\\mathcal{N}(\\epsilon|0,I)} \\Big [ \\log \\frac{p(x|z)p(z)}{q(z|x)} \\Big ] \\\\\n",
    "z = \\mu_\\theta(x) + \\sigma_\\phi(x) \\cdot \\epsilon \\\\\n",
    "\\frac{\\partial z}{\\partial \\theta} = \\frac{\\partial \\mu_\\theta(x)}{\\partial \\theta} \\\\\n",
    "\\frac{\\partial z}{\\partial \\phi} = \\frac{\\partial \\sigma_\\phi(x)}{\\partial \\phi} \\cdot \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
