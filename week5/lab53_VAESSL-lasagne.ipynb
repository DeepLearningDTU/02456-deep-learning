{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, clear_output\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto-encoders and Semi-supervised Learning\n",
    "\n",
    "Now we are going to extend the VAE from lab52 with a variable $y$, so that we can apply supervised and semi-supervised learning. We'll model the supervised VAE in which we model $p(x|y,z)$ and $q(z|x,y)$ (cf. [Kingma et al.](https://arxiv.org/abs/1406.5298)). This way we condition our latent variable and our generated sample on our label $y$. Next we'll define semi-supervised learning in which $y$ is given for the labeled data and as a latent variable that is marginalized out for the unlabeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: Supervised VAE\n",
    "First we'll load the MNIST dataset and plot a few examples. We only load a limited amount of number classes, so that we can speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "#To speed up training we'll only work on a subset of the data\n",
    "#We discretize the data to 0 and 1 in order to use it with a bernoulli observation model p(x|z) = Ber(mu(z))\n",
    "\n",
    "def bernoullisample(x):\n",
    "    return np.random.binomial(1,x,size=x.shape).astype(theano.config.floatX)\n",
    "\n",
    "def onehot(t, num_classes):\n",
    "    out = np.zeros((t.shape[0], num_classes)).astype('float32')\n",
    "    for row, col in enumerate(t):\n",
    "        out[row, col] = 1\n",
    "    return out\n",
    "\n",
    "data = np.load('../week1/mnist.npz')\n",
    "num_classes = 3\n",
    "idxs_train = []\n",
    "idxs_valid = []\n",
    "idxs_test = []\n",
    "for i in range(num_classes):\n",
    "    idxs_train += np.where(data['y_train'] == i)[0].tolist()\n",
    "    idxs_valid += np.where(data['y_valid'] == i)[0].tolist()\n",
    "    idxs_test += np.where(data['y_test'] == i)[0].tolist()\n",
    "\n",
    "x_train = bernoullisample(data['X_train'][idxs_train]).astype('float32')\n",
    "targets_train = data['y_train'][idxs_train].astype('int32') # Since this is unsupervised, the targets are only used for validation.\n",
    "x_train, targets_train = shuffle(x_train, targets_train, random_state=1234)\n",
    "t_train = onehot(targets_train, num_classes)\n",
    "\n",
    "x_valid = bernoullisample(data['X_valid'][idxs_valid]).astype('float32')\n",
    "targets_valid = data['y_valid'][idxs_valid].astype('int32')\n",
    "t_valid = onehot(targets_valid, num_classes)\n",
    "\n",
    "x_test = bernoullisample(data['X_test'][idxs_test]).astype('float32')\n",
    "targets_test = data['y_test'][idxs_test].astype('int32')\n",
    "t_test = onehot(targets_test, num_classes)\n",
    "\n",
    "print(\"training set dim(%i, %i).\" % x_train.shape)\n",
    "print(\"validation set dim(%i, %i).\" % x_valid.shape)\n",
    "print(\"test set dim(%i, %i).\" % x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot a few MNIST examples\n",
    "def plot_samples(x,title=''):\n",
    "    idx = 0\n",
    "    canvas = np.zeros((28*10, 10*28))\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_train[idx].reshape((28, 28))\n",
    "            idx += 1\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(canvas, cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_samples(x_train[:100],title='MNIST handwritten digits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for lab3, we define the helper functions for calculating the lowerbound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defined a couple of helper functions\n",
    "c = - 0.5 * math.log(2*math.pi)\n",
    "def log_bernoulli(x, p, eps=0.0):\n",
    "    p = T.clip(p, eps, 1.0 - eps)\n",
    "    return -T.nnet.binary_crossentropy(p, x)\n",
    "\n",
    "def kl_normal2_stdnormal(mean, log_var):\n",
    "    return -0.5*(1 + log_var - mean**2 - T.exp(log_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model should now be modified to condition on the labeled data $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer, DenseLayer, get_output, get_all_params, ConcatLayer\n",
    "from lasagne.nonlinearities import rectify, sigmoid\n",
    "from samplelayer import SimpleSampleLayer\n",
    "\n",
    "num_features = x_train.shape[-1]\n",
    "num_latent_z = 2\n",
    "\n",
    "#MODEL SPECIFICATION\n",
    "\n",
    "#ENCODER\n",
    "l_in_x = InputLayer(shape=(None, num_features))\n",
    "l_in_y = InputLayer(shape=(None, num_classes))\n",
    "l_enc = ConcatLayer([l_in_x, l_in_y])\n",
    "l_enc = DenseLayer(l_enc, num_units=128, nonlinearity=rectify)\n",
    "l_muq = DenseLayer(l_enc, num_units=num_latent_z, nonlinearity=None)     #mu(x)\n",
    "l_logvarq = DenseLayer(l_enc, num_units=num_latent_z, nonlinearity=lambda x: T.clip(x,-10,10)) #logvar(x), \n",
    "l_z = SimpleSampleLayer(mean=l_muq, log_var=l_logvarq) #sample a latent representation z \\sim q(z|x) = N(mu(x),logvar(x))\n",
    "#we split the in two parts to allow sampling from the decoder model separately\n",
    "#DECODER\n",
    "l_in_z = InputLayer(shape=(None, num_latent_z))\n",
    "l_dec = ConcatLayer([l_in_z, l_in_y])\n",
    "l_dec = DenseLayer(l_dec, num_units=128, nonlinearity=rectify) \n",
    "l_mux = DenseLayer(l_dec, num_units=num_features, nonlinearity=sigmoid)  #reconstruction of input using a sigmoid output since mux \\in [0,1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective functions are defined similarly to lab52."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "sym_x = T.matrix('x')\n",
    "sym_y = T.matrix('y')\n",
    "sym_z = T.matrix('z')\n",
    "\n",
    "z_train, muq_train, logvarq_train = get_output([l_z,l_muq,l_logvarq],{l_in_x:sym_x, l_in_y:sym_y},deterministic=False)\n",
    "mux_train = get_output(l_mux,{l_in_z:z_train, l_in_y:sym_y},deterministic=False)\n",
    "\n",
    "z_eval, muq_eval, logvarq_eval = get_output([l_z,l_muq,l_logvarq],{l_in_x:sym_x, l_in_y:sym_y},deterministic=True)\n",
    "mux_eval = get_output(l_mux,{l_in_z:z_eval, l_in_y:sym_y},deterministic=True)\n",
    "\n",
    "mux_sample = get_output(l_mux,{l_in_z:sym_z, l_in_y:sym_y},deterministic=True)\n",
    "\n",
    "#define the cost function\n",
    "def LogLikelihood(mux,x,muq,logvarq):\n",
    "    log_px_given_z = log_bernoulli(x, mux, eps=1e-6).sum(axis=1).mean() #note that we sum the latent dimension and mean over the samples\n",
    "    KL_qp = kl_normal2_stdnormal(muq, logvarq).sum(axis=1).mean()\n",
    "    LL = log_px_given_z - KL_qp\n",
    "    return LL, log_px_given_z, KL_qp\n",
    "\n",
    "LL_train, logpx_train, KL_train = LogLikelihood(mux_train, sym_x, muq_train, logvarq_train)\n",
    "LL_eval, logpx_eval, KL_eval = LogLikelihood(mux_eval, sym_x, muq_eval, logvarq_eval)\n",
    "\n",
    "all_params = get_all_params([l_z,l_mux],trainable=True)\n",
    "\n",
    "# Let Theano do its magic and get all the gradients we need for training\n",
    "all_grads = T.grad(-LL_train, all_params)\n",
    "\n",
    "# Set the update function for parameters. The Adam optimizer works really well with VAEs.\n",
    "updates = lasagne.updates.adam(all_grads, all_params, learning_rate=1e-3)\n",
    "\n",
    "f_train = theano.function(inputs=[sym_x, sym_y],\n",
    "                          outputs=[LL_train, logpx_train, KL_train],\n",
    "                          updates=updates)\n",
    "\n",
    "f_eval = theano.function(inputs=[sym_x, sym_y],\n",
    "                         outputs=[LL_eval, logpx_eval, KL_eval])\n",
    "\n",
    "f_z = theano.function(inputs=[sym_x, sym_y],\n",
    "                         outputs=[z_eval])\n",
    "\n",
    "f_sample = theano.function(inputs=[sym_z, sym_y],\n",
    "                         outputs=[mux_sample])\n",
    "\n",
    "f_recon = theano.function(inputs=[sym_x, sym_y],\n",
    "                         outputs=[mux_eval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "samples_to_process = 1e4\n",
    "val_interval = 5e2\n",
    " \n",
    "LL_train, KL_train, logpx_train = [],[],[]\n",
    "LL_valid, KL_valid, logpx_valid = [],[],[]\n",
    "samples_processed = 0\n",
    "plt.figure(figsize=(12, 24))\n",
    "valid_samples_processed = []\n",
    "\n",
    "try:\n",
    "    while samples_processed < samples_to_process:\n",
    "        _LL_train, _KL_train, _logpx_train = [],[],[]\n",
    "        idxs = np.random.choice(range(x_train.shape[0]), size=(batch_size), replace=False)  \n",
    "        x_batch = x_train[idxs]\n",
    "        t_batch = t_train[idxs]\n",
    "        out = f_train(x_batch, t_batch)\n",
    "        samples_processed += batch_size\n",
    "           \n",
    "        if samples_processed % val_interval == 0:\n",
    "            valid_samples_processed += [samples_processed]\n",
    "            out = f_eval(x_train, t_train)\n",
    "            LL_train += [out[0]] \n",
    "            logpx_train += [out[1]]\n",
    "            KL_train += [out[2]]\n",
    "            \n",
    "            out = f_eval(x_valid, t_valid)\n",
    "            LL_valid += [out[0]]\n",
    "            logpx_valid += [out[1]]\n",
    "            KL_valid += [out[2]]\n",
    "            \n",
    "            z_eval = f_z(x_valid, t_valid)[0]\n",
    "            x_sample = f_sample(np.random.normal(size=(100, num_latent_z)).astype('float32'), onehot(np.random.randint(0, num_classes, size=100), num_classes))[0]\n",
    "            x_recon = f_recon(x_valid, t_valid)[0]\n",
    "            \n",
    "            plt.subplot(num_classes+2,2,1)\n",
    "            plt.legend(['LL', 'log(p(x))'], loc=2)\n",
    "            plt.xlabel('Updates')\n",
    "            plt.plot(valid_samples_processed, LL_train, color=\"black\")\n",
    "            plt.plot(valid_samples_processed, logpx_train, color=\"red\")\n",
    "            plt.plot(valid_samples_processed, LL_valid, color=\"black\", linestyle=\"--\")\n",
    "            plt.plot(valid_samples_processed, logpx_valid, color=\"red\", linestyle=\"--\")\n",
    "            plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "            plt.grid('on')\n",
    "            \n",
    "            plt.subplot(num_classes+2,2,2)\n",
    "            plt.cla()\n",
    "            plt.xlabel('z0'), plt.ylabel('z1')\n",
    "            color = iter(plt.get_cmap('brg')(np.linspace(0, 1.0, num_classes)))\n",
    "            for i in range(num_classes):\n",
    "                clr = next(color)\n",
    "                plt.scatter(z_eval[targets_valid==i, 0], z_eval[targets_valid==i, 1], c=clr, s=5., lw=0, marker='o', )\n",
    "            plt.grid('on')\n",
    "            \n",
    "            \n",
    "            plt.subplot(num_classes+2,2,3)\n",
    "            plt.legend(['KL(q||p)'])\n",
    "            plt.xlabel('Updates')\n",
    "            plt.plot(valid_samples_processed, KL_train, color=\"blue\")\n",
    "            plt.plot(valid_samples_processed, KL_valid, color=\"blue\", linestyle=\"--\")\n",
    "            plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "            plt.grid('on')\n",
    "            \n",
    "            plt.subplot(num_classes+2,2,4)\n",
    "            plt.cla()\n",
    "            plt.title('Samples')\n",
    "            plt.axis('off')\n",
    "            idx = 0\n",
    "            canvas = np.zeros((28*10, 10*28))\n",
    "            for i in range(10):\n",
    "                for j in range(10):\n",
    "                    canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_sample[idx].reshape((28, 28))\n",
    "                    idx += 1\n",
    "            plt.imshow(canvas, cmap='gray')\n",
    "            \n",
    "            c=0\n",
    "            for k in range(5, 5 + num_classes*2, 2):\n",
    "                plt.subplot(num_classes+2,2,k)\n",
    "                plt.cla()\n",
    "                plt.title('Inputs for %i' % c)\n",
    "                plt.axis('off')\n",
    "                idx = 0\n",
    "                canvas = np.zeros((28*10, 10*28))\n",
    "                for i in range(10):\n",
    "                    for j in range(10):\n",
    "                        canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_valid[targets_valid==c][idx].reshape((28, 28))\n",
    "                        idx += 1\n",
    "                plt.imshow(canvas, cmap='gray')\n",
    "\n",
    "                plt.subplot(num_classes+2,2,k+1)\n",
    "                plt.cla()\n",
    "                plt.title('Reconstructions for %i' % c)\n",
    "                plt.axis('off')\n",
    "                idx = 0\n",
    "                canvas = np.zeros((28*10, 10*28))\n",
    "                for i in range(10):\n",
    "                    for j in range(10):\n",
    "                        canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_recon[targets_valid==c][idx].reshape((28, 28))\n",
    "                        idx += 1\n",
    "                plt.imshow(canvas, cmap='gray')\n",
    "                c += 1\n",
    "            \n",
    "            plt.savefig(\"out53.png\")\n",
    "            display(Image(filename=\"out53.png\"))\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Analyzing the supervised VAE (Optional)\n",
    "1. Test different network structures and evaluate the performance of the model.\n",
    "2. Investigate the latent space $z$ of the supervised VAE by performing a random walk and generating samples in the same way as [Kingma et al.](https://arxiv.org/abs/1406.5298)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST: Semi-supervised VAE (for the very ambitous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the semi-supervised modeling, we generate a labeled subset of our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bernoullisample(x):\n",
    "    return np.random.binomial(1,x,size=x.shape).astype(theano.config.floatX)\n",
    "\n",
    "data = np.load('../week1/mnist.npz')\n",
    "num_classes = 5\n",
    "idxs_train = []\n",
    "idxs_valid = []\n",
    "idxs_test = []\n",
    "for i in range(num_classes):\n",
    "    idxs_train += np.where(data['y_train'] == i)[0].tolist()\n",
    "    idxs_valid += np.where(data['y_valid'] == i)[0].tolist()\n",
    "    idxs_test += np.where(data['y_test'] == i)[0].tolist()\n",
    "\n",
    "x_train = bernoullisample(data['X_train'][idxs_train]).astype('float32')\n",
    "targets_train = data['y_train'][idxs_train].astype('int32') # Since this is unsupervised, the targets are only used for validation.\n",
    "x_train, targets_train = shuffle(x_train, targets_train, random_state=1234)\n",
    "t_train = onehot(targets_train, num_classes)\n",
    "\n",
    "x_valid = bernoullisample(data['X_valid'][idxs_valid]).astype('float32')\n",
    "targets_valid = data['y_valid'][idxs_valid].astype('int32')\n",
    "t_valid = onehot(targets_valid, num_classes)\n",
    "\n",
    "x_test = bernoullisample(data['X_test'][idxs_test]).astype('float32')\n",
    "targets_test = data['y_test'][idxs_test].astype('int32')\n",
    "t_test = onehot(targets_test, num_classes)\n",
    "\n",
    "print(\"training set dim(%i, %i).\" % x_train.shape)\n",
    "print(\"validation set dim(%i, %i).\" % x_valid.shape)\n",
    "print(\"test set dim(%i, %i).\" % x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a subset of labeled data points\n",
    "\n",
    "num_labeled = 2 # You decide on the size of the fraction...\n",
    "\n",
    "idxs_train_l = []\n",
    "for i in range(num_classes):\n",
    "    idxs = np.where(targets_train == i)[0]\n",
    "    idxs_train_l += np.random.choice(idxs, size=num_labeled).tolist()\n",
    "\n",
    "x_train_l = x_train[idxs_train_l]\n",
    "t_train_l = onehot(targets_train[idxs_train_l], num_classes)\n",
    "print(\"labeled training set dim(%i, %i).\" % x_train_l.shape)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "for i in range(num_classes*num_labeled):\n",
    "    im = x_train_l[i].reshape((28, 28))\n",
    "    plt.subplot(1, num_classes*num_labeled, i+1)\n",
    "    plt.imshow(im, cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the semi-supervised case, the lower bound is divided into two, so that for the labeled data we have:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(x,y) &= \\int_z \\log p(x,y,z) dz \\\\\n",
    "&\\ge \\int_z q(z|x,y) \\log \\bigg[ \\frac{p(x,y,z)}{q(z|x,y)} \\bigg]dz \\\\\n",
    "&= \\mathbb{E}_{q(z|x,y)} [\\log p(x,y,z) - \\log q(z|x,y)]dz \\equiv -\\mathcal{L}(x,y)\\ ,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where in our case the $p(x,y,z)=p(x|y,z)p(y)p(z)$. For the unlabeled data $y$ is now a latent variable that needs to be marginalized out:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(x,y) &= \\int_y \\int_z \\log p(x,y,z) dzdy \\\\\n",
    "&\\ge \\int_y q(y|x) \\int_z q(z|x,y) \\log \\bigg[ \\frac{p(x,y,z)}{q(y|x)q(z|x,y)} \\bigg]dzdy \\\\\n",
    "&= \\int_y q(y|x) \\int_z q(z|x,y) [\\log p(x,y,z) - \\log q(y|x) - \\log q(z|x,y) ]dzdy \\\\\n",
    "&= \\int_y q(y|x) \\int_z q(z|x,y) [\\log p(x,y,z) - \\log q(z|x,y)]dzdy - \\int_y \\int_z q(y|x)q(z|x,y) \\log q(y|x)dzdy \\\\\n",
    "&= \\int_y q(y|x) \\int_z q(z|x,y) [\\log p(x,y,z) - \\log q(z|x,y)]dzdy - \\int_y q(y|x) \\log q(y|x) \\int_z q(z|x,y)dzdy \\\\\n",
    "&= \\int_y q(y|x) \\int_z q(z|x,y) [\\log p(x,y,z) - \\log q(z|x,y)]dzdy - \\int_y q(y|x) \\log q(y|x)dy \\cdot 1 \\\\\n",
    "&= \\int_y q(y|x) \\int_z q(z|x,y) [\\log p(x,y,z) - \\log q(z|x,y)]dzdy + \\mathcal{H}(q(y|x)) \\\\\n",
    "&= \\sum_y q(y|x) (-\\mathcal{L}(x,y)) + \\mathcal{H}(q(y|x)) \\equiv -\\mathcal{U}(x)\\ .\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We collect the two lowerbounds by:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{J}^\\alpha = \\sum_{x_l,y_l} \\mathcal{L}(x_l, y_l) - \\alpha \\cdot \\log q(y_l|x_l) + \\sum_{x_u, y_u} \\mathcal{U}(x_u, y_u) \\ ,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the right hand-side is the addition of the cross-entropy scaled by an alpha constant which is equivalent to the fraction between unlabeled and labeled data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following you are provided with a code snippet for computing the lowerbound for the semi-supervised VAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer, DenseLayer, get_output, get_all_params, ConcatLayer\n",
    "from lasagne.nonlinearities import rectify, sigmoid, softmax\n",
    "from samplelayer import SimpleSampleLayer\n",
    "\n",
    "num_features = x_train.shape[-1]\n",
    "num_latent_z = 16\n",
    "\n",
    "#MODEL SPECIFICATION\n",
    "l_in_x = InputLayer(shape=(None, num_features))\n",
    "l_in_y = InputLayer(shape=(None, num_classes))\n",
    "\n",
    "#Classifier\n",
    "l_y = DenseLayer(l_in_x, num_units=128, nonlinearity=rectify)\n",
    "l_y = DenseLayer(l_y, num_units=num_classes, nonlinearity=softmax)\n",
    "\n",
    "#ENCODER\n",
    "l_enc = ConcatLayer([l_in_x, l_in_y])\n",
    "l_enc = DenseLayer(l_enc, num_units=128, nonlinearity=rectify)\n",
    "l_muq = DenseLayer(l_enc, num_units=num_latent_z, nonlinearity=None)     #mu(x)\n",
    "l_logvarq = DenseLayer(l_enc, num_units=num_latent_z, nonlinearity=lambda x: T.clip(x,-10,10)) #logvar(x), \n",
    "l_z = SimpleSampleLayer(mean=l_muq, log_var=l_logvarq) #sample a latent representation z \\sim q(z|x) = N(mu(x),logvar(x))\n",
    "#we split the in two parts to allow sampling from the decoder model separately\n",
    "#DECODER\n",
    "l_in_z = InputLayer(shape=(None, num_latent_z))\n",
    "l_dec = ConcatLayer([l_in_z, l_in_y])\n",
    "l_dec = DenseLayer(l_dec, num_units=128, nonlinearity=rectify)\n",
    "l_mux = DenseLayer(l_dec, num_units=num_features, nonlinearity=sigmoid)  #reconstruction of input using a sigmoid output since mux \\in [0,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.objectives import categorical_crossentropy, categorical_accuracy\n",
    "\n",
    "sym_x_l = T.matrix('x_l')\n",
    "sym_x = T.matrix('x')\n",
    "sym_y = T.matrix('y')\n",
    "sym_z = T.matrix('z')\n",
    "\n",
    "y_eval = get_output(l_y, {l_in_x:sym_x}, deterministic=True)\n",
    "z_eval = get_output(l_z,{l_in_x:sym_x, l_in_y:sym_y},deterministic=True)\n",
    "\n",
    "#define the cost function for labeled data points\n",
    "y_train_l = get_output(l_y, {l_in_x:sym_x_l}, deterministic=False)\n",
    "z_train_l, muq_train_l, logvarq_train_l = get_output([l_z,l_muq,l_logvarq],{l_in_x:sym_x_l, l_in_y:sym_y},deterministic=False)\n",
    "mux_train_l = get_output(l_mux,{l_in_z:z_train_l, l_in_y:sym_y},deterministic=False)\n",
    "\n",
    "log_px_l = log_bernoulli(sym_x_l, mux_train_l, eps=1e-6).sum(axis=-1)\n",
    "KL_qp_l = kl_normal2_stdnormal(muq_train_l, logvarq_train_l).sum(axis=-1)\n",
    "log_qy_l = T.sum(sym_y * T.log(y_train_l+1e-8), axis=1)\n",
    "py_l = softmax(T.zeros((sym_x_l.shape[0], num_classes)))\n",
    "log_py_l = -categorical_crossentropy(py_l, sym_y)\n",
    "alpha = 0.1*(x_train.shape[0]/x_train_l.shape[0])\n",
    "LL_l_eval = T.mean(log_px_l + log_py_l - KL_qp_l)\n",
    "LL_l = T.mean(log_px_l + log_py_l - KL_qp_l + alpha * log_qy_l)\n",
    "\n",
    "\n",
    "#define the cost function for unlabeled data points\n",
    "\n",
    "# For the integrating out approach, we repeat the input matrix x, and construct a target (bs * n_y) x n_y\n",
    "# Example of input and target matrix for a 3 class problem and batch_size=2. 2D tensors of the form\n",
    "#               x_repeat                     t_repeat\n",
    "#  [[x[0,0], x[0,1], ..., x[0,n_x]]         [[1, 0, 0]\n",
    "#   [x[1,0], x[1,1], ..., x[1,n_x]]          [1, 0, 0]\n",
    "#   [x[0,0], x[0,1], ..., x[0,n_x]]          [0, 1, 0]\n",
    "#   [x[1,0], x[1,1], ..., x[1,n_x]]          [0, 1, 0]\n",
    "#   [x[0,0], x[0,1], ..., x[0,n_x]]          [0, 0, 1]\n",
    "#   [x[1,0], x[1,1], ..., x[1,n_x]]]         [0, 0, 1]]\n",
    "t_eye = T.eye(num_classes, k=0)\n",
    "t_u = t_eye.reshape((num_classes, 1, num_classes)).repeat(sym_x.shape[0], axis=1).reshape((-1, num_classes))\n",
    "x_u = sym_x.reshape((1, sym_x.shape[0], sym_x.shape[1])).repeat(num_classes, axis=0).reshape((-1, sym_x.shape[1]))\n",
    "\n",
    "y_train = get_output(l_y, {l_in_x:sym_x}, deterministic=False)\n",
    "z_train, muq_train, logvarq_train = get_output([l_z,l_muq,l_logvarq],{l_in_x:x_u, l_in_y:t_u},deterministic=False)\n",
    "mux_train = get_output(l_mux,{l_in_z:z_train, l_in_y:t_u},deterministic=False)\n",
    "\n",
    "log_px_given_z_u = log_bernoulli(x_u, mux_train, eps=1e-6).sum(axis=-1)\n",
    "KL_qp_u = kl_normal2_stdnormal(muq_train, logvarq_train).sum(axis=-1)\n",
    "py_u = softmax(T.zeros((x_u.shape[0], num_classes)))\n",
    "log_py_u = -categorical_crossentropy(py_u, t_u)\n",
    "LL_u = log_px_given_z_u + log_py_u - KL_qp_u\n",
    "LL_u = LL_u.reshape((num_classes, sym_x.shape[0])).T\n",
    "LL_u = T.sum(y_train * (LL_u - T.log(y_train+1e-8)), axis=-1).mean()\n",
    "\n",
    "LL = LL_u + LL_l\n",
    "\n",
    "eval_acc = categorical_accuracy(y_eval, sym_y).mean()\n",
    "\n",
    "all_params = get_all_params([l_z,l_mux,l_y],trainable=True)\n",
    "\n",
    "# Let Theano do its magic and get all the gradients we need for training\n",
    "all_grads = T.grad(-LL, all_params)\n",
    "\n",
    "# Set the update function for parameters. The Adam optimizer works really well with VAEs.\n",
    "updates = lasagne.updates.adam(all_grads, all_params, learning_rate=3e-5)\n",
    "\n",
    "f_train = theano.function(inputs=[sym_x, sym_x_l, sym_y],\n",
    "                          outputs=[LL],\n",
    "                          updates=updates)\n",
    "\n",
    "f_train_eval = theano.function(inputs=[sym_x, sym_x_l, sym_y],\n",
    "                          outputs=[LL_l_eval, LL_u, KL_qp_l.mean(), KL_qp_u.mean()])\n",
    "\n",
    "f_eval = theano.function(inputs=[sym_x, sym_y],\n",
    "                         outputs=[eval_acc])\n",
    "\n",
    "f_z = theano.function(inputs=[sym_x, sym_y],\n",
    "                         outputs=[z_eval])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "samples_to_process = 9e4\n",
    "val_interval = 5e2\n",
    " \n",
    "LL_l_train, LL_u_train, KL_l_train, KL_u_train = [], [], [], []\n",
    "acc_valid = []\n",
    "samples_processed = 0\n",
    "plt.figure(figsize=(12, 6))\n",
    "valid_samples_processed = []\n",
    "\n",
    "try:\n",
    "    while samples_processed < samples_to_process:\n",
    "        idxs = np.random.choice(range(x_train.shape[0]), size=(batch_size), replace=False)  \n",
    "        x_batch = x_train[idxs]\n",
    "        t_batch = t_train[idxs]\n",
    "        out = f_train(x_batch, x_train_l, t_train_l)\n",
    "        samples_processed += batch_size\n",
    "           \n",
    "        if samples_processed % val_interval == 0:\n",
    "            valid_samples_processed += [samples_processed]\n",
    "            out = f_train_eval(x_train, x_train_l, t_train_l)\n",
    "            LL_l_train += [out[0]]\n",
    "            LL_u_train += [out[1]]\n",
    "            KL_l_train += [out[2]]\n",
    "            KL_u_train += [out[3]]\n",
    "            \n",
    "            out = f_eval(x_valid, t_valid)\n",
    "            acc_valid += [out[0]]\n",
    "            \n",
    "            #z_eval = f_z(x_valid, t_valid)[0]\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.legend(['acc. eval'], loc=2)\n",
    "            plt.xlabel('Updates')\n",
    "            plt.plot(valid_samples_processed, acc_valid, color=\"black\")\n",
    "            plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "            plt.grid('on')\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.legend(['LL_l train', 'LL_u train'], loc=2)\n",
    "            plt.xlabel('Updates')\n",
    "            plt.plot(valid_samples_processed, LL_l_train, color=\"black\")\n",
    "            plt.plot(valid_samples_processed, LL_u_train, color=\"red\")\n",
    "            plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "            plt.grid('on')\n",
    "            \n",
    "            \n",
    "            \n",
    "            plt.savefig(\"out53.png\")\n",
    "            display(Image(filename=\"out53.png\"))\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Analyzing the semi-supervised VAE (Optional)\n",
    "1. Understand how the above code works.\n",
    "2. Test different network structures and fractions of labeled/unlabeled data and evaluate the performance of the model.\n",
    "3. What happens when you remove the labeled lowerbound from the objective.\n",
    "4. What does the scaling of the labeled cross-entropy do to the training.\n",
    "3. Try to benchmark this model against other semi-supervised approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
