{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division, print_function\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "def plot_decision_boundary(pred_func, X, y):\n",
    "    #from https://github.com/dennybritz/nn-from-scratch/blob/master/nn-from-scratch.ipynb\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    \n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    yy = yy.astype('float32')\n",
    "    xx = xx.astype('float32')\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])[:,0]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.figure()\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
    "\n",
    "def onehot(t, num_classes):\n",
    "    out = np.zeros((t.shape[0], num_classes))\n",
    "    for row, col in enumerate(t):\n",
    "        out[row, col] = 1\n",
    "    return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks 101\n",
    "In this notebook you will implement a simple neural network in Lasagne utilizing the automatic differentiation engine of Theano. We assume that you are already familiar with backpropagation (if not please cf. [Andrej Karpathy](http://cs.stanford.edu/people/karpathy/) or [Michael Nielsen](http://neuralnetworksanddeeplearning.com/chap2.html).\n",
    "We'll not spend much time on how Theano works, but you can go through [this short tutorial](http://nbviewer.jupyter.org/github/craffel/theano-tutorial/blob/master/Theano%20Tutorial.ipynb) if you are interested).\n",
    "\n",
    "Additionally, for the ambitious people we have previously made an assignment where you will implement both the forward and backpropagation in a neural network by hand, https://github.com/DTU-deeplearning/day1-NN/blob/master/exercises_1.ipynb. By following this you will get a thorough understanding of the deep learning basics.\n",
    "\n",
    "In this exercise we'll start by defining a logistic regression model in Lasagne/Theano. Some details of Theano can be a bit confusing. For now you should pay most attention to the high-level network construction in Lasagne. We'll initially start with a simple 2-D and 2-class classification problem where the class decision boundary can be visualized. Initially we show that logistic regression can only separate classes linearly. Adding a non-linear hidden layer to the algorithm permits nonlinear class separation. If time permits we'll continue on to implement a fully connected neural network to classify the (in)famous MNIST dataset consisting of images of hand written digits. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem \n",
    "We'll initally demonstrate that MLPs can classify non-linear problems whereas simple logistic regression cannot. For ease of visualization and computational speed we'll initially experiment on the simple 2D half-moon dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a dataset and plot it\n",
    "np.random.seed(0)\n",
    "num_samples = 300\n",
    "\n",
    "X, y = sklearn.datasets.make_moons(num_samples, noise=0.20)\n",
    "\n",
    "X_tr = X[:100].astype('float32')\n",
    "X_val = X[100:200].astype('float32')\n",
    "X_te = X[200:].astype('float32')\n",
    "\n",
    "y_tr = y[:100].astype('int32')\n",
    "y_val = y[100:200].astype('int32')\n",
    "y_te = y[200:].astype('int32')\n",
    "\n",
    "plt.scatter(X_tr[:,0], X_tr[:,1], s=40, c=y_tr, cmap=plt.cm.BuGn)\n",
    "\n",
    "print(\"Shape of the input matrix dim(%i x %i).\" % X.shape)\n",
    "print(\"Shape of the target matrix dim(%i).\" % y.shape)\n",
    "\n",
    "num_features = X_tr.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Logistic Regression to \"Deep Learning\" in Lasagne\n",
    "The code implements logistic regression in Lasagne. In section __Assignments Half Moon__ you are asked to modify the code into a neural network. \n",
    "\n",
    "The building blocks of lasagne are the layers. To get started, the most frequently used layers are the DenseLayer and the InputLayer. \n",
    "\n",
    "The [InputLayer](http://lasagne.readthedocs.io/en/latest/modules/layers/input.html) is a \"special\" layer that lets you input data to the network. The InputLayer is initialized with a tuple specifying the shape of the input data. Note that it is common to provide ``None`` for the first dimension which allows you to vary the batch size at runtime. \n",
    "\n",
    "The [DenseLayer](http://lasagne.readthedocs.io/en/latest/modules/layers/dense.html) implements the computation: \n",
    "\n",
    "$$y = nonlinearity(xW + b)$$\n",
    "\n",
    "where $x$ is the layer input, $y$ is the layer output and $\\{W, b\\}$ are the layer parameters. The DenseLayer is initialized with a pointer to the previous layer, the desired number of units in the layer and the nonlinearity. \n",
    "x has shape ```[batchsize, num_features]```. From this we can infer the size of ```W``` as ```[num_features, num_units]``` and b as ```[num_units]```. y is then ```[batch_size, num_units]```.\n",
    "\n",
    "\n",
    "A layer in Lasagne does the following:\n",
    "1. Given the shape of the input $x$ and the number of units in the layer, Lasagne infers the shapes of $W$ and $b$ and keep track of the layer parameters.\n",
    "2. Computes $y = nonlinearity(xW + b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.updates import sgd # stochastic gradient descent optimization algorithm.\n",
    "from lasagne.nonlinearities import leaky_rectify, softmax, tanh\n",
    "from lasagne.layers import InputLayer, DenseLayer\n",
    "\n",
    "\n",
    "#MODEL SPECIFICATION\n",
    "l_in = InputLayer(shape=(None, num_features))\n",
    "#INSERT HIDDEN LAYER HERE\n",
    "#l = DenseLayer(incoming=l,.....\n",
    "l_out = DenseLayer(incoming=l_in, num_units=2, nonlinearity=softmax, name='outputlayer') \n",
    "#We use two output units since we have two classes. The softmax function ensures that the the class probabilities sum to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have built the network we can use the helper functions of lasagne to \n",
    "\n",
    "1. Build the computational graph: __[lasagne.layers.get_output](http://lasagne.readthedocs.io/en/latest/modules/layers/helper.html#lasagne.layers.get_output)__ . The ``deterministic`` flag tells lasagne if we are in training mode or evaluation mode. When you build more complicated networks this is very important to remember! (Two important layers that behave differently in training mode and evaluation mode are the [DropoutLayer](http://lasagne.readthedocs.io/en/latest/modules/layers/noise.html#lasagne.layers.DropoutLayer) and the [BatchNormalizationLayer](http://lasagne.readthedocs.io/en/latest/modules/layers/normalization.html?highlight=batchnorm#lasagne.layers.BatchNormLayer)). Building the computational graph gives us the forward-pass of the network.  \n",
    "2. Collect the network parameters: __[lasagne.layers.get_all_params](http://lasagne.readthedocs.io/en/latest/modules/layers/helper.html#lasagne.layers.get_all_params)__ (Note the trainable flag which will only return parameters that are trainable. You'll get errors if your are using batchnorm and you forget this)\n",
    "\n",
    "Note that all the helper functions are called with the output layer or a list of output layers if you have multiple output layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sym_x = T.matrix('X') # a symbolic variable taking on the value of an input batch.\n",
    "sym_t = T.ivector('target') # a symbolic variable taking on the value of the target batch.\n",
    "\n",
    "# Get network output\n",
    "train_out = lasagne.layers.get_output(l_out, {l_in: sym_x}, deterministic=False)\n",
    "eval_out = lasagne.layers.get_output(l_out, {l_in: sym_x}, deterministic=True)\n",
    "\n",
    "# Get list of all trainable parameters in the network.\n",
    "all_params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "\n",
    "# print shapes of all the parameters in the network.\n",
    "for p in all_params:\n",
    "    print(p, p.get_value().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``train_out`` will be a symbolic variable representing the network output. Using ``train_out`` we  can define the [crossentropy error](http://deeplearning.net/software/theano/library/tensor/nnet/nnet.html#tensor.nnet.categorical_crossentropy) used for training the network.\n",
    "We ```mean``` over all the samples in the mini-batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost_train = T.nnet.categorical_crossentropy(train_out, sym_t).mean()\n",
    "cost_eval = T.nnet.categorical_crossentropy(eval_out, sym_t).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train a neural network we update the parameters in direction of the negative gradient w.r.t the cost.\n",
    "We can use ``T.grad`` to get the gradients for all parameters in the network w.r.t ``cost_train``.\n",
    "Imagine that ```cost_train``` is a function and we want to go downhill. We go downhill by changing the value of the parameters in direction of the negative gradient. \n",
    "\n",
    "Finally we can use __[lasagne.updates.sgd](http://lasagne.readthedocs.io/en/latest/modules/updates.html#lasagne.updates.sgd)__ to calculate the stochastic gradient descent (SGD) update rule for each parameter in the network. ``updates`` is a dictionary of the parameter update rules.\n",
    "\n",
    "Here's a small animation of [different optimizers doing](http://lasagne.readthedocs.io/en/latest/modules/updates.html) gradient descent: http://imgur.com/a/Hqolp . E.g why saddle points might be difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let Theano do its magic and get all the gradients we need for training. Essentially T.grad does backprop i.e. get the \n",
    "# gradient of cost_train w.r.t. the parameters.\n",
    "all_grads = T.grad(cost_train, all_params)\n",
    "\n",
    "# Set the update function for parameters \n",
    "# you might want to experiment with more advanded update schemes like rmsprob, adadelta etc.\n",
    "updates = lasagne.updates.sgd(all_grads, all_params, learning_rate=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to compile Theano functions for the network. For theano functions we need to specify which inputs the function should take. For our network that is ``sym_x``: the input data and ``sym_t``: the targets. Secondly we need to specify which outputs we want the network to return. In our case that is the cross-entropy cost and the network output.\n",
    "\n",
    "When we compile ``f_train`` we additionally give the update dictionary as input. This tells Theano to update the network parameters with the update rules everytime we call ``f_train``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_eval = theano.function(inputs=[sym_x, sym_t],\n",
    "                         outputs=[cost_eval, eval_out])\n",
    "\n",
    "f_train = theano.function(inputs=[sym_x, sym_t],\n",
    "                          outputs=[cost_train, eval_out],\n",
    "                          updates=updates)\n",
    "\n",
    "#now you have three functions. \n",
    "# f_train(X,y) -> cost, y_pred which will update the parameters using backprop each time you call it, only use this on the training data!\n",
    "# f_test(X,y) -> cost, y_pred which only calculates the forward pass\n",
    "\n",
    "#This us just a helper function for plotting the decision boundaries between the two classes\n",
    "f_pred = theano.function(inputs=[sym_x],\n",
    "                         outputs=eval_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "plot_decision_boundary(lambda x: f_pred(x), X_val,y_val)\n",
    "plt.title(\"Untrained Classifier\")\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "train_cost, val_cost = [],[]\n",
    "for e in range(num_epochs):\n",
    "    out = f_train(X_tr,y_tr)\n",
    "    #out = [cost, y_pred]\n",
    "    train_cost += [out[0]]\n",
    "    \n",
    "    out = f_eval(X_val,y_val)\n",
    "    val_cost += [out[0]]\n",
    "\n",
    "    if e % 100 == 0:\n",
    "        print(\"Epoch %i, Train Cost: %0.3f\\tVal Cost: %0.3f\"%(e, train_cost[-1],val_cost[-1]))\n",
    "    \n",
    "    \n",
    "out = f_eval(X_te,y_te)\n",
    "test_cost = out[0]\n",
    "print(\"\\nTest Cost: %0.3f\"%(test_cost))\n",
    "\n",
    "plot_decision_boundary(lambda x: f_pred(x), X_te, y_te)\n",
    "plt.title(\"Trained Classifier\")\n",
    "\n",
    "epoch = np.arange(len(train_cost))\n",
    "plt.figure()\n",
    "plt.plot(epoch,train_cost,'r',epoch,val_cost,'b')\n",
    "plt.legend(['Train Loss','Val Loss'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Loss')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments Half Moon\n",
    "\n",
    " 1) A linear logistic classifier is only able to create a linear decision boundary. Change the Logistic classifier into a (non-linear) neural network by inserting a dense hidden layer between the input and output layers of the model\n",
    " \n",
    " 2) Experiment with multiple hidden layers or more / less hidden units. What happens to the decision boundary?\n",
    " \n",
    " 3) Overfitting: When increasing the number of hidden layers / units the neural network will fit the training data better by creating a highly nonlinear decision boundary. If the model is too complex it will often generalize poorly to new data (validation and test set). Can you observe this from the training and validation errors? \n",
    " \n",
    " 4) We applied the stochastic gradient descent algorithm for parameter updates. This is usually slow to converge and more sophisticated pseudo-second-order methods usually works better. Try changing the optimizer to [adam or adamax](http://lasagne.readthedocs.io/en/latest/modules/updates.html) (lasagne.updates.adam, lasagne.updates.adamax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Exercise\n",
    "MNIST is a dataset that is often used for benchmarking. The MNIST dataset consists of 70,000 images of handwritten digits from 0-9. The dataset is split into 50,000 images for training, 10,000 images for validation and 10,000 images for testing. The images are 28x28 pixels, where each pixel represents a normalised value between 0-255 (0=black and 255=white).\n",
    "\n",
    "### Permutation invariance...\n",
    "We use a feed-forward neural network to classify the 28x28 MNIST images. ``num_features`` is therefore 28x28=784, hence we represent each image as a vector. The ordering of the pixels in the vector does not matter, so we could permutate all images using the same permuatation and still get the same performance. (You are of course encouraged to try this by ``numpy.random.permutation`` to get a random permutation :)). This task is therefore called the _permutation invariant_ MNIST. Obviously this throws away a lot of structure in the data. Next week we'll fix this with the convolutional neural network that encodes prior knowledge about data that has either spatial or temporal structure.\n",
    "\n",
    "### Ballpark estimates of hyperparameters\n",
    "__Optimizers:__\n",
    "    1. SGD + Momentum: learning rate 1.0 - 0.1 \n",
    "    2. ADAM: learning rate 3*1e-4 - 1e-5\n",
    "    3. RMSPROP: somewhere between SGD and ADAM\n",
    "\n",
    "__Regularization:__\n",
    "    1. Dropout. Dropout rate 0.1-0.5.\n",
    "    2. L2/L1 regularization.  http://lasagne.readthedocs.io/en/latest/modules/regularization.html . We don't use this that often but 1e-4  -  1e-8.\n",
    "    \n",
    "    3. Batchnorm: Batchnorm also act as a regularizer\n",
    "    \n",
    "__Parameter initialization__\n",
    "    Parameter initialization is extremely important. [Lasagne has a lot of different units](http://lasagne.readthedocs.io/en/latest/modules/init.html). Often used initializer use\n",
    "    1. He\n",
    "    2. Glorot\n",
    "    3. Uniform or Normal with small scale. (0.1 - 0.01)\n",
    "    4. Orthogonal (I find that this works very well for RNNs)\n",
    "\n",
    "Bias is nearly always initialized to zero. \n",
    "\n",
    "__Number of hidden units and network structure__\n",
    "   Probably as big network as possible and then apply regularization. You'll have to experiment :). One rarely goes below 512 units for feedforward networks unless you are training on CPU...\n",
    "   There is some research into stochastic depth networks: https://arxiv.org/pdf/1603.09382v2.pdf, but in general this is trial and error. \n",
    "\n",
    "__Nonlinearity__: [The most commonly used nonliearities are](http://lasagne.readthedocs.io/en/latest/modules/nonlinearities.html)\n",
    "    \n",
    "    1. ReLU\n",
    "    2. Leaky ReLU.\n",
    "    3. Elu\n",
    "    3. Sigmoids are used if your output is binary. It is not used in the hidden layers (anymore). Limits the output between 0 and 1.\n",
    "    4. Softmax is used as output if you have a classification problem. Normalizes the output to 1.\n",
    "\n",
    "\n",
    "See the plot below.\n",
    "\n",
    "__mini-batch size__\n",
    "   Usually people use 16-256. Bigger is not always better. With smaller mini-batch sizes you get more updates and your model might converge faster. Also small batchsizes use less memory  -> you can use a bigger model.\n",
    "\n",
    "Hyperparameters can be found by experience (guessing) or some search procedure. Random search is easy to implement and performs decent: http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf . \n",
    "More advanced search procedures include [SPEARMINT](https://github.com/JasperSnoek/spearmint) and many others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PLOT OF DIFFERENT OUTPUT USNITS\n",
    "x = np.linspace(-6, 6, 100)\n",
    "relu = lambda x: np.maximum(0, x)\n",
    "leaky_relu = lambda x: np.maximum(0, x) + 0.1*np.minimum(0, x) # probably a slow implementation....\n",
    "elu = lambda x: (x > 0)*x + (1 - (x > 0))*(np.exp(x) - 1) \n",
    "sigmoid = lambda x: (1+np.exp(-x))**(-1)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(x, relu(x), label='ReLU', lw=2)\n",
    "plt.plot(x, leaky_relu(x), label='Leaky ReLU',lw=2)\n",
    "plt.plot(x, elu(x), label='Elu', lw=2)\n",
    "plt.plot(x, sigmoid(x), label='Sigmoid',lw=2)\n",
    "plt.legend(loc=2, fontsize=16)\n",
    "plt.title('Non-linearities', fontsize=20)\n",
    "plt.ylim([-2, 5])\n",
    "plt.xlim([-6, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "First let's load the MNIST dataset and plot a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#To speed up training we'll only work on a subset of the data\n",
    "data = np.load('mnist.npz')\n",
    "num_classes = 10\n",
    "x_train = data['X_train'][:1000].astype('float32')\n",
    "targets_train = data['y_train'][:1000].astype('int32')\n",
    "\n",
    "x_valid = data['X_valid'][:500].astype('float32')\n",
    "targets_valid = data['y_valid'][:500].astype('int32')\n",
    "\n",
    "x_test = data['X_test'][:500].astype('float32')\n",
    "targets_test = data['y_test'][:500].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot a few MNIST examples\n",
    "idx = 0\n",
    "canvas = np.zeros((28*10, 10*28))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        canvas[i*28:(i+1)*28, j*28:(j+1)*28] = x_train[idx].reshape((28, 28))\n",
    "        idx += 1\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.axis('off')\n",
    "plt.imshow(canvas, cmap='gray')\n",
    "plt.title('MNIST handwritten digits')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define the model\n",
    "num_class = 10\n",
    "num_features = x_train.shape[1]\n",
    "\n",
    "l_in = InputLayer(shape=(None,num_features))\n",
    "l_hid = DenseLayer(incoming=l_in, num_units=500, nonlinearity=elu)\n",
    "l_out = DenseLayer(incoming=l_hid, num_units=num_class, nonlinearity=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sym_x = T.matrix('sym_x') # a symbolic variable taking on the value of a input batch.\n",
    "sym_t = T.ivector('sym_t') # a symbolic variable taking on the value of the target batch.\n",
    "\n",
    "# Get network output\n",
    "train_out = lasagne.layers.get_output(l_out, sym_x, deterministic=False)\n",
    "eval_out = lasagne.layers.get_output(l_out, sym_x, deterministic=True)\n",
    "\n",
    "\n",
    "# Get list of all trainable parameters in the network.\n",
    "all_params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "\n",
    "cost = T.nnet.categorical_crossentropy(train_out+1e-8, sym_t).mean()\n",
    "# Let Theano do its magic and get all the gradients we need for training\n",
    "all_grads = T.grad(cost, all_params)\n",
    "\n",
    "\n",
    "# Set the update function for parameters \n",
    "# you might wan't to experiment with more advanded update schemes like rmsprob, adadelta etc.\n",
    "updates = lasagne.updates.sgd(all_grads, all_params, learning_rate=0.1)\n",
    "\n",
    "\n",
    "f_eval = theano.function([sym_x],\n",
    "                     eval_out, on_unused_input='warn')\n",
    "\n",
    "f_train = theano.function([sym_x, sym_t],\n",
    "                          [cost],\n",
    "                          updates=updates, on_unused_input='warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Test the forward pass\n",
    "x = np.random.normal(0,1, (45, 28*28)).astype('float32') #dummy data\n",
    "\n",
    "model = lasagne.layers.get_output(l_out, sym_x)\n",
    "out = model.eval({sym_x:x}) #this could also include mask etc if used\n",
    "print(\"l_out dim(%i, %i).\" % out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the training loop.\n",
    "We train the network by calculating the gradient w.r.t the cost function and update the parameters in direction of the negative gradient. \n",
    "\n",
    "\n",
    "When training neural networks you always use mini batches. Instead of calculating the average gradient using the entire dataset you approximate the gradient using a mini-batch of typically 16 to 256 samples. The parameters are updated after each mini batch.\n",
    "\n",
    "We build a loop that iterates over the training data. Remember that the parameters are updated each time ``f_train`` is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from confusionmatrix import ConfusionMatrix\n",
    "batch_size = 100\n",
    "num_epochs = 100\n",
    "learning_rate = 0.1\n",
    "num_samples_train = x_train.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "num_samples_valid = x_valid.shape[0]\n",
    "num_batches_valid = num_samples_valid // batch_size\n",
    "\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "cur_loss = 0\n",
    "loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    #Forward->Backprob->Update params\n",
    "    cur_loss = 0\n",
    "    for i in range(num_batches_train):\n",
    "        idx = range(i*batch_size, (i+1)*batch_size)\n",
    "        x_batch = x_train[idx]\n",
    "        target_batch = targets_train[idx]    \n",
    "        batch_loss = f_train(x_batch,target_batch) #this will do the complete backprob pass\n",
    "        cur_loss += batch_loss[0]\n",
    "    loss += [cur_loss/batch_size]\n",
    "    \n",
    "    confusion_valid = ConfusionMatrix(num_classes)\n",
    "    confusion_train = ConfusionMatrix(num_classes)\n",
    "\n",
    "    for i in range(num_batches_train):\n",
    "        idx = range(i*batch_size, (i+1)*batch_size)\n",
    "        x_batch = x_train[idx]\n",
    "        targets_batch = targets_train[idx]\n",
    "        net_out = f_eval(x_batch)   \n",
    "        preds = np.argmax(net_out, axis=-1) \n",
    "        confusion_train.batch_add(targets_batch, preds)\n",
    "\n",
    "    confusion_valid = ConfusionMatrix(num_classes)\n",
    "    for i in range(num_batches_valid):\n",
    "        idx = range(i*batch_size, (i+1)*batch_size)\n",
    "        x_batch = x_valid[idx]\n",
    "        targets_batch = targets_valid[idx]\n",
    "        net_out = f_eval(x_batch)   \n",
    "        preds = np.argmax(net_out, axis=-1) \n",
    "        \n",
    "        confusion_valid.batch_add(targets_batch, preds)\n",
    "    \n",
    "    train_acc_cur = confusion_train.accuracy()\n",
    "    valid_acc_cur = confusion_valid.accuracy()\n",
    "\n",
    "    train_acc += [train_acc_cur]\n",
    "    valid_acc += [valid_acc_cur]\n",
    "    print(\"Epoch %i : Train Loss %e , Train acc %f,  Valid acc %f \" \\\n",
    "    % (epoch+1, loss[-1], train_acc_cur, valid_acc_cur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epoch = np.arange(len(train_acc))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(epoch,train_acc,'r',epoch,valid_acc,'b')\n",
    "plt.legend(['Train Acc','Val Acc'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More questions (NOTE that before retraining, the network must be reinitialized.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Do you see overfitting? Google overfitting if you don't know how to spot it.\n",
    "2. Regularization is a method to reduce overfitting. Adding noise to your network is a popular method to fight overfitting. Try using Dropout in your network. [Lasagne DropoutLayer](http://lasagne.readthedocs.io/en/latest/modules/layers/noise.html#lasagne.layers.DropoutLayer).\n",
    "3. Alternatively you can regularize your network by penalizing the L2 or L1 norm of the network parameters. [Read the docs for more info](http://lasagne.readthedocs.io/en/latest/modules/regularization.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
