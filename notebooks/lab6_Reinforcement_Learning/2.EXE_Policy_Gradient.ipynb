{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient\n",
    "\n",
    "In this part, we will create an agent that can learn to solve tasks from OpenAI Gym by applying the Policy Gradient method. We will implement the agent with a probabilistic policy, that given a state of the environment, $s$, outputs a probability distribution over available actions, $a$:\n",
    "\n",
    "$$\n",
    "p_\\theta(a|s)\n",
    "$$\n",
    "\n",
    "Since this is a deep learning course, we will model the policy as a neural network with parameters $\\theta$ and train it with gradient descent (now the name 'Policy Gradient' should start to make sense). \n",
    "When the actions are discrete, we can use a network with softmax output. \n",
    "\n",
    "The core idea of training the policy network is simple: *we want to maximize the expected total reward by increasing the probability of good actions and decreasing the probability of bad actions*. \n",
    "\n",
    "The expectation over the (discounted) total reward, $R$, is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[R|\\theta] = \\int p_\\theta({\\bf a}|{\\bf s}) R({\\bf a}) d{\\bf a} \\ ,\n",
    "$$\n",
    "\n",
    "where ${\\bf a} = a_1,\\ldots,a_T$, ${\\bf s}=s_1,\\ldots,s_T$. \n",
    "\n",
    "Then we can use the gradient to maximize the total reward:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\mathbb{E}[R|\\theta] &= \\nabla_\\theta \\int p_\\theta({\\bf a}|{\\bf s}) R({\\bf a}) \\, d{\\bf a} \\\\\n",
    "&= \\int \\nabla_\\theta p_\\theta({\\bf a}|{\\bf s}) R({\\bf a})  \\, d{\\bf a} \\\\\n",
    "&= \\int p_\\theta({\\bf a}|{\\bf s}) \\nabla_\\theta \\log p_\\theta({\\bf a}|{\\bf s}) R({\\bf a}) \\, d{\\bf a} \\\\\n",
    "&= \\mathbb{E}[R({\\bf a}) \\nabla_\\theta \\log p_\\theta({\\bf a}|{\\bf s})]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "using the identity \n",
    "\n",
    "$$\n",
    "\\nabla_\\theta p_\\theta({\\bf a}|{\\bf s}) = p_\\theta({\\bf a}|{\\bf s}) \\nabla_\\theta \\log p_\\theta({\\bf a}|{\\bf s})\n",
    "$$\n",
    "\n",
    "to express the gradient as an average over $p_\\theta({\\bf a},{\\bf s})$.\n",
    "\n",
    "We cannot evaluate the average over roll-outs analytically but we have an environment simulator that when supplied with our current policy $p_\\theta(a|s)$ can return the sequence of action, states and rewards. This allows us to replace the integral by a Monte Carlo average over $V$ roll-outs:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathbb{E}[R|\\theta] \\approx \\frac{1}{V} \\sum_{v=1}^V \\nabla_\\theta \\log p_\\theta({\\bf a}^{(v)}|{\\bf s}^{(v)}) R({\\bf a}^{(v)}) \\ .\n",
    "$$\n",
    "\n",
    "In practice, to reduce the variance of the gradient, instead of $R$, we use the adjusted discounted future reward, also known as the *advantage*, $A$:\n",
    "\n",
    "$$\n",
    "A_t = R_t - b_t \\ ,\n",
    "$$\n",
    "\n",
    "where the *baseline*, $b_t$, is the (discounted) total future reward at timestep $t$ averaged over the $V$ roll-outs:\n",
    "\n",
    "$$\n",
    "b_t = \\frac{1}{V} \\sum_{v=1}^V R_t^{(v)} \\ .\n",
    "$$\n",
    "\n",
    "This way we are always encouraging and discouraging roughly half of the performed actions, which gives us the final gradient estimator:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathbb{E}[R|\\theta] \\approx \\frac{1}{V} \\sum_{v=1}^V \\nabla_\\theta \\log p_\\theta({\\bf a}^{(v)}|{\\bf s}^{(v)}) A({\\bf a}^{(v)})\n",
    "$$\n",
    "\n",
    "And that's it! Please refer to [this blog post](http://karpathy.github.io/2016/05/31/rl/) by Karpathy for more discussion on the Policy Gradient method.\n",
    "\n",
    "--\n",
    "\n",
    "*Note: For simple reinforcement learning problems, like the one we will address here, there are simpler methods that work just fine. However, the Policy Gradient method has been shown to also work well for complex problems with high dimensional inputs and many parameters, where simple methods are inadequate.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.nn import relu, softmax\n",
    "import gym\n",
    "from utils import Viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will work with the [CartPole-v0](https://gym.openai.com/envs/CartPole-v0) environment. Later you can change the code below to explore other [environments](https://gym.openai.com/envs/) and solve different tasks. \n",
    "\n",
    "*Note: The policy implemented in this notebook is designed to work on environments with a discrete action space. Extending the code to also handle environments with a continuous action space is left as an optional exercise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how the environment looks when we just take random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "view = Viewer(env, custom_render=True) # we use this custom viewer to render the environment in the notebook with docker and on a server\n",
    "for _ in range(200):\n",
    "    view.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "view.render(close=True, display_gif=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking random actions does not do a very good job at balancing the pole. Let us now apply the Policy Gradient method described above to solve this task!\n",
    "\n",
    "To start with, our policy will be a rather simple neural network with one hidden layer. We can retrieve the shape of the state space (input) and action space (output) from the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup policy network\n",
    "n_inputs = env.observation_space.shape[0]\n",
    "n_hidden = 20\n",
    "n_outputs = env.action_space.n\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "states_pl = tf.placeholder(tf.float32, [None, n_inputs], name='states_pl')\n",
    "actions_pl = tf.placeholder(tf.int32, [None, n_outputs], name='actions_pl')\n",
    "advantages_pl = tf.placeholder(tf.float32, [None], name='advantages_pl')\n",
    "learning_rate_pl = tf.placeholder(tf.float32, name='learning_rate_pl')\n",
    "\n",
    "l_hidden = tf.layers.dense(inputs=states_pl, units=n_hidden, activation=relu, name='l_hidden')\n",
    "l_out = tf.layers.dense(inputs=l_hidden, units=n_outputs, activation=softmax, name='l_out')\n",
    "\n",
    "# print network\n",
    "print('states_pl:', states_pl.get_shape())\n",
    "print('actions_pl:', actions_pl.get_shape())\n",
    "print('advantages_pl:', advantages_pl.get_shape())\n",
    "print('l_hidden:', l_hidden.get_shape())\n",
    "print('l_out:', l_out.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define loss and optimizer\n",
    "loss_f = -tf.reduce_mean(tf.multiply(tf.log(tf.gather_nd(l_out, actions_pl)), advantages_pl))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_pl)\n",
    "train_f = optimizer.minimize(loss_f)\n",
    "\n",
    "saver = tf.train.Saver() # we use this later to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test forward pass\n",
    "state = env.reset()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    action_probabilities = sess.run(fetches=l_out, feed_dict={states_pl: [state]})\n",
    "print(state)\n",
    "print(action_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: As we create our solution, we will make very few assumptions about the cart-pole environment. We aim to develop a general model for solving reinforcement learning problems, and therefore care little about the specific meaning of the inputs and outputs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def get_rollout(sess, env, rollout_limit=None, stochastic=False, seed=None):\n",
    "    \"\"\"Generate rollout by iteratively evaluating the current policy on the environment.\"\"\"\n",
    "    rollout_limit = rollout_limit or env.spec.timestep_limit\n",
    "    env.seed(seed)\n",
    "    s = env.reset()\n",
    "    states, actions, rewards = [], [], []\n",
    "    for _ in range(rollout_limit):\n",
    "        a = get_action(sess, s, stochastic)\n",
    "        s1, r, done, _ = env.step(a)\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        s = s1\n",
    "        if done: break\n",
    "    env.seed(None)\n",
    "    return states, actions, rewards\n",
    "\n",
    "def get_action(sess, state, stochastic=False):\n",
    "    \"\"\"Choose an action, given a state, with the current policy network.\"\"\"\n",
    "    # get action probabilities\n",
    "    a_prob = sess.run(fetches=l_out, feed_dict={states_pl: np.atleast_2d(state)})\n",
    "    if stochastic:\n",
    "        # sample action from distribution\n",
    "        return (np.cumsum(np.asarray(a_prob)) > np.random.rand()).argmax()\n",
    "    else:\n",
    "        # select action with highest probability\n",
    "        return a_prob.argmax()\n",
    "\n",
    "def get_advantages(rewards, rollout_limit, discount_factor, eps=1e-12):\n",
    "    \"\"\"Compute advantages\"\"\"\n",
    "    returns = get_returns(rewards, rollout_limit, discount_factor)\n",
    "    # standardize columns of returns to get advantages\n",
    "    advantages = (returns - np.mean(returns, axis=0)) / (np.std(returns, axis=0) + eps)\n",
    "    # restore original lengths\n",
    "    advantages = [adv[:len(rewards[i])] for i, adv in enumerate(advantages)]\n",
    "    return advantages\n",
    "\n",
    "def get_returns(rewards, rollout_limit, discount_factor):\n",
    "    \"\"\"Compute the cumulative discounted rewards, a.k.a. returns.\"\"\"\n",
    "    returns = np.zeros((len(rewards), rollout_limit))\n",
    "    for i, r in enumerate(rewards):\n",
    "        returns[i, len(r) - 1] = r[-1]\n",
    "        for j in reversed(range(len(r)-1)):\n",
    "            returns[i,j] = r[j] + discount_factor * returns[i,j+1]\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training settings\n",
    "epochs = 10 # number of training batches\n",
    "batch_size = 1000 # number of timesteps in a batch\n",
    "rollout_limit = env.spec.timestep_limit # max rollout length\n",
    "discount_factor = 1.00 # reward discount factor (gamma), 1.0 means no discount\n",
    "learning_rate = 0.001 # you know this by now\n",
    "early_stop_loss = 0 # stop training if loss < early_stop_loss, 0 or False to disable\n",
    "\n",
    "# train policy network\n",
    "try:\n",
    "    statistics = []\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('start training')\n",
    "        for epoch in range(epochs):\n",
    "            # generate rollouts until at least batch_size total timesteps are collected\n",
    "            states, actions, rewards = [], [], []\n",
    "            timesteps = 0\n",
    "            while timesteps < batch_size:\n",
    "                _rollout_limit = min(rollout_limit, batch_size - timesteps) # limit rollout to match batch_size\n",
    "                s, a, r = get_rollout(sess, env, _rollout_limit, stochastic=True, seed=epoch)            \n",
    "                states.append(s)\n",
    "                actions.append(a)\n",
    "                rewards.append(r)\n",
    "                timesteps += len(s)\n",
    "            # compute advantages\n",
    "            advantages = get_advantages(rewards, rollout_limit, discount_factor)\n",
    "            # policy gradient update\n",
    "            loss, _ = sess.run(fetches=[loss_f, train_f], feed_dict={\n",
    "                states_pl: np.concatenate(states),\n",
    "                actions_pl: np.column_stack((np.arange(timesteps), np.concatenate(actions))),\n",
    "                advantages_pl: np.concatenate(advantages),\n",
    "                learning_rate_pl: learning_rate\n",
    "            })            \n",
    "            # validation\n",
    "            val_rewards = [get_rollout(sess, env, rollout_limit, stochastic=False, seed=(epochs+i))[2] for i in range(10)]\n",
    "            # store and print training statistics\n",
    "            mtr = np.mean([np.sum(r) for r in rewards])\n",
    "            mvr = np.mean([np.sum(r) for r in val_rewards])\n",
    "            statistics.append((mtr, mvr, loss))\n",
    "            print('%4d. training reward: %6.2f, validation reward: %6.2f, loss: %7.4f' % (epoch+1, mtr, mvr, loss))\n",
    "            # early stopping\n",
    "            if early_stop_loss and loss < early_stop_loss: break\n",
    "        print('done')\n",
    "        # save session\n",
    "        saver.save(sess, 'tmp/model.ckpt')\n",
    "except KeyboardInterrupt:\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot training statistics\n",
    "statistics = np.array(statistics).T\n",
    "mean_training_rewards = statistics[0]\n",
    "mean_validation_rewards = statistics[1]\n",
    "losses = statistics[2]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.subplot(211)\n",
    "plt.plot(losses, label='loss')\n",
    "plt.xlabel('epoch'); plt.ylabel('loss')\n",
    "plt.xlim((0, len(losses)))\n",
    "plt.legend(loc=1); plt.grid()\n",
    "plt.subplot(212)\n",
    "plt.plot(mean_training_rewards, label='mean training reward')\n",
    "plt.plot(mean_validation_rewards, label='mean validation reward')\n",
    "plt.xlabel('epoch'); plt.ylabel('mean reward')\n",
    "plt.xlim((0, len(mean_validation_rewards)))\n",
    "plt.legend(loc=4); plt.grid()\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run cell to review solution\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"tmp/model.ckpt\")\n",
    "    s = env.reset()\n",
    "    view = Viewer(env, custom_render=True)\n",
    "    for _ in range(500):\n",
    "        view.render()\n",
    "        a = get_action(sess, s, stochastic=False)\n",
    "        s, r, done, _ = env.step(a)\n",
    "    view.render(close=True, display_gif=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Now it is your turn! Play around the code above and try to make it learn better and faster.\n",
    "\n",
    "Experiment with the:\n",
    "\n",
    "* number of timesteps in a batch.\n",
    "* max length of rollouts.\n",
    "* discount factor.\n",
    "* learning rate.\n",
    "* number of hidden units and layers.\n",
    "\n",
    "\n",
    "### Exercise 1 \n",
    "\n",
    "*Describe any changes you made to the code and why you think they improve the agent. Are you able to get solutions consistently?*\n",
    "\n",
    "Answer here...\n",
    "\n",
    "### Exercise 2 \n",
    "\n",
    "*In the plot of the training and validation mean reward, you will sometimes see the validation reward starts out lower than the training reward but later they cross. How can you explain this behavior? [Hint: Do we use the policy network in the same way during training and validation?]*\n",
    "\n",
    "Answer here...\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "*Consider the following sequence of rewards produced by an agent interacting with an environment for 10 timesteps: [0, 1, 1, 1, 0, 1, 1, 0, 0, 0].*\n",
    "\n",
    "* *What is the total reward?*\n",
    "* *What is the total future reward in each timestep?*\n",
    "* *What is the discounted future reward in each timestep if $\\gamma = 0.9$?*\n",
    "\n",
    "*(See previous notebook.)*\n",
    "\n",
    "Answer here...\n",
    "\n",
    "### Exercise 4\n",
    "\n",
    "*How does the policy gradient method we have used address the exploration-exploitation dilemma (see the previous notebook for definition)?*\n",
    "\n",
    "Answer here...\n",
    "\n",
    "## Optional exercises:\n",
    "\n",
    "* **Explore!** Train a policy for a different [environment](https://gym.openai.com/envs/).\n",
    "* **Let's get real!** Modify the code to work on an environment with a continuous action space. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
