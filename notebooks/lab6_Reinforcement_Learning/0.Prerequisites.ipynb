{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "> <span style=\"color:gray\">\n",
    "Created by Jonas Busk ([jbusk@dtu.dk](mailto:jbusk@dtu.dk)).\n",
    "</span>\n",
    "\n",
    "In this lab we will train a neural network reinforcement learning agent with [TensorFlow](https://www.tensorflow.org/) to navigate various environments from [OpenAI Gym](https://gym.openai.com/) provided by [OpenAI](https://openai.com/).\n",
    "\n",
    "Please refer to the [docs](https://gym.openai.com/docs/) on how to get started with Gym. You are also encouraged to reed this short [post on the OpenAI blog](https://blog.openai.com/openai-gym-beta/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "**Note: If you are running this lab in docker or on a server, everything you need should already be installed.**\n",
    "\n",
    "Below is a brief guide on how to install OpenAI Gym. For more details, please refer to the Gym repository on [GitHub](https://github.com/openai/gym) and the [docs](https://gym.openai.com/docs).\n",
    "\n",
    "You can perform a minimal install of `gym` with:\n",
    "\n",
    "```\n",
    "git clone https://github.com/openai/gym.git\n",
    "cd gym\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "If you prefer, you can do a minimal install of the packaged version directly from PyPI:\n",
    "\n",
    "```\n",
    "pip install gym\n",
    "```\n",
    "\n",
    "You will also need `JSAnimation` to render the Gym environments inline in the notebooks.\n",
    "\n",
    "```\n",
    "pip install git+https://github.com/jakevdp/JSAnimation.git\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an environment\n",
    "\n",
    "Here's a bare minimum example of getting something running. This will run an instance of the [CartPole-v0](https://gym.openai.com/envs/CartPole-v0) environment for 200 timesteps, taking random actions and rendering the environment at each step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from utils import Viewer\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset() # reset the environment\n",
    "view = Viewer(env, custom_render=True) # we use this custom viewer to render the environment inline in the notebook\n",
    "for _ in range(200):\n",
    "    view.render()\n",
    "    # env.render() # uncomment this to use gym's own render function\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "view.render(close=True, display_gif=True) # display the environment inline in the notebook\n",
    "# env.render(close=True) # uncomment this to use gym'm own render function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the classic cart-pole problem, the task is to balance a pole attached to a cart by moving the cart left and right. Taking random action, as shown in the code above, does not cut it. We have to do something smarter. Later we will learn to solve the task by applying reinforcement learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
