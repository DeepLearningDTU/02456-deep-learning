{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network with Linear Algebra\n",
    "> <span style=\"color:gray\"> Created by Mikkel Vilstrup ([mvil](https://github.com/MVilstrup))\n",
    "\n",
    "In the former notebook we developed a simple toy network with one unit and went manually through backpropagation. It was tedius work with just one unit, to do this manually for more complicated networks would be almost impossible. Luckily for us, almost all the operations needed for propagating the inputs through the neural network and updating the weights afterwards can be calculated efficiently using linear algebra. \n",
    "\n",
    "*N.B is you are not fully comfortable you might want to look at our notebook **OPT_Pure_Python**. Here we build a neural network without the use of Linear Algebra. We also introduce several different activation functions in this notebook. These might be beneficial to get comfortable with*\n",
    "\n",
    "In this notebook we are going to use the scientific computing library Numpy to define a more complicated network. We do this to show how much more compact the network can be formulated if we use concepts from linear algebra. \n",
    "When we afterwards use specialized deep learning frameworks to define the networks, it is important to remember that these are based on linear algebra as well, and are doing almost the same as we do here. \n",
    "\n",
    "The biggest difference is that they include advanced techniques for redundant operations and use the available hardware optimally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies and supporting functions\n",
    "Load all dependancies and supporting functions by running the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys, os\n",
    "sys.path.append(os.path.join('.', '..')) # in order to import various .py files\n",
    "from time import sleep\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from intro_utils import *\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import precision_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define activation function and cost function\n",
    "\n",
    "When doing classification with neural network we most often want the output to be a probability distribution over the available classes since this enable us to quantify how sure we are of a certain prediction. \n",
    "\n",
    "Whenever we use the softmax as an activation function in the final layer, we tend to use the cross entropy as our loss function. This is because the derivative of the cross entropy function works well with the derivative of the softmax function when doing backpropagation with a better preforming models as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(values):\n",
    "    exp_scores = np.exp(values)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(model, X, Y):\n",
    "    hidden_weights, output_weights = model['HW'], model['OW']\n",
    "    probs = predict(model, X, get_probs=True)\n",
    "    \n",
    "    num_examples = len(Y)\n",
    "    \n",
    "    # Calculating the loss\n",
    "    corect_logprobs = -np.log(probs[range(num_examples), Y])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    \n",
    "    return 1. / num_examples * data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model\n",
    "\n",
    "The fully connected architecture we are trying to define can be visualized in the following manner: \n",
    "\n",
    "![alt text](images/ffn.png \"Fully Connected Neural Net\")\n",
    "\n",
    "Each hidden unit $h_i$ in each layer is connected to all the units in the former layer.\n",
    "The weights of a layer can therefore be defined as a matrix $M$ with one row for every hidden unit in the former layer and one column for every unit in the current layer.\n",
    "We also want to add a bias vector $b_i$ to each hidden unit $h_i$ (i.e. same number of elements as there are columns in the matrix).\n",
    "Mathematically this equals the function: \n",
    "\n",
    "$$ Y = X \\cdot H + b $$\n",
    "\n",
    "So in forward propagation we multiply each layer-matrix with its input and add a bias vector.\n",
    "We then apply an activation function to the result and pass the output of this activation function as the input to the next layer.\n",
    "\n",
    "Backpropagation is a bit more tricky, and we will not go into detail with it here. However, if you have gone through our notebook of backpropagation you shuld have understood it by now.\n",
    "\n",
    "Our model is much simpler than the general model sketched above. We have 1 input layer, 1 hidden layer and 1 output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(train_data,\n",
    "                test_data,\n",
    "                input_dimensions = 2,\n",
    "                hidden_dimensions = 3,\n",
    "                output_dimensions = 2,\n",
    "                epochs = 1000,\n",
    "                learning_rate = 0.01):\n",
    "    \n",
    "    X_train, Y_train = extract_data(train_data)\n",
    "    X_test, Y_test = extract_data(test_data)\n",
    "    \n",
    "    # Define the necessary weights and biases\n",
    "    # This time we normalize the random weights which is common practice (but done behind the scenes)\n",
    "    hidden_weights = np.random.randn(input_dimensions, hidden_dimensions) / np.sqrt(input_dimensions)\n",
    "    output_weights = np.random.randn(hidden_dimensions, output_dimensions) / np.sqrt(hidden_dimensions)\n",
    "    \n",
    "    hidden_bias = np.zeros((1, hidden_dimensions)) # The bias related to the weights\n",
    "    output_bias = np.zeros((1, output_dimensions)) # The bias related to the weights\n",
    "    \n",
    "    # We define a dictionary to maintain the state of the network\n",
    "    # this could be done in many other ways as well\n",
    "    model = { \n",
    "        \"HW\": hidden_weights,\n",
    "        \"HB\": hidden_bias,\n",
    "        \"OW\": output_weights,\n",
    "        \"OB\": output_bias\n",
    "    }\n",
    "    \n",
    "    test_loss = []\n",
    "    train_loss = []\n",
    "    \n",
    "    #   ------------- Code for plotting -------------\n",
    "    #   ---------- DON'T WORRY ABOUT THIS -----------\n",
    "    xx_train, yy_train = mesh_grid(X_train)\n",
    "    xx_test, yy_test = mesh_grid(X_test)\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,8))\n",
    "    ax1.scatter(X_train[:, 0], X_train[:, 1], s=60, cmap=plt.cm.coolwarm)\n",
    "    ax2.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, s=60, cmap=plt.cm.coolwarm)\n",
    "    \n",
    "    fig.suptitle(\"Epoch: 0\", fontsize=20)\n",
    "    plot(model, ax1, X_train, Y_train, \"Train\", xx_train, yy_train)\n",
    "    plot(model, ax2, X_test, Y_test, \"Test\", xx_test, yy_test)\n",
    "    plot_error(ax3, train_loss, test_loss)\n",
    "    sleep(2) \n",
    "    #   ^^^^^^^^^^^^ End of plotting code ^^^^^^^^^^^^\n",
    "        \n",
    "    #  ------------- TRAINING THE MODEL -------------  \n",
    "    for i in range(1, epochs + 1):\n",
    "        #  ------------- FORWARD PROPAGATION -------------\n",
    "        hidden_values = X_train.dot(hidden_weights) + hidden_bias\n",
    "        hidden_activations = np.tanh(hidden_values) # N.B we use TanH Activation Function\n",
    "        output_values = hidden_activations.dot(output_weights) + output_bias\n",
    "        probs = softmax(output_values)\n",
    "\n",
    "        #  ------------- BACKPROPAGATION -------------\n",
    "        output_delta = probs\n",
    "        output_delta[range(len(X_train)), Y_train] -= 1\n",
    "        output_weights_delta = (hidden_activations.T).dot(output_delta)\n",
    "        output_bias_delta = np.sum(output_delta, axis=0, keepdims=True)\n",
    "        \n",
    "        # Here we take the derivative of Tanh and multiplies it with the output delta\n",
    "        tan_h_derivative = (1 - np.power(hidden_activations, 2))\n",
    "        hidden_delta = output_delta.dot(output_weights.T) * tan_h_derivative\n",
    "        \n",
    "        hidden_weights_delta = np.dot(X_train.T, hidden_delta)\n",
    "        hidden_bias_delta = np.sum(hidden_delta, axis=0)\n",
    "        \n",
    "        #  ------------- UPDATE WEIGHTS (Gradient Descent) -------------\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        output_weights += -learning_rate * output_weights_delta\n",
    "        output_bias += -learning_rate * output_bias_delta\n",
    "        \n",
    "        hidden_weights += -learning_rate * hidden_weights_delta\n",
    "        hidden_bias += -learning_rate * hidden_bias_delta\n",
    "        \n",
    "        # Update the state of the model\n",
    "        model = { \n",
    "            \"HW\": hidden_weights,\n",
    "            \"HB\": hidden_bias,\n",
    "            \"OW\": output_weights,\n",
    "            \"OB\": output_bias\n",
    "        }\n",
    "        \n",
    "        #   ------------- Code for plotting -------------\n",
    "        #   ---------- DON'T WORRY ABOUT THIS -----------\n",
    "        if i % 20 == 0:\n",
    "            fig.suptitle(\"Epoch: {}\".format(i), fontsize=20)\n",
    "            plot(model, ax1, X_train, Y_train, \"Train\", xx_train, yy_train)\n",
    "            plot(model, ax2, X_test, Y_test, \"Test\", xx_test, yy_test)\n",
    "            plot_error(ax3, train_loss, test_loss)\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "        \n",
    "        test_loss.append(cross_entropy(model, X_test, Y_test))\n",
    "        train_loss.append(cross_entropy(model, X_train, Y_train))\n",
    "        #   ^^^^^^^^^^^^ End of plotting code ^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_size = 500\n",
    "test_size = 100\n",
    "\n",
    "X_train, Y_train = make_moons(train_size, noise=0.2)\n",
    "X_test, Y_test = make_moons(test_size, noise=0.2)\n",
    "\n",
    "\n",
    "train_data = list(zip(X_train, Y_train))\n",
    "test_data = list(zip(X_test, Y_test))\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "HIDDEN_DIMENSIONS = 50\n",
    "EPOCHS = 1500\n",
    "\n",
    "build_model(train_data, \n",
    "            test_data, \n",
    "            epochs=EPOCHS,\n",
    "            hidden_dimensions=HIDDEN_DIMENSIONS,\n",
    "            learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments\n",
    "\n",
    " 1) Try to play around with the learning rate and note what happens\n",
    " \n",
    " 2) Add another layer to the network"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
