{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network with Linear Algebra\n",
    "> <span style=\"color:gray\"> Created by Mikkel Vilstrup ([mvil](https://github.com/MVilstrup))\n",
    "\n",
    "In the former notebook we developed a simple FFN in pure python. Although it worked, it required a lot of code, and as you will see it was extremely slow. Luckily for us, almost all the operations needed for propagating the inputs through the neural network and updating the weights afterwards can be calculated efficiently using linear algebra. \n",
    "\n",
    "In this notebook we are going to use the scientific computing library Numpy to define the exact same network as we did before. We do this to show how much more compact the network can be formulated if we use concepts from linear algebra. \n",
    "When we afterwards use specialized deep learning frameworks do define the networks, it is important to remember that these are based on linear algebra as well, and are doing almost the same as we do here. \n",
    "\n",
    "The biggest difference is that they include advanced techniques for redundant operations and use the available hardware optimally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies and supporting functions\n",
    "Load all dependancies and supporting functions by running the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys, os\n",
    "sys.path.append(os.path.join('.', '..')) # in order to import various .py files\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from intro_utils import *\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import precision_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define activation function and cost function\n",
    "\n",
    "When doing classification with neural network we most often want the output to be a probability distribution over the available classes since this enable us to quantify how sure we are of a certain prediction. \n",
    "\n",
    "Whenever we use the softmax as an activation function in the final layer, we tend to use the cross entropy as our loss function. This is because the derivative of the cross entropy function works well with the derivative of the softmax function when doing backpropagation with a better preforming models as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(values):\n",
    "    exp_scores = np.exp(values)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(model, X, Y, regularization):\n",
    "    hidden_weights, output_weights = model['HW'], model['OW']\n",
    "    probs = predict(model, X, get_probs=True)\n",
    "    \n",
    "    num_examples = len(Y)\n",
    "    \n",
    "    # Calculating the loss\n",
    "    corect_logprobs = -np.log(probs[range(num_examples), Y])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    \n",
    "    # Add regulatization term to loss (optional)\n",
    "    data_loss += (regularization / 2) * (np.sum(np.square(hidden_weights)) + np.sum(np.square(output_weights)))\n",
    "    \n",
    "    return 1. / num_examples * data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model\n",
    "\n",
    "The fully connected architecture we are trying to define can be visualized in the following manner: \n",
    "\n",
    "![alt text](images/ffn.png \"Fully Connected Neural Net\")\n",
    "\n",
    "Each hidden unit $h_i$ in each layer is connected to all the units in the former layer.\n",
    "The weights of a layer can therefore be defined as a matrix $M$ with one row for every hidden unit in the former layer and one column for every unit in the current layer.\n",
    "We also want to add a bias vector $b_i$ to each hidden unit $h_i$ (i.e. same number of elements as there are columns in the matrix).\n",
    "Mathematically this equals the function: \n",
    "\n",
    "$$ Y = X * H + b $$\n",
    "\n",
    "So in forward propagation we multiply each layer-matrix with its input and add a bias vector.\n",
    "We then apply an activation function to the result and pass the output of this activation function as the input to the next layer.\n",
    "\n",
    "Backpropagation is a bit more tricky, and we will not go into detail with it here. However, simply put, backpropagation is a method of finding the derivatives of each weight in order to optimize the weight according to the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(train_data,\n",
    "                test_data,\n",
    "                input_dimensions = 2,\n",
    "                hidden_dimensions = 3,\n",
    "                output_dimensions = 2,\n",
    "                epochs = 1000,\n",
    "                learning_rate = 0.01,\n",
    "                regularization = 0.01):\n",
    "    \n",
    "    X_train, Y_train = extract_data(train_data)\n",
    "    X_test, Y_test = extract_data(test_data)\n",
    "    \n",
    "    # Define the necessary weights and biases\n",
    "    # This time we normalize the random weights which is common practice (but done behind the scenes)\n",
    "    hidden_weights = np.random.randn(input_dimensions, hidden_dimensions) / np.sqrt(input_dimensions)\n",
    "    hidden_bias = np.zeros((1, hidden_dimensions)) # The bias is handled explicitly\n",
    "    \n",
    "    output_weights = np.random.randn(hidden_dimensions, output_dimensions) / np.sqrt(hidden_dimensions)\n",
    "    output_bias = np.zeros((1, output_dimensions)) # The bias is handled explicitly\n",
    "    \n",
    "    model = { \n",
    "        \"HW\": hidden_weights,\n",
    "        \"Hb\": hidden_bias,\n",
    "        \"OW\": output_weights,\n",
    "        \"OB\": output_bias\n",
    "    }\n",
    "    \n",
    "    #   ------------- Code for plotting -------------\n",
    "    xx_train, yy_train = mesh_grid(X_train)\n",
    "    xx_test, yy_test = mesh_grid(X_test)\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,8))\n",
    "    ax1.scatter(X_train[:, 0], X_train[:, 1], s=60, cmap=plt.cm.coolwarm)\n",
    "    ax2.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, s=60, cmap=plt.cm.coolwarm)\n",
    "    \n",
    "    #   ^^^^^^^^^^^^ End of plotting code ^^^^^^^^^^^^\n",
    "    \n",
    "    \n",
    "    #  ------------- TRAINING THE MODEL -------------\n",
    "    test_loss = []\n",
    "    train_loss = []\n",
    "    for i in range(1, epochs + 1):\n",
    "        #  ------------- FORWARD PROPAGATION -------------\n",
    "        hidden_values = X_train.dot(hidden_weights) + hidden_bias\n",
    "        hidden_activations = np.tanh(hidden_values)\n",
    "        output_values = hidden_activations.dot(output_weights) + output_bias\n",
    "        probs = softmax(output_values)\n",
    "\n",
    "        #  ------------- BACKPROPAGATION -------------\n",
    "        output_delta = probs\n",
    "        output_delta[range(len(X_train)), Y_train] -= 1\n",
    "        output_weights_delta = (hidden_activations.T).dot(output_delta)\n",
    "        output_bias_delta = np.sum(output_delta, axis=0, keepdims=True)\n",
    "        \n",
    "        # Here we take the derivative of Tanh and multiplies \n",
    "        hidden_delta = output_delta.dot(output_weights.T) * (1 - np.power(hidden_activations, 2))\n",
    "        hidden_weights_delta = np.dot(X_train.T, hidden_delta)\n",
    "        hidden_bias_delta = np.sum(hidden_delta, axis=0)\n",
    "\n",
    "        #  ------------- ADD REGULARIZATION -------------\n",
    "        \n",
    "        # Add regularization terms (the biases don't have regularization terms)\n",
    "        output_weights_delta += regularization * output_weights\n",
    "        hidden_weights_delta += regularization * hidden_weights\n",
    "        \n",
    "        #  ------------- UPDATE WEIGHTS (Gradient Descent) -------------\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        output_weights += -learning_rate * output_weights_delta\n",
    "        output_bias += -learning_rate * output_bias_delta\n",
    "        \n",
    "        hidden_weights += -learning_rate * hidden_weights_delta\n",
    "        hidden_bias += -learning_rate * hidden_bias_delta\n",
    "        \n",
    "        model = { \n",
    "            \"HW\": hidden_weights,\n",
    "            \"HB\": hidden_bias,\n",
    "            \"OW\": output_weights,\n",
    "            \"OB\": output_bias\n",
    "        }\n",
    "        \n",
    "        #   ------------- Code for plotting -------------\n",
    "        # Plot the current results\n",
    "        if i % (epochs / 20) == 0:\n",
    "            plot(model, ax1, X_train, Y_train, \"Train\", xx_train, yy_train)\n",
    "            plot(model, ax2, X_test, Y_test, \"Test\", xx_test, yy_test)\n",
    "            plot_error(ax3, train_loss, test_loss)\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "        \n",
    "        test_loss.append(cross_entropy(model, X_test, Y_test, regularization))\n",
    "        train_loss.append(cross_entropy(model, X_train, Y_train, regularization))\n",
    "        #   ^^^^^^^^^^^^ End of plotting code ^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_size = 500\n",
    "test_size = 100\n",
    "\n",
    "X_train, Y_train = make_moons(train_size, noise=0.2)\n",
    "X_test, Y_test = make_moons(test_size, noise=0.2)\n",
    "\n",
    "\n",
    "train_data = list(zip(X_train, Y_train))\n",
    "test_data = list(zip(X_test, Y_test))\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "REGULARIZATION = 0.01\n",
    "HIDDEN_DIMENSIONS = 50\n",
    "EPOCHS = 10000\n",
    "\n",
    "build_model(train_data, \n",
    "            test_data, \n",
    "            epochs=EPOCHS,\n",
    "            hidden_dimensions=HIDDEN_DIMENSIONS,\n",
    "            regularization=REGULARIZATION,\n",
    "            learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments\n",
    "\n",
    " 1) Add another layer to the network"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
