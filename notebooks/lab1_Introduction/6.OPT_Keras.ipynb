{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network with Keras\n",
    "> <span style=\"color:gray\"> Created by Mikkel Vilstrup ([mvil](https://github.com/MVilstrup)) and \n",
    "Toke Faurby ([faur](https://github.com/Faur))\n",
    "</span>\n",
    "\n",
    "\n",
    "Pure TF is very verbose, and it is therefore a good idea to use a high-level API of some sort.\n",
    "This simplifies and speeds-up development, reduces the risk of bugs, and generally reduces headache.\n",
    "To handle this complexity various researchers and companies have gone in different directions with their frameworks.\n",
    "Some have made it easier to explicitly state what you want, others have made it easier to quickly iterate over possible network structures.\n",
    "There is no wrong or right here, just different implementations. \n",
    "Another neat benefit is that a lot of best practices (initialization, scoping, etc.) are hard-coded into the functions.\n",
    "\n",
    "To showcase how different the implementations are, we have included this notebook to showcase **[Keras](https://keras.io/)**.\n",
    "Keras is a high-level API that can use TensorFlow or Theano as backend.\n",
    "It makes it really easy to try various configurations quickly.\n",
    "\n",
    "In early 2017 [TensorFlow chose Keras](http://www.fast.ai/2017/01/03/keras/) as the first high-level API to include into the tensorflow core.\n",
    "Keras has official Google support and has a large community and pre-existing examples.\n",
    "Making it a good choice for high-level API to learn at the moment.\n",
    "It is built on top of various backends, and can thus be utilized on top of almost all the other frameworks. \n",
    "\n",
    "Keras makes it easy to iterate over many standard building blocks, it makes it difficult too try to build new building blocks or experiment with small alterations of the standard building blocks specialized for your particular problem. \n",
    "Keras is however compatible with TF core, so you can easily create custom functionality when needed.\n",
    "\n",
    "There several other high-level APIs: \n",
    "* [TFLearn](http://tflearn.org/),\n",
    "* [tf.learn](https://www.tensorflow.org/get_started/tflearn) (yes, the names are confusing :( ),\n",
    "* [tf.slim](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim), and\n",
    "* [tf.layers](https://www.tensorflow.org/api_docs/python/tf/layers).\n",
    "\n",
    "These are generally compatible with each other, and offer overlaping functionality.\n",
    "This can be confusing.\n",
    "\n",
    "\n",
    "\n",
    "#### External resources\n",
    "If you wish to dig deeper into Keras have a look at:\n",
    "* Tutorial: [Keras as a simplified interface to TensorFlow](https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html)\n",
    "* [Keras source code](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/keras) in the TF github repo. Looking at the code is often the best way to understand what is going on. Keras also provides some good [example code](https://github.com/fchollet/keras/tree/master/examples), that demonstrates how to do many things.\n",
    "* [TensorFlow High-Level APIs: Models in a Box](https://www.youtube.com/watch?v=t64ortpgS-E) 17 min video  (TensorFlow Dev Summit 2017)\n",
    "* [Integrating Keras & TensorFlow: The Keras workflow, expanded](https://www.youtube.com/watch?v=UeheTiBJ0Io) 18 min video (TensorFlow Dev Summit 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Fundamentals\n",
    "\n",
    "Keras sits of top of `tf.layers`, and shares its implementation (e.g. `tf.layers.dense` and `keras.layers.Dense` are the same).\n",
    "Keras helps with model creation, training, and evaluation.\n",
    "We won't use Keras to its full potential in this course, as it sometimes encapsulates the details to such an extent that it hinders learning.\n",
    "\n",
    "![](images/keras_overview.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading dependancies and supporting functions by running the code block below.\n",
    "from __future__ import absolute_import, division, print_function \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join('.', '..'))\n",
    "\n",
    "import utils\n",
    "from intro_utils import ffn_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing networks in Keras\n",
    "\n",
    "In previous notebooks you saw how to implement neural networks more or less by hand.\n",
    "Now we will look at how it can be done in Keras.\n",
    "This is actually very simple, as you will soon see.\n",
    "\n",
    "There is nothing new with respect to the network we implement:\n",
    "2 fully connected layers with a Relu activation function in between.\n",
    "The final output is softmax in order to output a probability distribution over the likelyhood the input belongs to a certain class. \n",
    "We train the network with cross entropy since this is the normal loss function to use when utilizing softmax, and train the network with regular stochastic gradient descent. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Import data\n",
    "num_samples = 2000\n",
    "train_size = int(num_samples *(2/3))\n",
    "X, y = make_moons(num_samples, noise=0.20)\n",
    "\n",
    "x_train = X[:train_size].astype('float32')\n",
    "x_test = X[train_size:].astype('float32')\n",
    "\n",
    "targets_train = y[:train_size].astype('int32')\n",
    "targets_test = y[train_size:].astype('int32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we will import the necessary modules ad classes from Keras.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import backend as K\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is needed to be able to work properly in Jupyter Notebooks with Keras\n",
    "# We tell Keras to remove all the elements in the graph it had previously created\n",
    "session = K.get_session()\n",
    "if model is not None:\n",
    "    model.reset_states()\n",
    "\n",
    "# We use the exact same parameters as out implementation in Tensorflow\n",
    "num_features = x_train.shape[1]\n",
    "num_classes = 2\n",
    "num_neurons = 512\n",
    "num_layers = 2\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "\n",
    "# We define a sequential model which simply stack layers on top of each other sequentially\n",
    "model = Sequential()\n",
    "\n",
    "# The first layer needs to know the input_dim, so it is handled separately\n",
    "print('Creating layer', 0)\n",
    "model.add(Dense(units=num_neurons, input_dim=num_features))\n",
    "# Then we add a Relu activation function\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We can now easily create as many layers as we want!\n",
    "for i in range(1, num_layers):\n",
    "    print('Creating layer', i)\n",
    "    model.add(Dense(units=num_neurons, activation='relu'))\n",
    "\n",
    "# then we add a new Dense layer with the size of our 10 classes \n",
    "print('Creating output layer')\n",
    "model.add(Dense(units=num_classes))\n",
    "\n",
    "# Finally we add a softmax activation function in order to transform the output into a propability distribution\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# We then transform the model into a tensorflow graph and use Stochastic Gradient Descent as optimization function\n",
    "# and cross entropy as our loss\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras allows us to get a quick summary of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is just a wrapper around Tensorflow it relies completely on Tensorflows graph (or Theano, CNTK or other low level frameworks).\n",
    "When using TensorFlow we can inspect the graph that Keras has created using TensorBoard, just like before.\n",
    "\n",
    "Keras is however not made for this, so it isn't very informative, as there isn't any convenient way of naming the various keras layers etc. \n",
    "So there is no way to tell TensorBoard how to display the variables and operations. \n",
    "However, **the goal of Keras is to make the code so simple the structure of the network can be read directly**. \n",
    "So we don't really need to look at the graph anyway.\n",
    "\n",
    "For good measure, this is how a Keras graph looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Launch TensorBoard, and visualize the TF graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tmp_def = utils.rename_nodes(sess.graph_def, lambda s:\"/\".join(s.split('_',1)))\n",
    "    utils.show_graph(tmp_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a defined model we can now easily train it using a single function call. The function takes all our:\n",
    "- training inputs\n",
    "- training targets\n",
    "- number of epochs\n",
    "- the decired batch size\n",
    "\n",
    "It then takes care of dividing the data into proper batches and loading it into the graph. On top of this, all the weights are initialized optimally and the defualt hyperparameters are in general the state of the art. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_history = model.fit(x_train, utils.onehot(targets_train, num_classes),\n",
    "          validation_data = (x_test, utils.onehot(targets_test, num_classes)),\n",
    "          epochs=num_epochs, \n",
    "          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model is fitted Keras also makes it easy to evaluate  the performance. Like above all it takes is a single function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, \n",
    "                                utils.onehot(targets_test, num_classes), \n",
    "                                batch_size=128)\n",
    "print()\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plotting the results\n",
    "print('Training history', train_history.history.keys())\n",
    "\n",
    "test_pred_prob = model.predict(x_test)\n",
    "test_pred_labels = test_pred_prob[:,0]<0.5\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "plt.subplot(221)\n",
    "plt.title('True labels')\n",
    "plt.scatter(x_test[:,0], x_test[:,1], c=targets_test)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.title('Predicted labels')\n",
    "plt.scatter(x_test[:,0], x_test[:,1], c=test_pred_labels)\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.title('Errors')\n",
    "plt.plot(x_test[targets_test==test_pred_labels,0], x_test[targets_test==test_pred_labels,1], 'o', c='b', label='Correct')\n",
    "plt.plot(x_test[targets_test!=test_pred_labels,0], x_test[targets_test!=test_pred_labels,1], 'o', c='r', label='Wrong')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(247)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(train_history.history['acc'], label='Train acc')\n",
    "plt.plot(train_history.history['val_acc'], label='Val acc')\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.subplot(248)\n",
    "plt.title('Loss')\n",
    "plt.plot(train_history.history['loss'], label='Train loss')\n",
    "plt.plot(train_history.history['val_loss'], label='Val loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments\n",
    "\n",
    " 1) Notice the difference between the train accuracy and the validation accuracy and try to minimize this difference.\n",
    " \n",
    " 2) Try to experiment by changing the  \n",
    " - number of layers\n",
    " - number of hidden units\n",
    " - optimizer\n",
    "  - i.e. change '`sgd`' in the '`model.compile`' call. A list of Keras optimizers can be found [here](https://keras.io/optimizers/).\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
