{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUT: Neural networks 101\n",
    "> <span style=\"color:gray\">\n",
    "Original [Theano/Lasagne tutorial](https://github.com/DeepLearningDTU/nvidia_deep_learning_summercamp_2016/blob/master/lab1/lab1_FFN.ipynb) by \n",
    "Lars Maaløe ([larsmaaloee](https://github.com/larsmaaloee)),\n",
    "Søren Kaae Sønderby ([skaae](https://github.com/skaae)), and \n",
    "Casper Sønderby ([casperkaae](https://github.com/casperkaae)). \n",
    "Converted to TensorFlow by \n",
    "Alexander R. Johansen ([alrojo](https://github.com/alrojo)), \n",
    "and updated by \n",
    "Toke Faurby ([faur](https://github.com/Faur)) and \n",
    "Mikkel Vilstrup ([mvilstrup](https://github.com/MVilstrup)).\n",
    "\n",
    "\n",
    "In this notebook you will implement a simple neural network in TensorFlow utilizing the graph building and automatic differentiation engine of TensorFlow. \n",
    "In this notebook we'll go through a simple 2-D and 2-class classification problem using the **half-moon dataset**.\n",
    "The half-moon dataset is a nice place to start as it offers simple visualization and the network can be trained quickly.\n",
    "\n",
    "\n",
    "First we show how to implement the **logistic regression** in TensorFlow.\n",
    "Logistic regression is linear and cannot solve the problem.\n",
    "Then you will extend the model to a simple neural network (formally called a dense feedforward neural network), which is able to do much better, as it is able to capture non-linear relations.\n",
    "In the bottom of the notebook there are some exercises to guide you through this.\n",
    "\n",
    "\n",
    "Some details of TensorFlow can be a bit confusing, so don't get discouraged if you don't get it right away.\n",
    "You'll pick them up when you worked with it for some time.\n",
    "\n",
    "\n",
    "We assume that you are already familiar with backpropagation/gradient descent (if not please see \n",
    "[Andrej Karpathy](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b) or \n",
    "[Michal Nielsen](http://neuralnetworksanddeeplearning.com/chap2.html)).\n",
    "\n",
    "### TensorFlow Playerground\n",
    "\n",
    "If you are new to Neural Networks, start by using the **[TensorFlow playground](http://playground.tensorflow.org/)** (very instructive visualization) to familiarize yourself with hidden layers, hidden units, activations, learning rate, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading dependancies and supporting functions by running the code block below.\n",
    "from __future__ import absolute_import, division, print_function \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "import tensorflow as tf\n",
    "\n",
    "import os, sys\n",
    "sys.path.append(os.path.join('.', '..'))\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The half-moon data set\n",
    "\n",
    "Before begining to model it is always a good idea to explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a dataset and plot it\n",
    "np.random.seed(0)\n",
    "num_samples = 300\n",
    "\n",
    "X, y = sklearn.datasets.make_moons(num_samples, noise=0.20)\n",
    "\n",
    "X_tr = X[:100].astype('float32')\n",
    "X_val = X[100:200].astype('float32')\n",
    "X_te = X[200:].astype('float32')\n",
    "\n",
    "y_tr = y[:100].astype('int32')\n",
    "y_val = y[100:200].astype('int32')\n",
    "y_te = y[200:].astype('int32')\n",
    "\n",
    "plt.scatter(X_tr[:,0], X_tr[:,1], s=40, c=y_tr, cmap=plt.cm.BuGn)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "num_features = X_tr.shape[-1]\n",
    "num_output = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the graph\n",
    "\n",
    "The building blocks of TensorFlow are variables and operations, with these we can form computational graphs.\n",
    "Such as when computing the logistic regression which is implemented below:\n",
    "$$y = nonlinearity(xW + b)$$\n",
    "\n",
    "where $x$ is the input tensor, $y$ is the output tensor and $\\{W, b\\}$ are the weights (variable tensors).\n",
    "The weights are initialized with an initializer of our choice.\n",
    "* x has shape ```[batchsize, num_input]```. \n",
    "* ```W``` has shape ```[num_input, num_units]``` and \n",
    "* b has ```[num_units]```.\n",
    "* y has then ```[batch_size, num_units]```.\n",
    "\n",
    "In this case we will have 2 inputs, one for each dimension, and 2 output units, one for each class.\n",
    "\n",
    "**NOTE**: to make building neural networks easier there are several high-level APIs that wraps TensorFlow functionality into high-level abstractions, such as layers.\n",
    "In this first exercise we will use basic TensorFlow functions so that you can learn how to build it from scratch.\n",
    "This will help you later if you want to build your own custom operations.\n",
    "\n",
    "These ops will define edges along our computational graph. \n",
    "The below ops will compute a logistic regression, but can be modified to compute a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# resets the graph, needed when initializing weights multiple times, like in this notebook\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Setting up placeholder, this is where your data enters the graph!\n",
    "x_pl = tf.placeholder(tf.float32, [None, num_features], name='xPlaceholder')\n",
    "# 'None' means that the size is determined at run time. This is commonly done for the \n",
    "# batch size.\n",
    "\n",
    "## Define initializer for the weigths\n",
    "# How the weights are initialized is VERY important for how well the network trains.\n",
    "# We will look into this later, but for now we will just use a normal distribution.\n",
    "weight_initializer = tf.truncated_normal_initializer(stddev=0.1)\n",
    "\n",
    "## Set up variables\n",
    "# These variables are weights in your network that can be update while running our graph.\n",
    "\n",
    "### Create layer 1\n",
    "with tf.variable_scope('layer1'): \n",
    "    W_1 = tf.get_variable('W', [num_features, num_output], # change num_output to 100 for mlp\n",
    "                          initializer=weight_initializer)\n",
    "    b_1 = tf.get_variable('b', [num_output], # change num_output to 100 for mlp\n",
    "                          initializer=tf.constant_initializer(0.0))\n",
    "    with tf.variable_scope('output'):\n",
    "        \n",
    "        l_1 = tf.matmul(x_pl, W_1) + b_1\n",
    "        # The layer before the softmax should not have a nonlinearity\n",
    "        # We will cover choosing nonlinearity later. For now use the linear rectifier\n",
    "        # https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "\n",
    "#         l_1 = tf.nn.relu(l_1)\n",
    "\n",
    "\n",
    "### Create layer 2 (left as an exercise)\n",
    "### Ignore this until you reach the exercises in the bottom.\n",
    "# 1) add a non-linearity to layer 1 by uncommenting the last line\n",
    "# 2) Change the dimensions of W_1 and b_1.\n",
    "#     Notice, to make a hidden layer, the weights needs to have the following dimensionality\n",
    "#     W[number_of_units_going_in, number_of_units_going_out]\n",
    "#     b[number_of_units_going_out]\n",
    "#     If we want to make a hidden layer with 100 units, we need to define the shape of the\n",
    "#     first weight to W[2, 100], b[2] and the shape of the second weight to W[100, 2], b[2]\n",
    "# 3) create layer 2. Look to how layer 1 was implemented for inspiration.\n",
    "# 4) Change the definition of `y` to take layer 2 as input.\n",
    "\n",
    "\n",
    "y = tf.nn.softmax(l_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# knowing how to print your tensors and ops is useful, here are some examples\n",
    "print(\"---placeholders---\")\n",
    "print('x_pl')\n",
    "print('Name:', x_pl.name)\n",
    "print('Shape:', x_pl.get_shape())\n",
    "print()\n",
    "print(\"---weights---\")\n",
    "print('W_1')\n",
    "print('Name:', W_1.name)\n",
    "print('Shape:', W_1.get_shape())\n",
    "print('b_1')\n",
    "print('Name:', b_1.name)\n",
    "print('Shape:', b_1.get_shape())\n",
    "print()\n",
    "print(\"---Tensors---\")\n",
    "print('l_1')\n",
    "print('Name:', l_1.name)\n",
    "print('Shape:', l_1.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to automatically print all the ops and variables by iterating through the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using the graph to print ops\n",
    "print(\"< operations >\")\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)\n",
    "print()\n",
    "\n",
    "# variables are accessed through tensorflow\n",
    "print(\"< variables >\")\n",
    "for var in tf.global_variables():\n",
    "    print(var.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our weights and operations defined in the `layer1` space are saved in the `layer1` directory of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training ops\n",
    "After we have built the network we have our tensors in our default [graph](https://www.tensorflow.org/api_docs/python/tf/Graph), which we can use to build the cost function and ops used for training.\n",
    "Further, using our default graph we can print the operations and variables of our default graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y_ is a placeholder variable taking on the value of the target batch.\n",
    "# This is used for computing the loss.\n",
    "y_ = tf.placeholder(tf.float32, [None, num_output], name='yPlaceholder')\n",
    "\n",
    "with tf.variable_scope('loss'):\n",
    "    # computing cross entropy per sample\n",
    "    cross_entropy = -tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1])\n",
    "\n",
    "    # Average over samples\n",
    "    # Averaging makes the loss invariant to batch size, which is very nice.\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimzers\n",
    "To train our neural network we need to update the parameters in direction of the negative gradient w.r.t the cost function we defined earlier.\n",
    "This is called gradient descent.\n",
    "We can use `tf.train.Optimizer` to get the gradients (using `compute_gradients`) for all parameters in the network w.r.t ``cross_entropy``.\n",
    "\n",
    "*Intuition behind gradient descent*: Imagine that `cross_entropy` is a function and we want to go downhill.\n",
    "We go downhill by changing the value of the paramters in direction of the negative gradient. \n",
    "Finally we can use the built-in `minimize` to calculate the stochastic gradient descent (SGD) update rule for each paramter in the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('trainOP'):\n",
    "    # Defining our optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "\n",
    "    # Computing our gradients\n",
    "    grads_and_vars = optimizer.compute_gradients(cross_entropy)\n",
    "\n",
    "    # Applying the gradients\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "\n",
    "    ## The three steps above can be compressed into one: \n",
    "    # train_op = optimizer.minimize(crossentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define an op that computes the accuracy.\n",
    "This is used to monitor the performance during training, but doesn't have any influence on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('performance'):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = tf.equal(tf.argmax(y, axis=1), tf.argmax(y_, axis=1))\n",
    "\n",
    "    # averaging the one-hot encoded vector\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to utilize our `train_op` function repeatedly in order to optimize our weights `W_1` and `b_1` to make the best possible linear seperation of the half moon dataset.\n",
    "\n",
    "It is worth or read a short introduction on TensorFlow [sessions](https://www.tensorflow.org/versions/r0.10/api_docs/python/client.html#Session) before continuing to the next codeblock. Sessions are used to run TensorFlow graphs, they uses `fetches` to decide which parts of the graph to compute and `feed_dicts` to load data into the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the graph\n",
    "\n",
    "Before begining the training we will have a look at the graph.\n",
    "Notice how the graph visualization is shaped by `tf.variable_scope`.\n",
    "\n",
    "\n",
    "*Note*: The Jupyter inline graph visualizer is not as good as when TensorBoard is run in a browser, especially not for large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tmp_def = utils.rename_nodes(sess.graph_def, lambda s:\"/\".join(s.split('_',1)))\n",
    "    utils.show_graph(tmp_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "The next step is to utilize our `train_op` function repeatedly in order to optimize our weights (`W_1` and `b_1`) to make the best possible linear seperation of the half moon dataset.\n",
    "\n",
    "It is worth or read a short introduction on TensorFlow [sessions](https://www.tensorflow.org/versions/r0.10/api_docs/python/client.html#Session) before continuing to the next codeblock. Sessions are used to run TensorFlow graphs, they uses `fetches` to decide which parts of the graph to compute and `feed_dicts` to load data into the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining a function to make predictions using our classifier\n",
    "def pred(X_in, sess):\n",
    "    # first we must define what data to give it\n",
    "    feed_dict = {x_pl: X_in}\n",
    "    # secondly our fetches\n",
    "    fetches = [y]\n",
    "    # utilizing the given session (ref. sess) to compute results\n",
    "    res = sess.run(fetches, feed_dict)\n",
    "    # res is a list with each indices representing the corresponding element in fetches\n",
    "    return res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "\n",
    "# restricting memory usage, TensorFlow is greedy and will use all memory otherwise\n",
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "\n",
    "train_cost, val_cost, train_acc, val_acc = [],[],[],[]\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:\n",
    "    try:\n",
    "        print('Begin training')\n",
    "        # initializing all variables \n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        ## Plotting\n",
    "        fig = plt.figure(figsize=(16,12))\n",
    "        fig.add_subplot(321)\n",
    "        plt.title(\"Untrained Classifier, training\")\n",
    "        utils.plot_decision_boundary(lambda x: pred(x, sess), X_tr, y_tr)\n",
    "        fig.add_subplot(323)\n",
    "        plt.title(\"Untrained Classifier, validation\")\n",
    "        utils.plot_decision_boundary(lambda x: pred(x, sess), X_val, y_val)\n",
    "\n",
    "        for e in range(num_epochs):\n",
    "            ### TRAINING ###\n",
    "            # what to feed to our train_op\n",
    "            # notice we onehot encode our predictions to change shape from (batch,) -> (batch, num_output)\n",
    "            feed_dict_train = {x_pl: X_tr, y_: utils.onehot(y_tr, num_output)}\n",
    "\n",
    "            # deciding which parts to fetch, train_op makes the classifier \"train\"\n",
    "            fetches_train = [train_op, cross_entropy, accuracy]\n",
    "\n",
    "            # running the train_op\n",
    "            res = sess.run(fetches=fetches_train, feed_dict=feed_dict_train)\n",
    "            # storing cross entropy (second fetch argument, so index=1)\n",
    "            train_cost += [res[1]]\n",
    "            train_acc += [res[2]]\n",
    "\n",
    "            ### VALIDATING ###\n",
    "            # what to feed our accuracy op\n",
    "            feed_dict_valid = {x_pl: X_val, y_: utils.onehot(y_val, num_output)}\n",
    "\n",
    "            # deciding which parts to fetch\n",
    "            fetches_valid = [cross_entropy, accuracy]\n",
    "\n",
    "            # running the validation\n",
    "            res = sess.run(fetches=fetches_valid, feed_dict=feed_dict_valid)\n",
    "            val_cost += [res[0]]\n",
    "            val_acc += [res[1]]\n",
    "\n",
    "\n",
    "            if e % 100 == 0:\n",
    "                print(\"Epoch %i, Train Cost: %0.3f\\tVal Cost: %0.3f\\t Val acc: %0.3f\" \\\n",
    "                      %(e, train_cost[-1],val_cost[-1],val_acc[-1]))\n",
    "\n",
    "        ### TESTING ###\n",
    "        # what to feed our accuracy op\n",
    "        feed_dict_test = {x_pl: X_te, y_: utils.onehot(y_te, num_output)}\n",
    "\n",
    "        # deciding which parts to fetch\n",
    "        fetches_test = [cross_entropy, accuracy]\n",
    "\n",
    "        # running the validation\n",
    "        res = sess.run(fetches=fetches_test, feed_dict=feed_dict_test)\n",
    "        test_cost = res[0]\n",
    "        test_acc = res[1]\n",
    "        print(\"\\nTest Cost: %0.3f\\tTest Accuracy: %0.3f\"%(test_cost, test_acc))\n",
    "\n",
    "        ## Plotting\n",
    "        fig.add_subplot(322)\n",
    "        plt.title(\"Trained Classifier, training\")\n",
    "        utils.plot_decision_boundary(lambda x: pred(x, sess), X_tr, y_tr)\n",
    "        fig.add_subplot(324)\n",
    "        plt.title(\"Trained Classifier, validation\")\n",
    "        utils.plot_decision_boundary(lambda x: pred(x, sess), X_val, y_val)\n",
    "\n",
    "        epoch = np.arange(len(train_cost))\n",
    "        fig.add_subplot(325)\n",
    "        plt.title('Loss')\n",
    "        plt.plot(epoch, train_cost,'r', label='Train Loss')\n",
    "        plt.plot(epoch, val_cost,'b', label='Val Loss')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Updates'), plt.ylabel('Loss')\n",
    "\n",
    "        fig.add_subplot(326)\n",
    "        plt.title('Accuracy')\n",
    "        plt.plot(epoch, train_acc,'r', label='Train Accuracy')\n",
    "        plt.plot(epoch, val_acc,'b', label='Val Accuracy')\n",
    "        plt.legend(loc=4)\n",
    "        plt.xlabel('Updates'), plt.ylabel('Accuracy')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except KeyboardInterrupt:\n",
    "        print('KeyboardInterrupt')\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now seen how to use TensorFlow to setup and train a logistic regression classifier.\n",
    "Now you will have to extend the code into a neural network.\n",
    "\n",
    "\n",
    "# <span style=\"color:red\"> Exercise 1: From logistic regression to 'deep learning'</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"> EXE 1.1) Add a hidden layer. \n",
    "A linear logistic classifier is only able to create a linear decision boundary. \n",
    "Make the model into a nonlinear neural network by inserting a dense hidden layer between the input and output layers of the model.\n",
    "\n",
    "The fully connected architecture we are trying to define can be visualized in the following manner: \n",
    "\n",
    "<img src=\"images/ffn.png\" style=\"width: 500px;\"/> \n",
    "\n",
    "Each hidden unit $h_i$ in each layer is connected to all the units in the former layer.\n",
    "The weights of a layer can therefore be defined as a matrix $M$ with one row for every hidden unit in the former layer and one column for every unit in the current layer.\n",
    "We also want to add a bias vector $b_i$ to each hidden unit $h_i$ (i.e. same number of elements as there are columns in the matrix).\n",
    "Mathematically this equals the function: \n",
    "\n",
    "$$ Y = X * H + b $$\n",
    "\n",
    "So in forward propagation we multiply each layer-matrix with its input and add a bias vector.\n",
    "We then apply an activation function to the result and pass the output of this activation function as the input to the next layer.\n",
    "\n",
    "1. **Modify the code in 'Creating the graph' by following the instructions in the comments.**\n",
    "___\n",
    "<span style=\"color:blue\"> Answer: </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"> EXE 1.2) Experiment with the number of hidden layers and number of units\n",
    " 1. **What happens to the decision boundary?**\n",
    "___\n",
    "<span style=\"color:blue\"> Answer: </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"> EXE 1.3) Overfitting\n",
    "When increasing the number of hidden layers / units the neural network will fit the training data better by creating a highly nonlinear decision boundary. If the model is to complex it will often generalize poorly to new data (validation and test set). \n",
    "  1. **How high can you get the training accuracy?**\n",
    "  1. **Can you obseve this from the training and validation errors? **\n",
    "\n",
    "___\n",
    "<span style=\"color:blue\"> Answer: </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"> EXE 1.4) Changing optimizer\n",
    "We used the vanilla stocastic gradient descent algorithm for parameter updates. This is usually slow to converge and more sophisticated pseudo-second-order methods usually works better. \n",
    "To use the other optimizers checkout which optimizers TensorFlow [supports](https://www.tensorflow.org/api_guides/python/train). \n",
    "In practice the ADAM algorithm (Kingma and Welling 2014 [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980v9)) or RMSProp are the most commonly used.\n",
    "\n",
    "**Setting the learning rate**: The learning rate is a hyperparameter, which can be tricky to set. The following values often work, and are good initial guesses, but you should always try a couple of different learning rates, to see which is best.\n",
    "\n",
    "\n",
    "1. SGD + Momentum: learning rate 1.0 - 0.1 \n",
    "2. ADAM: learning rate: 1e-4 - 1e-5\n",
    "3. RMSPROP: somewhere between SGD and ADAM   \n",
    "\n",
    "Heres a small animation comparing different optimization algorithms under hard circumstances.\n",
    "\n",
    "Long Valley | Saddle Point\n",
    "- | - \n",
    "<img src=\"http://i.imgur.com/2dKCQHh.gif?1\" style=\"width: 400px;\"/> | <img src=\"http://i.imgur.com/NKsFHJb.gif?1\" style=\"width: 400px;\"/>\n",
    "\n",
    "Animations are from at http://imgur.com/a/Hqolp (which includes another example).\n",
    "\n",
    "  1. **Try changing the optimizer to [ADAM](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer), and compare with SGD. **\n",
    "  1. **What are the differences in training time / performance?**\n",
    "___\n",
    "<span style=\"color:blue\"> Answer: </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
