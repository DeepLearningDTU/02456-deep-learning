{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset\n",
    "> <span style=\"color:gray\">\n",
    "Original [Theano/Lasagne tutorial](https://github.com/DeepLearningDTU/nvidia_deep_learning_summercamp_2016/blob/master/lab1/lab1_FFN.ipynb) by \n",
    "Lars Maaløe ([larsmaaloee](https://github.com/larsmaaloee)),\n",
    "Søren Kaae Sønderby ([skaae](https://github.com/skaae)), and \n",
    "Casper Sønderby ([casperkaae](https://github.com/casperkaae)). \n",
    "Converted to TensorFlow by \n",
    "Alexander R. Johansen ([alrojo](https://github.com/alrojo)), \n",
    "and updated by \n",
    "Toke Faurby ([faur](https://github.com/Faur)) and \n",
    "Mikkel Vilstrup ([mvilstrup](https://github.com/MVilstrup)).\n",
    "</span>\n",
    "\n",
    "In this notebook we will guide you through building a neural network classifier for the (in)famous MNIST dataset.\n",
    "MNIST is a dataset that is often used for benchmarking, consisting of 70.000 image-label paris of handwritten digits from 0-9.\n",
    "During the notebook we have provided a program skeleton and severalt tips to help guid you through it.\n",
    "Further more the task is quite similar to what you did in the previous notebook, so you should reffer to it in order to remind yourself about the implementation.\n",
    "\n",
    "Along the way you will also learn about \n",
    "* selecting nonlinearity, and\n",
    "* stochastic gradient descent with mini-batches.\n",
    "\n",
    "We will implement a dense feedforward neural network (just like before).\n",
    "In this setting we represent each image as a vector, instead of a 2D map.\n",
    "We therefore lose the spatial information of the images.\n",
    "The ordering of the pixels in the vector does not matter, so we could permutate all images using the same permutation and still get the same performance.\n",
    "This task is therefore called the _permutation invariant_ MNIST.\n",
    "Obviously this throws away a lot of structure in the data that could have been used.\n",
    "Later we will see how spatial information can be included, by using the convolutional neural network architecture.\n",
    "\n",
    "\n",
    "## Implementation Rules\n",
    "\n",
    "In this exercise we want you to get some experience with setting up training for yourself, and we therefore want you to implement the training loop yourself.\n",
    "You are therefore **NOT** allowed to use the `keras.models.Sequential.fit` function.\n",
    "The `Sequential` model, or built in layers, such as `keras.layers.Dense` are allowed, as long as you define the training ops and training loop yourself.\n",
    "(If are having problems using `keras.layers` have a look at the [Keras as a simplified interface to TensorFlow](https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html) tutorial).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function \n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join('.', '..')) \n",
    "import utils\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The MNIST dataset\n",
    "The 70,000 images-label pairs are split into a \n",
    "* 55,000 images training set, \n",
    "* 5,000 images validation set and \n",
    "* 10,000 images test set.\n",
    "\n",
    "The images are 28x28 pixels, where each pixel represents the light intensity normalised value between 0-1 (0=black, 1=white).\n",
    "The number of features is therefore 28x28=784.\n",
    "\n",
    "First let's load the MNIST dataset and plot a few examples. TensorFlow has a [convenient MNIST interface](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/datasets/mnist.py) that we will use to download and manage the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data (download if you haven't already)\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist_data = input_data.read_data_sets('MNIST_data', \n",
    "                                       one_hot=True,   # Convert the labels into one hot encoding\n",
    "                                       dtype='float32' # rescale images to `[0, 1]`\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Print dataset statistics and visualize\n",
    "print(\"\"\"Information on dataset\n",
    "----------------------\"\"\")\n",
    "print(\"Training size:\\t\", mnist_data.train.num_examples)\n",
    "print(\"Test size\\t\", mnist_data.test.num_examples)\n",
    "print(\"Validation size\\t\", mnist_data.validation.num_examples)\n",
    "\n",
    "num_features = mnist_data.train.images[0].shape[0]\n",
    "num_classes = mnist_data.train.labels[0].shape[0]\n",
    "\n",
    "print('\\nData summaries')\n",
    "print(\"Image shape\\t\\t\", num_features)\n",
    "print(\"Image type\\t\\t\", type(mnist_data.train.images[0][0]))\n",
    "print(\"Image min/max value\\t\", np.min(mnist_data.train.images), '/', np.max(mnist_data.train.images))\n",
    "print(\"Label shape\\t\\t\", num_classes)\n",
    "print(\"Label type\\t\\t\", type(mnist_data.train.labels[0][0]))\n",
    "\n",
    "\n",
    "## Plot a few MNIST examples\n",
    "img_to_show = 15\n",
    "idx = 0\n",
    "canvas = np.zeros((28*img_to_show, img_to_show*28))\n",
    "for i in range(img_to_show):\n",
    "    for j in range(img_to_show):\n",
    "        canvas[i*28:(i+1)*28, j*28:(j+1)*28] = mnist_data.train.images[idx].reshape((28, 28))\n",
    "        idx += 1\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.axis('off')\n",
    "plt.imshow(canvas, cmap='gray')\n",
    "plt.title('MNIST handwritten digits')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the graph\n",
    "\n",
    "#### <span style=\"color:red\"> EXE 1) Defining the network\n",
    "</span>\n",
    "\n",
    "In the cell below, at the place that says `### YOUR CODE HERE ###` implement the following:\n",
    "\n",
    "* Define two layers. Remember to give them proper names.\n",
    "    * Choose any standard nonlinearity. You are encouraged to come back, and compare their performances once you have defined the training loop.\n",
    "* The non-linearity of the output layer should be softmax ([tf.nn.softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Build the network\n",
    "tf.reset_default_graph()\n",
    "\n",
    "## Define placeholders\n",
    "x_pl = tf.placeholder(tf.float32, [None, num_features], name='xPlaceholder')\n",
    "y_pl = tf.placeholder(tf.float64, [None, num_classes], name='yPlaceholder')\n",
    "# Depending on your implementation you might need to cast y_pl differently\n",
    "y_pl = tf.cast(y_pl, tf.float32)\n",
    "\n",
    "## Define the model\n",
    "\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"> EXE 2) Defining the training operations.\n",
    "</span>\n",
    "\n",
    "In the cell below, at the place that says `### YOUR CODE HERE ###` implement the following:\n",
    "\n",
    "* Cross_entropy loss\n",
    "* An optimizer and training op\n",
    "* An op computing the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Implement training ops\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "# 1) Define cross entropy loss\n",
    "# 2) Define the training op\n",
    "# 3) Define accuracy op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent with mini-batches\n",
    "\n",
    "In the last notebook we used the entire dataset to compute every parameter update.\n",
    "For large datasets this is very costly and inefficient.\n",
    "Instead we can compute the gradient on only a small subset of the data, a *mini-batches*, and use it to update the parameters.\n",
    "Bath size varies a lot depending on application but between 8-256 is common.\n",
    "Bigger is not allways better.\n",
    "With smaller mini-batch size you get more updates and your model might converge faster.\n",
    "Also small batchsizes uses less memory -> freeing up memory so you train a model with more parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"> EXE 3) Testing the forward pass\n",
    "</span>\n",
    "\n",
    "Before building the training loop lets ensure that the forward pass works.\n",
    "\n",
    "In the cell below, at the place that says `### YOUR CODE HERE ###` implement the following:\n",
    "\n",
    "* define the `feed_dict`\n",
    "* compute the network predictions (`y_pred`) by passing `x_batch` through the network using `sess.run`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Test the forward pass\n",
    "batch_size = 32\n",
    "\n",
    "# Get a mini-batch\n",
    "x_batch, y_batch = mnist_data.train.next_batch(batch_size)\n",
    "\n",
    "# Restricting memory usage, TensorFlow is greedy and will use all memory otherwise\n",
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    feed_dict = \n",
    "    y_pred = sess.run( #... )\n",
    "\n",
    "        \n",
    "assert y_pred.shape == y_batch.shape, \"ERROR the output shape is not as expected!\" \\\n",
    "        + \" Output shape should be \" + str(y.shape) + ' but was ' + str(y_pred.shape)\n",
    "\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the training loop.\n",
    "We train the network by calculating the gradient w.r.t the cost function and update the parameters in direction of the negative gradient. \n",
    "\n",
    "\n",
    "When training neural network you always use mini batches.\n",
    "Instead of calculating the average gradient using the entire dataset you approximate the gradient using a mini-batch of typically 16 to 256 samples.\n",
    "The paramters are updated after each mini batch.\n",
    "Networks converges much faster using minibatches because the paramters are updated more often.\n",
    "\n",
    "\n",
    "\n",
    "#### <span style=\"color:red\"> EXE 4) Building the training loop.\n",
    "</span>\n",
    "\n",
    "In the cell below, at the place that says `### YOUR CODE HERE ###` implement the following:\n",
    "\n",
    "* Perform the training operation\n",
    "* Collect training statistics\n",
    "* [Optional] Monitor training in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "\n",
    "mini_batch_per_epoch = int(mnist_data.train.num_examples/max_epochs)\n",
    "\n",
    "# restricting memory usage, TensorFlow is greedy and will use all memory otherwise\n",
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "\n",
    "train_cost, val_cost, train_acc, val_acc = [],[],[],[]\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:    \n",
    "    try:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for e in range(max_epochs):\n",
    "#         while mnist_data.train.epochs_completed < max_epochs:\n",
    "            for mb in range(mini_batch_per_epoch):\n",
    "                # Load one mini-batch\n",
    "                x_batch, y_batch = mnist_data.train.next_batch(batch_size)\n",
    "                ### YOUR CODE HERE ###\n",
    "                # 1) Run the train op\n",
    "\n",
    "\n",
    "            ### YOUR CODE HERE ###\n",
    "            # (outside the mini-batch loop)\n",
    "            # 2) Compute train_cost, val_cost, train_acc, val_acc\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('KeyboardInterrupt')\n",
    "\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"> EXE 5) Building the training loop.\n",
    "</span>\n",
    "\n",
    "You should have saved training statistics in `train_cost`, `val_cost`, `train_acc`, `val_acc`.\n",
    "1. Plot train and validation loss as a function of time\n",
    "2. Plot train and validation accuracy as a function of time\n",
    "3. Select some random validation images, visualize them, and examine the output of the network.\n",
    "    * How well does it do? When does it make mistakes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "# 1) Plot train and validation loss as a function of time\n",
    "# 2) Plot train and validation accuracy as a function of time\n",
    "# 3) Select some random validation images, visualize them, and examine the output of the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Closing Remarks\n",
    "\n",
    "You have now seen several different ways in which feed forward neural networks can be created.\n",
    "There is no \"right\" way to define a neural network.\n",
    "Ideally you should be able to understand how to implement everything at a low-level, and use high-level APIs to reduce human error and speedup development.\n",
    "There are numerous frameworks which takes care the fundamental concepts are implemented correctly.\n",
    "In this class we will mostly use Tensorflow and show Keras implementations here and there.\n",
    "There are however alternatives that each have their strenghts and weaknesses: \n",
    "- [Pytorch](http://pytorch.org/)\n",
    "- [Theano](http://deeplearning.net/software/theano/)\n",
    "- [CNTK](https://github.com/Microsoft/CNTK)\n",
    "- [Chainer](https://github.com/chainer/chainer)\n",
    "- ... etc.\n",
    "\n",
    "There is no framework that is \"better\" than the others.\n",
    "However, each has their particular focus.\n",
    "Keras tries to be the easiest to use and takes the most \"High-level\" approach to Deep Learning.\n",
    "This might be useful at times, however, at others you might want to have more flexibility than such a High-level framework can provide. \n",
    "\n",
    "As you will see, FFNs are just one of many types of architectures and from here we will take a look at the extremely useful alternative called Convolutional Neural Networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
