{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Network in pure Python\n",
    "> <span style=\"color:gray\"> Created by Mikkel Vilstrup ([mvil](https://github.com/MVilstrup))\n",
    "\n",
    "This notebook demonstrates how to implement a neural network from scratch using basic programming concepts and pure Python.\n",
    "It is intended for people who are not fully comfortable with linear algebra in order to show what is going on in \"simpler\" terms. \n",
    "\n",
    "We construct the network without the convenience of linear algebra and use simple for-loops instead. \n",
    "The problem we are going to solve is the **half-moons problem**. \n",
    "This problem is used because of it is simple yet cannot be solved with linear functions alone, e.g. Logistic Regression.\n",
    "It is also convenient because models can be trained quickly and the results can be visualized easily.\n",
    "\n",
    "In order to solve it with a neural network we will go through:\n",
    "* Selecting a non-linear activation function\n",
    "* Implementing the forward propagation\n",
    "* Implementing the backward propagation\n",
    "* Training the network on the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import some librarires that we need\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join('.', '..')) # Allow us to import shared custom \n",
    "                                         # libraries, like utils.py\n",
    "from intro_utils import (onehot, mesh_grid, prediction_contours)\n",
    "    \n",
    "from sklearn.datasets import make_moons\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "from time import sleep\n",
    "from sklearn.metrics import precision_score, accuracy_score\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The half-moon data set\n",
    "\n",
    "Before begining to model it is always a good idea to explore the data.\n",
    "\n",
    "It is impossible to know when to stop a neural network from training using only one data set.\n",
    "The neural network might get to 99% accuracy on the training set, while still having very poor performance on new data.\n",
    "In this case it has **overfitted** to features in the training set that is unique to this particular partition of the data, and not the data as a whole.\n",
    "\n",
    "In order to overcome this problem we tend to extract a tiny partition of the dataset which we call the **validation set**.\n",
    "We can then use the accuracy of the validation set to judge when it is appropriate to stop the training. \n",
    "This set is not used to update the parameters, but monitor the progress during training, which is an indirect form of training.\n",
    "We must therefore create yet another partition, the test set, which we are only allowed to use as the final validation of the model.\n",
    "The **test set** is used for the results that are reported\n",
    "\n",
    "In this notebook we are lazy and combine the test/validation set, but in general this is a no-no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Load and visualize the half-moon data set\n",
    "train_size = 300\n",
    "test_size = 60\n",
    "noise = 0.3\n",
    "\n",
    "X_train, Y_train = make_moons(train_size, noise=noise)\n",
    "X_test, Y_test = make_moons(test_size, noise=noise)\n",
    "\n",
    "train_data = list(zip(X_train, onehot(Y_train, 2)))\n",
    "test_data = list(zip(X_test, onehot(Y_test, 2)))\n",
    "\n",
    "plt.scatter(X_train[:,0], X_train[:,1], s=40, c=Y_train, cmap=plt.cm.coolwarm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependancies and supporting functions\n",
    "Load all dependancies and supporting functions by running the code block below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing variables\n",
    "The parameters in a neural networ are initialized randomly following some distribution.\n",
    "In our extremely simplified neural network we only need to know three things:\n",
    "* The amount of features of the data we train on (2 in our case)\n",
    "* The number of hidden units we want in our hidden layer\n",
    "* The number of possible classes available in the dataset (2 in our case)\n",
    "\n",
    "With this knowledge we can go about and define the variables we need.\n",
    "In order to make this network as simple as possible we will save a lot of information explicitly. \n",
    "There are two things to pay attention to in the code below:\n",
    "* the sizes of the weights and \n",
    "* way the the bias is implemented. You will always add a bias when you multiply a data feature with a weight during training.\n",
    "In the following notebooks this will be done explicitly.\n",
    "However, here we do it implicitly by adding a single neuron which we will not update ourselves.\n",
    "This node will end up doing the same as a bias since all weights will be summed together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize(input_dimensions, hidden_dimensions, output_dimensions):\n",
    "    input_nodes = input_dimensions + 1 # We add one to have room for bias node\n",
    "    hidden_nodes = hidden_dimensions\n",
    "    output_nodes = output_dimensions\n",
    "\n",
    "    # Initilize arrays of 1s for activations \n",
    "    # These need to be ones since we will multiply them with the inputs\n",
    "    input_activations = np.ones(input_nodes)\n",
    "    hidden_activations = np.ones(hidden_nodes)\n",
    "    output_activations = np.ones(output_nodes)\n",
    "\n",
    "    # Initialize randomized weights\n",
    "    # This initialization is very simplistic and naive. Normally we would need \n",
    "    # to think more about the initial values since they impact the performance of the algorithm\n",
    "    # However, Deep learning frameworks tend to do this for us\n",
    "    input_weights = np.random.randn(input_nodes, hidden_nodes)\n",
    "    output_weights = np.random.randn(hidden_nodes, output_nodes)\n",
    "\n",
    "    # Initialize a cache used to store the last step size in backpropagation\n",
    "    input_stepsize = np.zeros((input_nodes, hidden_nodes)) # Input x Hidden\n",
    "    output_stepsize = np.zeros((hidden_nodes, output_nodes)) # Hidden x Output\n",
    "    \n",
    "    return (input_nodes, hidden_nodes, output_nodes, input_activations, hidden_activations, \n",
    "            output_activations, input_weights, output_weights, input_stepsize, output_stepsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing activation functions\n",
    "\n",
    "With the variables in place we need to choose a non-linearity for our activation function.\n",
    "Using a non-linear activation function is fundamental for deep learning.\n",
    "This is because all other parts of deep learning is simply mulitplying weights to the input which is a linear function.\n",
    "Without a non-liniear activation function any network regardless of its size or shape would thus only be able to represent linear functions.\n",
    "\n",
    "Activation functions are still a hot research topic and new activation functions appear regularly.\n",
    "However, there are a few which have been proven to work over and over again.\n",
    "Of these Sigmoid and TanH are the most classical choices, and was the preffered activation functions for many years. \n",
    "\n",
    "Recently the trend is moving towards more minimalistic activation functions which generally only differ from linear functions when the input is less than zero.\n",
    "In theory any non-linear function can be used as long as its derivative can be computed efficiently.\n",
    "However, in practice the functions shown below tend to do very well, and one should have a really good reason to try and come up with a new activation function instead of just using one of the existing.\n",
    "\n",
    "Below several common nonliear activation functions are implemented and described.\n",
    "Remember to use the TensorFlow implemention in your code, NOT the implementaions below!\n",
    "TensorFlow provides the [most commonly used nonliearities](https://www.tensorflow.org/api_guides/python/nn).\n",
    "Wikipedia has a [much longer list](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions) that can be fun to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sigmoid Activation Function\n",
    "# Sigmoid was initially the most common choice for activation functions. However, it was shown to be much worse than\n",
    "# the activation functions below. Now it is used for gating and to approximate percentages \n",
    "# (by setting the value between 0 and 1)\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "# TanH Activation Function\n",
    "# TanH was also a very popular activation function. It is still the activation function of choice in \n",
    "# Recurrent Neural Networks (RNNs) but it is almost never used in Convolutional Neural Networks (CNNs) or \n",
    "# Feed Forward Neural Netowrks (FFNs).\n",
    "def tanh(x):\n",
    "    return (1 - np.exp(-2*x)) / (1 + np.exp(-2*x))\n",
    "\n",
    "# Relu Activation Function\n",
    "# Relus are likely the most common activation functions used in CNNs and FFNs. It seems to be a good choice\n",
    "# since all the best performing models use it. The theory of why it works is that it is seemingly the simplest\n",
    "# Non-linearity possible. This is because it is linear in all positive values of x. \n",
    "# However, sometimes one encounter the problem of dead neurons with Relu where the output of the neuron will be zero\n",
    "# no matter the input. \n",
    "def relu(x):\n",
    "    return max(0., x)\n",
    "\n",
    "# Leaky Relu Activation Function\n",
    "# Leaky Relu is almost the same as Relu. It sometimes works better than Relu, but the main advantage is that\n",
    "# neurons cannot die since it can output both positive and negative values. \n",
    "def leaky_relu(x):\n",
    "    return max(0., x) + 0.1 * min(0, x)\n",
    "\n",
    "# Elu Activation function\n",
    "# https://arxiv.org/pdf/1511.07289.pdf\n",
    "# Elu also resembles Relu quite a lot. The biggest difference is that Relu is capped at 0, \n",
    "# whereas Elu is capped at -1.\n",
    "def elu(x, alpha=1):\n",
    "    return x if x >= 0 else alpha * (np.exp(x) - 1)\n",
    "\n",
    "# Selu activatiosn function\n",
    "# Selu is only shown here because it is the newest kid on the block from June 8, 2017. \n",
    "# The aim is to add a self-normalizing property to the neural network in order to decrease variance \n",
    "# and training errors. The Selu activation push neuron activations to zero mean and unit variance thereby \n",
    "# leading to the same effect as batch normalization, which enables to robustly learn many layers. \n",
    "# However, in order to use it we need to handle weight initialization properly which we do not do here. \n",
    "# https://arxiv.org/pdf/1706.02515.pdf\n",
    "def selu(x, alpha=1.6732632423543772848170429916717, scale=1.0507009873554804934193349852946):\n",
    "    return scale * x if x > 0 else scale * (alpha * np.exp(x) - alpha)\n",
    "    \n",
    "X = np.linspace(-6, 6, 100)\n",
    "labels = ['Sigmoid',\n",
    "          'TanH',\n",
    "          'Relu',\n",
    "          'L-Relu',\n",
    "          'ELU',\n",
    "          'Selu']\n",
    "data = [[sigmoid(x) for x in X],\n",
    "        [tanh(x) for x in X],\n",
    "        [relu(x) for x in X],\n",
    "        [leaky_relu(x) for x in X],\n",
    "        [elu(x) for x in X],\n",
    "        [selu(x) for x in X]]\n",
    "\n",
    "rows = 2\n",
    "columns = int(len(labels) / rows)\n",
    "fig, axn = plt.subplots(rows, columns, figsize=(8*columns,4*rows), sharey=True, sharex=True)\n",
    "plt.suptitle('Common Non-linearities', fontsize=20)\n",
    "\n",
    "for i, row in enumerate(axn):\n",
    "    for j, ax in enumerate(row):\n",
    "        index = i*columns+j\n",
    "        ax.plot(X, data[index], label=labels[index], lw=2)\n",
    "        ax.legend(loc=2, fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the reasons why the field moved away from Sigmoid and TanH is the dissapearing gradients when the absolute value of the weights increases.\n",
    "It's only in a small range centered around 0, that the gradient of the activation functions is big enough for efficient learning to occur.\n",
    "When the absolute value increases the gradient vanishes which influences the learning curve. \n",
    "\n",
    "With the newer functions which are linear for positive weights the gradient only vanishes with negative weights.\n",
    "This has been shown to have a positive effect on the convergence rate.\n",
    "\n",
    "A general advice is thus to start out with the Relu activation function and then try the other functions if the performance does not live up to expectations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradients of activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.linspace(-6, 6, 100)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def tanh_derivative(y):\n",
    "    return 1 - np.square(tanh(y))\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return 1. if x > 0. else 0.\n",
    "\n",
    "def leaky_relu_derivative(x):\n",
    "    return 1 if x >= 0 else 0.01\n",
    "\n",
    "def elu_derivative(x, alpha=1):\n",
    "    return 1 if x >= 0 else elu(x, alpha) + alpha\n",
    "\n",
    "def selu_derivative(x, alpha=1.6732632423543772848170429916717, scale=1.0507009873554804934193349852946):\n",
    "    return scale if x > 0 else scale * alpha * np.exp(x)\n",
    "\n",
    "labels = ['Sigmoid Derivative',\n",
    "          'TanH Derivative',\n",
    "          'Relu Derivative',\n",
    "          'L-Relu Derivative',\n",
    "          'ELU Derivative',\n",
    "          'Selu Derivative']\n",
    "data = [[sigmoid_derivative(x) for x in X],\n",
    "        [tanh_derivative(x) for x in X],\n",
    "        [relu_derivative(x) for x in X],\n",
    "        [leaky_relu_derivative(x) for x in X],\n",
    "        [elu_derivative(x) for x in X],\n",
    "        [selu_derivative(x) for x in X]]\n",
    "\n",
    "rows = 2\n",
    "columns = int(len(labels) / rows)\n",
    "fig, axn = plt.subplots(rows, columns, figsize=(8*columns,4*rows), sharey=True, sharex=True)\n",
    "plt.suptitle('Derivatives', fontsize=20)\n",
    "\n",
    "for i, row in enumerate(axn):\n",
    "    for j, ax in enumerate(row):\n",
    "        index = i*columns+j\n",
    "        ax.plot(X, data[index], label=labels[index], lw=2)\n",
    "        ax.legend(loc=2, fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "With the variables initialized and the activation function chosen. We are ready to define the forward propagation function used to map the input to a prediction. \n",
    "The forward propagation is a relatively simple function.\n",
    "In each layer we multiply the input with the weights and add a bias (implicitly).\n",
    "The result of each of these linear operations we transform with our non-linear activation function. \n",
    "\n",
    "In the final layer the output can either be out through a linear function (typically used for regression) or through a non-linear function such as we do here (typically used for classification). For classification the most normal function to use is [Softmax](https://en.wikipedia.org/wiki/Softmax_function) which transforms the output into a propability distributions indicating the likelyhood of the input belonging to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(inputs):\n",
    "    if len(inputs) != input_nodes - 1:\n",
    "        raise ValueError(\"Wrong number of inputs ;) ({} instead of {})\".format(len(inputs), input_nodes - 1))\n",
    "\n",
    "    # Set all input activations to be equal to new inputs\n",
    "    for i in range(input_nodes - 1): # we subtract one in order not to touch the last element (the bias)\n",
    "        input_activations[i] = inputs[i]\n",
    "\n",
    "    # Calculate the weighted value for each hidden neuron\n",
    "    # Remember that in FFNs each neuron is connected to all neurons in the previous layer\n",
    "    for i in range(hidden_nodes):\n",
    "        value = 0.0\n",
    "        for j in range(input_nodes):\n",
    "            value += input_activations[j] * input_weights[j][i]\n",
    "\n",
    "        hidden_activations[i] = relu(value) # Apply the non-linear function to the weighted value\n",
    "\n",
    "    # Calculate the weighted value for each output neuron\n",
    "    # !! Notice: This is almost the same as the step before !!\n",
    "    for i in range(output_nodes):\n",
    "        value = 0.0\n",
    "        for j in range(hidden_nodes):\n",
    "            value += hidden_activations[j] * output_weights[j][i]\n",
    "\n",
    "        # Apply the sigmoid function to convert each activation to a value between 0 and 1.\n",
    "        output_activations[i] = sigmoid(value)\n",
    "\n",
    "    return output_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation\n",
    "Backpropagation is one of the primary reasons why deep learning is computationally tractable.\n",
    "It is used to compute how much each weight contribute to the error of the network. \n",
    "This knowledge combined with an optimization algorithm is used to figure out which weights to tune and how much. \n",
    "\n",
    "We will not go into detail with backpropagation here, but only show how it can be written in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagation(targets, learning_rate):\n",
    "    if len(targets) != output_nodes:\n",
    "        raise ValueError(\"Wrong number of targets ;) ({} instead of {})\".format(len(targets), output_nodes))\n",
    "\n",
    "    # Initialize deltas cache. This cache is important since it would otherwise not be feasible to \n",
    "    # train deep neural networks. This is beacuse the derivative of any layer is dependent on the derivative\n",
    "    # of the succeding layer.\n",
    "    output_deltas = np.zeros(output_nodes)\n",
    "    hidden_deltas = np.zeros(hidden_nodes)\n",
    "\n",
    "    # Update the deltas for the output layer (As the algorithms' name applies we start from the back of the network)\n",
    "    for i in range(output_nodes):\n",
    "        error = -(targets[i] - output_activations[i]) \n",
    "        output_deltas[i] = sigmoid_derivative(output_activations[i]) * error\n",
    "\n",
    "    # Update the deltas or the hidden layer\n",
    "    for i in range(hidden_nodes):\n",
    "        error = 0.0\n",
    "        for j in range(output_nodes):\n",
    "            error += output_deltas[j] * output_weights[i][j] # Here we use the cache\n",
    "\n",
    "        hidden_deltas[i] = relu_derivative(hidden_activations[i]) * error\n",
    "\n",
    "    # ------------------ END OF BACK PROPAGATION ------------------\n",
    "    # From here we update the weights. \n",
    "    # This is the optimization algorithm and should not be confused with backpropagation. \n",
    "    # We just include the two in one function for simplicity\n",
    "    # We use Stochastic Gradient Descent (SGD) as optimisation algorithm\n",
    "    \n",
    "    \n",
    "    # Update the weights connecting the hidden layer to the output layer\n",
    "    for i in range(hidden_nodes):\n",
    "        for j in range(output_nodes):\n",
    "            change = output_deltas[j] * hidden_activations[i]\n",
    "            output_weights[i][j] -= learning_rate * change + output_stepsize[i][j]\n",
    "            output_stepsize[i][j] = change\n",
    "\n",
    "    # Update the weights connecting the input layer to the hidden layer\n",
    "    for i in range(input_nodes):\n",
    "        for j in range(hidden_nodes):\n",
    "            change = hidden_deltas[j] * input_activations[i]\n",
    "            input_weights[i][j] -= learning_rate * change + input_stepsize[i][j]\n",
    "            input_stepsize[i][j] = change\n",
    "\n",
    "    # Calculate Mean Squared Error\n",
    "    # This is a very simple cost function and is almost never used for classification purposes\n",
    "    # However, we will use it here anyway for simplicity\n",
    "    error = sum((targets - output_activations) ** 2) / len(targets)\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Prediction\n",
    "\n",
    "With forward and backward propagation defined, the training the network is really simple.\n",
    "One simply iterates through each input and propagate it through the network with forward propagation.\n",
    "From here we calculate the error, and run back propagation to update the weights accordingly. \n",
    "\n",
    "The procedure for making predictions with the network is then just a single forward propagation through the network.\n",
    "(We then run an argmax on the output vector to convert it back into the original classes defined by an integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(patterns, epochs, learning_rate):\n",
    "    error_per_epoch = []\n",
    "    for i in range(epochs):\n",
    "        error = 0.0\n",
    "\n",
    "        for p in patterns:\n",
    "            inputs, targets = p\n",
    "\n",
    "            forward_propagation(inputs)\n",
    "            error += backward_propagation(targets, learning_rate)\n",
    "            \n",
    "        error_per_epoch.append(error) \n",
    "    \n",
    "    return error_per_epoch\n",
    "\n",
    "def predict(X):\n",
    "    predictions = []\n",
    "\n",
    "    for x in X:\n",
    "        pred = forward_propagation(x)\n",
    "        pred = np.argmax(pred)\n",
    "        predictions.append(pred)\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above the code for training a network is quite simple when forward/back-propagation is defined (as well as cost function and choice of optimization funciton).\n",
    "However, It is here we encounter one of the hardest parts of working with neural networks.\n",
    "What is going on inside the network??\n",
    "\n",
    "This is still an unsolved problem and many researchers spend all their time trying to come up with novel ways to get insights into why a network \"acts\" as it does.\n",
    "\n",
    "In order to give you a slight insight into this network we have included another training function which does the exact same thing as the one above, except it includes a visualisation of the training data, evaluation data and the error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_with_animation(train_data, test_data, epochs = 10, learning_rate = 0.002, plot_step=2):\n",
    "    num_frames = epochs\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,8))\n",
    "    ax1.set_title(\"Train Targets\")\n",
    "    \n",
    "    X_train, Y_train_orig = zip(*train_data)\n",
    "    Y_train = [np.argmax(l) for l in Y_train_orig]\n",
    "    X_train = np.array(X_train)\n",
    "\n",
    "    # create a mesh to plot in\n",
    "    xx_train, yy_train = mesh_grid(X_train)\n",
    "\n",
    "    \n",
    "    ax1.scatter(X_train[:, 0], X_train[:, 1], s=60, cmap=plt.cm.coolwarm)\n",
    "    \n",
    "    X_test, Y_test_orig = zip(*test_data)\n",
    "    Y_test = [np.argmax(l) for l in Y_test_orig]\n",
    "    X_test = np.array(X_test)\n",
    "    \n",
    "    # create a mesh to plot in\n",
    "    xx_test, yy_test = mesh_grid(X_test)\n",
    "    \n",
    "    test_scat = ax2.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, s=60, cmap=plt.cm.coolwarm)\n",
    "\n",
    "    cost = []\n",
    "    for i in range(1, epochs + 1):\n",
    "        error = 0.0\n",
    "        \n",
    "        for p in train_data:\n",
    "            inputs, targets = p\n",
    "            \n",
    "            forward_propagation(inputs)\n",
    "            error += backward_propagation(targets, learning_rate)\n",
    "        \n",
    "        plt.suptitle(\"Epoch: {}\".format(i), fontsize=14, fontweight='bold')\n",
    "        cost.append(error)\n",
    "        if i % plot_step == 0:\n",
    "            ax1.cla()\n",
    "            pred_train = predict(X_train)\n",
    "            train_precision = precision_score(Y_train, pred_train)\n",
    "            train_accuracy = accuracy_score(Y_train, pred_train)\n",
    "            ax1.set_title(\"Train Precision: {0:0.2f}, Train Accuracy: {1:0.2f}\".format(train_precision, train_accuracy))\n",
    "            ax1.scatter(X_train[:, 0], X_train[:, 1], s=60, c=Y_train, cmap=plt.cm.coolwarm)\n",
    "            ax1.contourf(xx_train, yy_train, prediction_contours(xx_train, yy_train, predict), cmap=plt.cm.coolwarm, alpha=0.3)\n",
    "\n",
    "            ax2.cla()\n",
    "            pred_test = predict(X_test)\n",
    "            test_precision = precision_score(Y_test, pred_test)\n",
    "            test_accuracy = accuracy_score(Y_test, pred_test)\n",
    "            ax2.set_title(\"Test Precision: {0:0.2f}, Test Accuracy: {1:0.2f}\".format(test_precision, test_accuracy))\n",
    "            ax2.scatter(X_test[:, 0], X_test[:, 1], s=60, c=Y_test, cmap=plt.cm.coolwarm)\n",
    "            ax2.contourf(xx_test, yy_test, prediction_contours(xx_test, yy_test, predict), cmap=plt.cm.coolwarm, alpha=0.3)\n",
    "\n",
    "\n",
    "            ax3.cla()\n",
    "            ax3.set_title(\"Mean Squared Error: {0:0.2f}\".format(error))\n",
    "            ax3.plot(cost, label=\"Training Cost\")\n",
    "            ax3.legend()\n",
    "\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally all we need is to define the hyperparameters.\n",
    "This is simple in our case, since we have so few parameters.\n",
    "However in more complex networks this can be a daunting task.\n",
    "In many cases researchers end up using search algorithms to train the network with various parameters automatically instead of trying to fiddle with the parameters themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "input_dimensions = 2 # N.B - This is fixed in our case\n",
    "hidden_dimensions = 10 \n",
    "output_dimensions = 2 # N.B - This is fixed in our case\n",
    "\n",
    "epochs = 120\n",
    "plot_steps = 5\n",
    "learning_rate = 0.1\n",
    "\n",
    "input_nodes, hidden_nodes, output_nodes,\\\n",
    "    input_activations, hidden_activations, output_activations,\\\n",
    "    input_weights, output_weights,\\\n",
    "    input_stepsize, output_stepsize = initialize(input_dimensions, hidden_dimensions, output_dimensions)\n",
    "\n",
    "train_with_animation(train_data, test_data, epochs=epochs, learning_rate=learning_rate, plot_step=plot_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignments\n",
    "1) Investigate the Non-Linearities\n",
    "* Try the various activation functions and see how the network behaves\n",
    "    * Which activation function achieves the lowest training cost?\n",
    "    * Which activation function seems most stable?\n",
    "* Try changing various hyper parameters\n",
    "    * Does more/less hidden_dimensions help perfomance?\n",
    "    * Does more training data (train_size) help?\n",
    "    * What about learning rate? (Try making it very small or very large  *(e.g 0.00001 or 1.0)*)\n",
    "    \n",
    "2) What is the difference between precision and accuracy and why are they sometimes very different?</span>\n",
    "\n",
    "3) What does the value Mean Squared Error (shown in the left most graph) represent in our network?</span>\n",
    "\n",
    "**Extra:** Try to add another layer to the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
