{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function \n",
    "\n",
    "%matplotlib inline \n",
    "# %matplotlib nbagg\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from data_generator_tensorflow import get_batch, print_valid_characters\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join('.', '..')) \n",
    "import utils \n",
    "\n",
    "import tf_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Decoder\n",
    "> <span style=\"color:gray\">\n",
    "Original [Theano/Lasagne tutorial](https://github.com/DeepLearningDTU/nvidia_deep_learning_summercamp_2016/) by \n",
    "Lars Maaløe ([larsmaaloee](https://github.com/larsmaaloee)),\n",
    "Søren Kaae Sønderby ([skaae](https://github.com/skaae)), and \n",
    "Casper Sønderby ([casperkaae](https://github.com/casperkaae)). \n",
    "Converted to TensorFlow by \n",
    "Alexander R. Johansen ([alrojo](https://github.com/alrojo)), \n",
    "and updated by \n",
    "Toke Faurby ([faur](https://github.com/Faur)).\n",
    "> </span>\n",
    "\n",
    "Soft attention for recurrent neural networks have recently attracted a lot of interest.\n",
    "These methods let the Decoder model selective focus on which part of the encoder sequence it will use for each decoded output symbol.\n",
    "This relieves the encoder from having to compress the input sequence into a fixed size vector representation passed on to the decoder.\n",
    "Secondly we can interrogate the decoder network about where it attends while producing the ouputs.\n",
    "below we'll implement an decoder with selective attention and show that it significantly improves the performance of the toy translation task.\n",
    "\n",
    "The seminal attention paper is https://arxiv.org/pdf/1409.0473v7.pdf\n",
    "\n",
    "The principle of attention models is:\n",
    "\n",
    "1. Use the encoder to get the hidden represention $\\{h^1_e, ...h^n_e\\}$ for each position in the input sequence.\n",
    "2. For timestep $t$ in the decoder do for $m = 1...n$ : $a_{mt} = f(h^e_m, h^d_t)$. Where f is a function returning a scalar value.\n",
    "4. Weight each $h^e_m$ by its probability $p_{mt}$ and sum to get $h_{in}$.\n",
    "5. Use $h_{in}$ as an additional input to the decoder. $h_{in}$ is recalculated each time the decoder is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# At the bottom of the script there is some code which saves the model.\n",
    "# If you wish to restore your model from a previous state use this function.\n",
    "load_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# resetting the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Setting up hyperparameters and general configs\n",
    "MAX_DIGITS = 10\n",
    "MIN_DIGITS = 5\n",
    "NUM_INPUTS = 27\n",
    "NUM_OUTPUTS = 11 #(0-9 + '#')\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "# try various learning rates 1e-2 to 1e-5\n",
    "LEARNING_RATE = 0.005\n",
    "X_EMBEDDINGS = 8\n",
    "t_EMBEDDINGS = 8\n",
    "NUM_UNITS_ENC = 16\n",
    "NUM_UNITS_DEC = 16\n",
    "NUM_UNITS_ATTN = 16\n",
    "\n",
    "\n",
    "# Setting up placeholders, these are the tensors that we \"feed\" to our network\n",
    "Xs = tf.placeholder(tf.int32, shape=[None, None], name='X_input')\n",
    "ts_in = tf.placeholder(tf.int32, shape=[None, None], name='t_input_in')\n",
    "ts_out = tf.placeholder(tf.int32, shape=[None, None], name='t_input_out')\n",
    "X_len = tf.placeholder(tf.int32, shape=[None], name='X_len')\n",
    "t_len = tf.placeholder(tf.int32, shape=[None], name='X_len')\n",
    "t_mask = tf.placeholder(tf.float32, shape=[None, None], name='t_mask')\n",
    "\n",
    "# Building the model\n",
    "\n",
    "# first we build the embeddings to make our characters into dense, trainable vectors\n",
    "X_embeddings = tf.get_variable('X_embeddings', [NUM_INPUTS, X_EMBEDDINGS],\n",
    "                               initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "t_embeddings = tf.get_variable('t_embeddings', [NUM_OUTPUTS, t_EMBEDDINGS],\n",
    "                               initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "\n",
    "# setting up weights for computing the final output\n",
    "W_out = tf.get_variable('W_out', [NUM_UNITS_DEC, NUM_OUTPUTS])\n",
    "b_out = tf.get_variable('b_out', [NUM_OUTPUTS])\n",
    "\n",
    "X_embedded = tf.gather(X_embeddings, Xs, name='embed_X')\n",
    "t_embedded = tf.gather(t_embeddings, ts_in, name='embed_t')\n",
    "\n",
    "# forward encoding\n",
    "enc_cell = tf.nn.rnn_cell.GRUCell(NUM_UNITS_ENC)#python.ops.rnn_cell.GRUCell\n",
    "enc_out, enc_state = tf.nn.dynamic_rnn(cell=enc_cell, inputs=X_embedded,\n",
    "                                 sequence_length=X_len, dtype=tf.float32)\n",
    "# use below in case TF's does not work as intended\n",
    "#enc_state, _ = tf_utils.encoder(X_embedded, X_len, 'encoder', NUM_UNITS_ENC)\n",
    "#\n",
    "#enc_state = tf.concat(1, [enc_state, enc_state])\n",
    "\n",
    "# decoding\n",
    "# note that we are using a wrapper for decoding here, this wrapper is hardcoded to only use GRU\n",
    "# check out tf_utils to see how you make your own decoder\n",
    "dec_out, dec_out_valid, alpha_valid = \\\n",
    "    tf_utils.attention_decoder(enc_out, X_len, enc_state, t_embedded, t_len,\n",
    "                               NUM_UNITS_DEC, NUM_UNITS_ATTN, t_embeddings,\n",
    "                               W_out, b_out)\n",
    "\n",
    "# reshaping to have [batch_size*seqlen, num_units]\n",
    "out_tensor = tf.reshape(dec_out, [-1, NUM_UNITS_DEC])\n",
    "out_tensor_valid = tf.reshape(dec_out_valid, [-1, NUM_UNITS_DEC])\n",
    "\n",
    "# computing output\n",
    "out_tensor = tf.matmul(out_tensor, W_out) + b_out\n",
    "out_tensor_valid = tf.matmul(out_tensor_valid, W_out) + b_out\n",
    "\n",
    "# reshaping back to sequence\n",
    "b_size = tf.shape(X_len)[0] # use a variable we know has batch_size in [0]\n",
    "seq_len = tf.shape(t_embedded)[1] # variable we know has sequence length in [1]\n",
    "num_out = tf.constant(NUM_OUTPUTS) # casting NUM_OUTPUTS to a tensor variable\n",
    "\n",
    "out_shape = tf.concat([tf.expand_dims(b_size, 0),\n",
    "                      tf.expand_dims(seq_len, 0),\n",
    "                      tf.expand_dims(num_out, 0)],\n",
    "                      axis=0)\n",
    "out_tensor = tf.reshape(out_tensor, out_shape)\n",
    "out_tensor_valid = tf.reshape(out_tensor_valid, out_shape)\n",
    "\n",
    "## handling shape loss\n",
    "y = out_tensor\n",
    "y_valid = out_tensor_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the cost function, gradient clipping and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_and_acc(preds):\n",
    "    # sequence_loss_tensor is a modification of TensorFlow's own sequence_to_sequence_loss\n",
    "    # TensorFlow's seq2seq loss works with a 2D list instead of a 3D tensors\n",
    "    loss = tf_utils.sequence_loss_tensor(preds, ts_out, t_mask, NUM_OUTPUTS) # notice that we use ts_out here!\n",
    "    # if you want regularization\n",
    "    reg_scale = 0.00001\n",
    "    regularize = tf.contrib.layers.l2_regularizer(reg_scale)\n",
    "    params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "    reg_term = sum([regularize(param) for param in params])\n",
    "    loss += reg_term\n",
    "    # calculate accuracy\n",
    "    argmax = tf.to_int32(tf.argmax(preds, 2))\n",
    "    correct = tf.to_float(tf.equal(argmax, ts_out)) * t_mask\n",
    "    accuracy = tf.reduce_sum(correct) / tf.reduce_sum(t_mask)\n",
    "    return loss, accuracy, argmax\n",
    "\n",
    "loss, accuracy, predictions = loss_and_acc(y)\n",
    "loss_valid, accuracy_valid, predictions_valid = loss_and_acc(y_valid)\n",
    "\n",
    "# use lobal step to keep track of our iterations\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "# pick optimizer, try momentum or adadelta\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "\n",
    "# extract gradients for each variable\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "\n",
    "## add below for clipping by norm\n",
    "#gradients, variables = zip(*grads_and_vars)  # unzip list of tuples\n",
    "#clipped_gradients, global_norm = (\n",
    "#    tf.clip_by_global_norm(gradients, self.clip_norm) )\n",
    "#grads_and_vars = zip(clipped_gradients, variables)\n",
    "\n",
    "## apply gradients and make trainable function\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print all the variable names and shapes\n",
    "# notice that W_z is now packed, such that it contains both W_z_h and W_x_h, this is for optimization\n",
    "# further, we now have W_s, b_s. This is so NUM_UNITS_ENC and NUM_UNITS_DEC does not have to share shape ..!\n",
    "for var in tf.global_variables ():\n",
    "    s = var.name + \" \"*(40-len(var.name))\n",
    "    print (s, var.value().get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate some validation data\n",
    "X_val, X_len_val, t_in_val, t_out_val, t_len_val, t_mask_val, \\\n",
    "text_inputs_val, text_targets_in_val, text_targets_out_val = \\\n",
    "    get_batch(batch_size=5000, max_digits=MAX_DIGITS,min_digits=MIN_DIGITS)\n",
    "print(\"X_val\", X_val.shape)\n",
    "print(\"t_out_val\", t_out_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Start the session\n",
    "# restricting memory usage, TensorFlow is greedy and will use all memory otherwise\n",
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.35)\n",
    "# initialize the Session\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts))\n",
    "# test train part\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "## If you get an error, remove this line! It makes the error message hard to understand.\n",
    "\n",
    "# NOTICE - THIS MIGHT TAKE UPTO 30 MINUTES ON CPU..!\n",
    "# setting up running parameters\n",
    "val_interval = 5000\n",
    "samples_to_process = 2e5\n",
    "samples_processed = 0\n",
    "samples_val = []\n",
    "costs, accs = [], []\n",
    "plt.figure()\n",
    "try:\n",
    "    while samples_processed < samples_to_process:\n",
    "        # load data\n",
    "        X_tr, X_len_tr, t_in_tr, t_out_tr, t_len_tr, t_mask_tr, \\\n",
    "        text_inputs_tr, text_targets_in_tr, text_targets_out_tr = \\\n",
    "            get_batch(batch_size=BATCH_SIZE,max_digits=MAX_DIGITS,min_digits=MIN_DIGITS)\n",
    "        # make fetches\n",
    "        fetches_tr = [train_op, loss, accuracy]\n",
    "        # set up feed dict\n",
    "        feed_dict_tr = {Xs: X_tr, X_len: X_len_tr, ts_in: t_in_tr,\n",
    "             ts_out: t_out_tr, t_len: t_len_tr, t_mask: t_mask_tr}\n",
    "        # run the model\n",
    "        res = tuple(sess.run(fetches=fetches_tr, feed_dict=feed_dict_tr))\n",
    "        _, batch_cost, batch_acc = res\n",
    "        costs += [batch_cost]\n",
    "        samples_processed += BATCH_SIZE\n",
    "        #if samples_processed % 1000 == 0: print batch_cost, batch_acc\n",
    "        #validation data\n",
    "        if samples_processed % val_interval == 0:\n",
    "            #print \"validating\"\n",
    "            fetches_val = [accuracy_valid, y_valid, alpha_valid]\n",
    "            feed_dict_val = {Xs: X_val, X_len: X_len_val, ts_in: t_in_val,\n",
    "             ts_out: t_out_val, t_len: t_len_val, t_mask: t_mask_val}\n",
    "            res = tuple(sess.run(fetches=fetches_val, feed_dict=feed_dict_val))\n",
    "            acc_val, output_val, alp_val = res\n",
    "            samples_val += [samples_processed]\n",
    "            accs += [acc_val]\n",
    "            plt.plot(samples_val, accs, 'b-')\n",
    "            plt.ylabel('Validation Accuracy', fontsize=15)\n",
    "            plt.xlabel('Processed samples', fontsize=15)\n",
    "            plt.title('', fontsize=20)\n",
    "            plt.grid('on')\n",
    "            plt.savefig(\"out_attention.png\")\n",
    "            display.display(display.Image(filename=\"out_attention.png\"))\n",
    "            display.clear_output(wait=True)\n",
    "# NOTICE - THIS MIGHT TAKE UPTO 30 MINUTES ON CPU..!\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot of validation accuracy for each target position\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(np.mean(np.argmax(output_val,axis=2)==t_out_val,axis=0))\n",
    "plt.ylabel('Accuracy', fontsize=15)\n",
    "plt.xlabel('Target position', fontsize=15)\n",
    "#plt.title('', fontsize=20)\n",
    "plt.grid('on')\n",
    "plt.show()\n",
    "#why do the plot look like this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### attention plot, try with different i = 1, 2, ..., 1000\n",
    "i = 42\n",
    "\n",
    "column_labels = map(str, list(t_out_val[i]))\n",
    "row_labels = map(str, (list(X_val[i])))\n",
    "data = alp_val[i]\n",
    "fig, ax = plt.subplots()\n",
    "heatmap = ax.pcolor(data, cmap=plt.cm.Blues)\n",
    "\n",
    "# put the major ticks at the middle of each cell\n",
    "ax.set_xticks(np.arange(data.shape[1])+0.5, minor=False)\n",
    "ax.set_yticks(np.arange(data.shape[0])+0.5, minor=False)\n",
    "\n",
    "# want a more natural, table-like display\n",
    "ax.invert_yaxis()\n",
    "ax.xaxis.tick_top()\n",
    "\n",
    "ax.set_xticklabels(row_labels, minor=False)\n",
    "ax.set_yticklabels(column_labels, minor=False)\n",
    "\n",
    "plt.ylabel('output', fontsize=15)\n",
    "plt.xlabel('Attention plot', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot of average attention weight as a function of the sequence position for each of \n",
    "# the 21 targets in the output sequence i.e. each line is the mean postion of the \n",
    "# attention for each target position.\n",
    "\n",
    "np.mean(alp_val, axis=0).shape\n",
    "plt.figure()\n",
    "plt.plot(np.mean(alp_val, axis=0).T)\n",
    "plt.ylabel('alpha', fontsize=15)\n",
    "plt.xlabel('Input Sequence position', fontsize=15)\n",
    "plt.title('Alpha weights', fontsize=20)\n",
    "plt.legend(map(str,range(1,22)), bbox_to_anchor=(1.125,1.0), fontsize=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Save model\n",
    "# Read more about saving and loading models at https://www.tensorflow.org/programmers_guide/saved_model\n",
    "\n",
    "# Save model\n",
    "save_path = tf.train.Saver().save(sess, \"/tmp/model.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Assignments for the attention decoder\n",
    "1. Explain what the attention plot shows.\n",
    "2. Explain what the alpha weights plot shows.\n",
    " 3. Why are the alpha curve for the first digit narrow and peaked while later digits have alpha curves that are wider and less peaked?\n",
    "4. Why is attention a good idea for this problem? Can you think of other problems where attention is a good choice?\n",
    " 1. Compare the performance and training time (number of samples processed) for the models with and without attention.\n",
    "5. Try setting MIN_DIGITS and MAX_DIGITS to 20\n",
    "6. Enable gradient clipping (under the loss codeblock)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
