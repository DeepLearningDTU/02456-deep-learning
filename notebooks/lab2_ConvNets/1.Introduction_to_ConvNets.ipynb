{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### RUN THIS CELL ASAP!\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "base_model = ResNet50(include_top=False, weights='imagenet')\n",
    "\n",
    "# It will download something we will need later\n",
    "# (it takes a while)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural networks 101\n",
    "\n",
    "> <span style=\"color:gray\">\n",
    "Original [Theano/Lasagne tutorial](https://github.com/DeepLearningDTU/nvidia_deep_learning_summercamp_2016/blob/master/lab1/lab1_FFN.ipynb) by \n",
    "Lars Maaløe ([larsmaaloee](https://github.com/larsmaaloee)),\n",
    "Søren Kaae Sønderby ([skaae](https://github.com/skaae)), and \n",
    "Casper Sønderby ([casperkaae](https://github.com/casperkaae)). \n",
    "Converted to TensorFlow by \n",
    "Alexander R. Johansen ([alrojo](https://github.com/alrojo)), \n",
    "and updated by \n",
    "Toke Faurby ([faur](https://github.com/Faur)).\n",
    "</span>\n",
    "\n",
    "\n",
    "Convolution neural networks (or ConvNets) are a very succesfull type of neural networks, and are an integral part of reigniting the interest in neural networks.\n",
    "They are abel to extract structural relations in the data such as spatial in images or temporal in time series.\n",
    "\n",
    "Jason Yosinski and colleague developed a [toolbox for visualizing convolutional networks](http://yosinski.com/deepvis).\n",
    "Unfortunately this toolbox is for *Caffe*, so we can't use it here, but they made a [VERY instructive video](https://www.youtube.com/watch?v=AgkfIQ4IGaM) that does a great job of conveying the intuitions.\n",
    "\n",
    "In this lab you will learn what *convolutional layers* are and how they work, as well as important related concepts such as *padding*, *stride*, and *pooling*.\n",
    "\n",
    "\n",
    "\n",
    "#### External resources:\n",
    "For a indept tutorial please see [stanford cs231n](http://cs231n.github.io/convolutional-networks/) or to read more see [Michael Nielsen](http://neuralnetworksanddeeplearning.com/chap6.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are convolutional networks?\n",
    "\n",
    "ConvNets are in may respects very similar to the dense feedforward networks we saw previously:\n",
    " * The network is still organized into layers\n",
    " * Each layer is parameterized by weights and biases\n",
    " * Each layer has an element-wise non-linear transformation (activation function)\n",
    " * There are no cycles in the connections (more on this in later labs)\n",
    "\n",
    "*So what is the difference?*\n",
    "The networks we saw previously are called *dense* because each unit recieves input from all the units in the previous layer.\n",
    "This is not the case for ConvNets.\n",
    "In ConvNets each unit is only connected to a small subset of the input units.\n",
    "This is called the *receptive field* of the unit.\n",
    "\n",
    "#### Let us look at a quick example.\n",
    "Let us define a `2x2` window (learned kernel weights).\n",
    "We apply the window by performning elementwise multiplication, and then summing the results, as shown in this animation:\n",
    "\n",
    "<img src=\"images/convolutions.gif\" style=\"width: 600px;\"/> \n",
    "\n",
    "After the window has been applied we would perform an elementwise non-linear transformation on the *convolved features*.\n",
    "In this example the input is a 2D *feature map* with depth 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strides, padding, and pooling\n",
    "\n",
    "Two important concepts for ConvNets are *strides* and *padding*.\n",
    "#### Padding\n",
    "describes what we do at the edges of the feature map. If we don't use padding the feature map will get smaller every time, as we can see above. If we do use padding we can maintain the same resolution. In deep learning we generally just padd with zeros.\n",
    "\n",
    "#### Strides\n",
    "describe how far the window is moved each time. Strides can be used to reduce the size of the feature map, and the number of computations that needs to be performed.\n",
    "\n",
    "Strides and pooling (exerpted from [here](https://github.com/vdumoulin/conv_arithmetic#convolution-animations)) are shown in the table below.\n",
    "Notice how the output (green) changes shape.\n",
    "\n",
    "| ![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_no_strides.gif) | ![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/same_padding_no_strides.gif) | ![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_strides.gif) | ![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/padding_strides.gif) |\n",
    "|--------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|\n",
    "| No padding, no strides                                                                     | Padding, no strides                                                                          | No padding, stride of 2                                                                 | Padding, stride of 2                                                                 |\n",
    "\n",
    "\n",
    "#### Pooling\n",
    "is another method for reducing the spatial resolution. Similar to convolutional layers it works by sliding a window accross the feature map. Unlike the convolutional layers there are no learnable parameters, and the pooling layers perform the same simple operation every time. The most common types of pooling are:\n",
    " * *Max pooling* where the output of the pooling operation is the highest value in the window, and\n",
    " * *Mean pooling* which outputs the mean of the elements in the window.\n",
    " \n",
    "![Max pooling image](images/maxpooling.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few pointers for image classification\n",
    "If you want do image classification using a pretrained model is often a good choice.\n",
    "It can greatly reduce training time, and help if you have limited amounts of labeled data.\n",
    "\n",
    "There are several popular choices, such as Google Inception model, VGG16, VGG19 and ResNet to name a few.\n",
    "Keras provides several of these models as [nicely packaged ready-to-go models](https://keras.io/applications).\n",
    "And TensorFlow has a guide for using their current state-of-the-art pretrained model in their [model repository](https://github.com/tensorflow/models/tree/master/inception).\n",
    "Torch7 and Theano have similar pretrained models that you can find with google. \n",
    "\n",
    "\n",
    "## Visualizing the filters\n",
    "\n",
    "In the following we will load **ResNet** model, and visualize the kinds of transformations that it sees.\n",
    "This is both useful for giving you an intuition about what is happening, and showing you how pre-trained models can be loaded.\n",
    "\n",
    "\n",
    "Convolution filters can be interpreted as spatial feature detectors picking up different image features such as edges, corners etc.\n",
    "Below we provide code for visualization of the filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet\n",
    "\n",
    "ResNet was very sucessful when it first came out, having the worlds best performance for a while, and still forming the foundation for many of the state-of-the-art networks that are being proposed currently.\n",
    "See the original paper, [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) for more details, or this interesting [blog post](http://torch.ch/blog/2016/02/04/resnets.html). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve2d\n",
    "from scipy import misc\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Collect all the variables and names in a dict\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    names_and_vars = {var.name: sess.run(var.value()) for var in tf.global_variables()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### If you ware interested you can uncomment this line and see the architecture details\n",
    "# base_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### This code was used to find the name of the weights\n",
    "# names = [key for key in names_and_vars.keys()]\n",
    "# names.sort()\n",
    "# for n in names:\n",
    "#     if n.endswith('kernel:0') and \\\n",
    "#             n.startswith(''):\n",
    "\n",
    "#         np_W = names_and_vars[n] # get the filter values from the first conv layer\n",
    "#         print(n, '  \\t', np_W.shape)\n",
    "\n",
    "weight_to_vis = 'conv1/kernel:0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Visualize the weights\n",
    "if not weight_to_vis in names_and_vars:\n",
    "    print(\"ERROR: The name provided wasn't found.\")\n",
    "\n",
    "else:\n",
    "    np_W = names_and_vars[weight_to_vis] # get the filter values from the first conv layer\n",
    "    print(np_W.shape, \"i.e. the shape is filter_size, filter_size, num_channels, num_filters\")\n",
    "    filter_size, _, num_channels, num_filters = np_W.shape\n",
    "    n = int(num_filters**0.5)\n",
    "\n",
    "    idx = 1\n",
    "    plt.figure()\n",
    "    img = np.mean(misc.face(), axis=2)\n",
    "    img = misc.imresize(img, 0.33)\n",
    "    print('img size', img.shape)\n",
    "    plt.imshow(img,cmap='gray',interpolation='none')\n",
    "    plt.title('Input Image')\n",
    "    plt.show()\n",
    "    \n",
    "    # reshaping the last dimension to be n by n\n",
    "    np_W_res = np_W.reshape(filter_size, filter_size, num_channels, n, n)\n",
    "    fig, ax = plt.subplots(n,n,figsize=(10,10))\n",
    "    print(\"learned filter values\")\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ax[i,j].imshow(np_W_res[:,:,0,i,j], cmap='gray',interpolation='none')\n",
    "            ax[i,j].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "            ax[i,j].yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    #visalize the filters convolved with an input image\n",
    "    np_W_res = np_W.reshape(filter_size, filter_size, num_channels, n, n)\n",
    "    fig, ax = plt.subplots(n,n,figsize=(20,20))\n",
    "\n",
    "    print(\"Response from input image convolved with the filters\")\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            ax[i,j].imshow(convolve2d(img, np_W_res[:,:,0,i,j], mode='same'),\n",
    "                           cmap='gray', interpolation='none')\n",
    "            ax[i,j].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "            ax[i,j].yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If you didn't already, you really should [watch this video](https://www.youtube.com/watch?v=AgkfIQ4IGaM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
