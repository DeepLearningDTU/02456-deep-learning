{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with ConvNets\n",
    "\n",
    "> <span style=\"color:gray\">\n",
    "Original [Theano/Lasagne tutorial](https://github.com/DeepLearningDTU/nvidia_deep_learning_summercamp_2016/blob/master/lab1/lab1_FFN.ipynb) by \n",
    "Lars Maaløe ([larsmaaloee](https://github.com/larsmaaloee)),\n",
    "Søren Kaae Sønderby ([skaae](https://github.com/skaae)), and \n",
    "Casper Sønderby ([casperkaae](https://github.com/casperkaae)). \n",
    "Converted to TensorFlow by \n",
    "Alexander R. Johansen ([alrojo](https://github.com/alrojo)), \n",
    "and updated by \n",
    "Toke Faurby ([faur](https://github.com/Faur)).\n",
    "</span>\n",
    "\n",
    "\n",
    "In this lab we will solve the MNIST problem again, but this time with convolutional networks.\n",
    "You will get a to try stacking of convolutional layers, max pooling and strided convolutions which are all important techniques in current convolutional layers network architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependancies and supporting functions\n",
    "\n",
    "\n",
    "Loading dependancies and supporting functions by running the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import sklearn.datasets\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join('.', '..')) \n",
    "import utils \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST data set\n",
    "\n",
    "We load the MNIST dataset.\n",
    "This time the data is keept as images (`shape = [28, 28, 1]`), and not flattended into vectors (`shape = [784]`).\n",
    "This allows the convolutional network to take advantage of the structure of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data (download if you haven't already)\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist_data = input_data.read_data_sets('MNIST_data', \n",
    "                                       one_hot=True,   # Convert the labels into one hot encoding\n",
    "                                       dtype='float32', # rescale images to `[0, 1]`\n",
    "                                       reshape=False, # Don't flatten the images to vectors\n",
    "                                      )\n",
    "\n",
    "## Print dataset statistics and visualize\n",
    "print('')\n",
    "utils.mnist_summary(mnist_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "We will use Keras layers, which are documented [here](https://keras.io/layers/about-keras-layers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D\n",
    "from tensorflow.contrib.layers import flatten # We use this flatten, as it works better than \n",
    "                                              # the Keras 'Flatten' for some reason\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "num_classes = 10\n",
    "height, width, nchannels = 28, 28, 1\n",
    "padding = 'same'\n",
    "\n",
    "filters_1 = 16\n",
    "kernel_size_1 = (5,5)\n",
    "pool_size_1 = (2,2)\n",
    "\n",
    "x_pl = tf.placeholder(tf.float32, [None, height, width, nchannels], name='xPlaceholder')\n",
    "y_pl = tf.placeholder(tf.float64, [None, num_classes], name='yPlaceholder')\n",
    "y_pl = tf.cast(y_pl, tf.float32)\n",
    "\n",
    "print('Trace of the tensors shape as it is propagated through the network.')\n",
    "print('Layer name \\t Output size')\n",
    "print('----------------------------')\n",
    "\n",
    "with tf.variable_scope('convLayer1'):\n",
    "    conv1 = Conv2D(filters_1, kernel_size_1, strides=(1,1), padding=padding, activation='relu')\n",
    "    print('x_pl \\t\\t', x_pl.get_shape())\n",
    "    x = conv1(x_pl)\n",
    "    print('conv1 \\t\\t', x.get_shape())\n",
    "\n",
    "    pool1 = MaxPooling2D(pool_size=pool_size_1, strides=None, padding=padding)\n",
    "    x = pool1(x)\n",
    "    print('pool1 \\t\\t', x.get_shape())\n",
    "    x = flatten(x)\n",
    "    print('Flatten \\t', x.get_shape())\n",
    "\n",
    "\n",
    "with tf.variable_scope('output_layer'):\n",
    "    denseOut = Dense(units=num_classes, activation='softmax')\n",
    "    \n",
    "    y = denseOut(x)\n",
    "    print('denseOut\\t', y.get_shape())    \n",
    "\n",
    "print('Model consits of ', utils.num_params(), 'trainable parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Launch TensorBoard, and visualize the TF graph\n",
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.45)\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:\n",
    "    tmp_def = utils.rename_nodes(sess.graph_def, lambda s:\"/\".join(s.split('_',1)))\n",
    "    utils.show_graph(tmp_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('loss'):\n",
    "    # computing cross entropy per sample\n",
    "    cross_entropy = -tf.reduce_sum(y_pl * tf.log(y+1e-8), reduction_indices=[1])\n",
    "\n",
    "    # averaging over samples\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    \n",
    "with tf.variable_scope('training'):\n",
    "    # defining our optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "\n",
    "    # applying the gradients\n",
    "    train_op = optimizer.minimize(cross_entropy)\n",
    "\n",
    "    \n",
    "with tf.variable_scope('performance'):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = tf.equal(tf.argmax(y, axis=1), tf.argmax(y_pl, axis=1))\n",
    "\n",
    "    # averaging the one-hot encoded vector\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Test the forward pass\n",
    "x_batch, y_batch = mnist_data.train.next_batch(4)\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_opts)) as sess:\n",
    "# with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    y_pred = sess.run(fetches=y, feed_dict={x_pl: x_batch})\n",
    "\n",
    "assert y_pred.shape == y_batch.shape, \"ERROR the output shape is not as expected!\" \\\n",
    "        + \" Output shape should be \" + str(y.shape) + ' but was ' + str(y_pred.shape)\n",
    "\n",
    "print('Forward pass successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Training Loop\n",
    "batch_size = 100\n",
    "max_epochs = 10\n",
    "\n",
    "\n",
    "valid_loss, valid_accuracy = [], []\n",
    "train_loss, train_accuracy = [], []\n",
    "test_loss, test_accuracy = [], []\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Begin training loop')\n",
    "\n",
    "    try:\n",
    "        while mnist_data.train.epochs_completed < max_epochs:\n",
    "            _train_loss, _train_accuracy = [], []\n",
    "            \n",
    "            ## Run train op\n",
    "            x_batch, y_batch = mnist_data.train.next_batch(batch_size)\n",
    "            fetches_train = [train_op, cross_entropy, accuracy]\n",
    "            feed_dict_train = {x_pl: x_batch, y_pl: y_batch}\n",
    "            _, _loss, _acc = sess.run(fetches_train, feed_dict_train)\n",
    "            \n",
    "            _train_loss.append(_loss)\n",
    "            _train_accuracy.append(_acc)\n",
    "            \n",
    "\n",
    "            ## Compute validation loss and accuracy\n",
    "            if mnist_data.train.epochs_completed % 1 == 0 \\\n",
    "                    and mnist_data.train._index_in_epoch <= batch_size:\n",
    "                train_loss.append(np.mean(_train_loss))\n",
    "                train_accuracy.append(np.mean(_train_accuracy))\n",
    "\n",
    "                fetches_valid = [cross_entropy, accuracy]\n",
    "                \n",
    "                feed_dict_valid = {x_pl: mnist_data.validation.images, y_pl: mnist_data.validation.labels}\n",
    "                _loss, _acc = sess.run(fetches_valid, feed_dict_valid)\n",
    "                \n",
    "                valid_loss.append(_loss)\n",
    "                valid_accuracy.append(_acc)\n",
    "                print(\"Epoch {} : Train Loss {:6.3f}, Train acc {:6.3f},  Valid loss {:6.3f},  Valid acc {:6.3f}\".format(\n",
    "                    mnist_data.train.epochs_completed, train_loss[-1], train_accuracy[-1], valid_loss[-1], valid_accuracy[-1]))\n",
    "        \n",
    "        \n",
    "        test_epoch = mnist_data.test.epochs_completed\n",
    "        while mnist_data.test.epochs_completed == test_epoch:\n",
    "            x_batch, y_batch = mnist_data.test.next_batch(batch_size)\n",
    "            feed_dict_test = {x_pl: x_batch, y_pl: y_batch}\n",
    "            _loss, _acc = sess.run(fetches_valid, feed_dict_test)\n",
    "            test_loss.append(_loss)\n",
    "            test_accuracy.append(_acc)\n",
    "        print('Test Loss {:6.3f}, Test acc {:6.3f}'.format(\n",
    "                    np.mean(test_loss), np.mean(test_accuracy)))\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epoch = np.arange(len(train_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_accuracy,'r', epoch, valid_accuracy,'b')\n",
    "plt.legend(['Train Acc','Val Acc'], loc=4)\n",
    "plt.xlabel('Epochs'), plt.ylabel('Acc'), plt.ylim([0.75,1.03])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"> EXE 1.1 </span> Manual calculations\n",
    "\n",
    "![](images/conv_exe.png)\n",
    "\n",
    "\n",
    "\n",
    "1. Manually convolve the input, and compute the convolved features. No padding and no strieds.\n",
    "1. Perform `2x2` max pooling on the convolved features. Stride of 2.\n",
    "\n",
    "___\n",
    "\n",
    "<span style=\"color:blue\"> Answer: </span>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### <span style=\"color:red\"> EXE 1.2 </span> Reducing the resolution\n",
    "One of the important features of convolutional networks are their ability to reduce the spatial resolution, while retaining the important features.\n",
    "Effectively this gives a local translational invariance and reduces the computation. \n",
    "This is most often done with **maxpooling** or by using strides.\n",
    "\n",
    "1. Using only convolutional layers and pooling operations reduce the feature map size to `1x1xF`.\n",
    "    * The number of feature maps, `F`, is up to you.\n",
    "\n",
    "___\n",
    "\n",
    "<span style=\"color:blue\"> Write down what you did: </span>\n",
    "\n",
    "``` \n",
    "Paste your code here\n",
    "```\n",
    "\n",
    "\n",
    "``` \n",
    "Paste the trace of the tensors shape as it is propagated through the network here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"> EXE 1.3 </span> Play around with the network.\n",
    "The MNIST dataset is so easy to solve with convolutional networks that it isn't interesting to spend to much time on maximizing performance.\n",
    "A more interesting question is *how few parameters can you solve it with?*\n",
    "\n",
    "1. Try and minimize the number of parameters, while keeping validation accuracy about 95%. Try changing the\n",
    "\n",
    "    * Number of layers\n",
    "    * Number of filters\n",
    "    * Kernel size\n",
    "    * Pooling size\n",
    "1. Once happy take note of the performance, number of parameters (printed automatically), and describe the network below.\n",
    "___\n",
    "\n",
    "\n",
    "<span style=\"color:blue\"> Answer: </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"> EXE 1.4 </span> Comparing dense and convolutional networks\n",
    "\n",
    "1. Now create a densely connected network (the ones from lab 1), and see how good performance you can get with a similar number of parameters.\n",
    "___\n",
    "\n",
    "<span style=\"color:blue\"> Describe your findings: </span>\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
