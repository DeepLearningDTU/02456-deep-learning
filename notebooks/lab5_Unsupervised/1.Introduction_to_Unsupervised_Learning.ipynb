{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Representation Learning\n",
    "\n",
    "### What is a good representation and why do we need it?\n",
    "\n",
    "Skim the three first pages of [Representation Learning: A Review and New Perspectives](https://arxiv.org/abs/1206.5538) and consider how and why we learn a good representation of data? \n",
    "\n",
    "Even though the datasets we have been using so far always had labels, $y$, connected to $x$, this is rarely the case, as most high-level abstraction labels are given by humans, which is costful and time consuming. So with a limited amount or no labels, how can we then train a classifier? We can of course learn a good representation, where similar data points moves closer together and separates from disimilar points, which can easily be clustered by either k-means, Gaussian mixture model or unsupervised KNN. \n",
    "\n",
    "Solution: We can learn a latent representation holding the internal generative structure of the data, $x$, which can be used for unsupervised classification.\n",
    "\n",
    "Common shallow methods are decompositions into new feature maps. This is called blind signal separation.   \n",
    "\n",
    "Most commonly used is Principal Component Analysis (PCA), where the different components of the features maps can be ordered according to the variance in the features they explain. Choosing the first number of principal components in the loadings matrix, L, for transforming the features, will result in a low dimensional representation favouring the directions of maximum variance. The diagonal variance matrix (eigenvalues) and loadings matrix (eigenvectors) can be obtained through the eigenvalue decomposition of the covariance matrix:\n",
    "\n",
    "$$ [L,D] = eig(cov(X)) $$\n",
    "\n",
    "\n",
    "\n",
    "### AE\n",
    "$$ x \\rightarrow z \\rightarrow \\hat{x} $$\n",
    "\n",
    "### DAE\n",
    "$$ x \\rightarrow \\tilde{x} + noise \\rightarrow z \\rightarrow \\hat{x} $$\n",
    "\n",
    "\n",
    "### VAE\n",
    "$$ x \\rightarrow p(z) \\rightarrow \\hat{x} \\sim p(x|z) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
